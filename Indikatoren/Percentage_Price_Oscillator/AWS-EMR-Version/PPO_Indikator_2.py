# -*- coding: utf-8 -*-
"""PPO_Indikator_v7_EMR_Teil_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pQZBPvx4WKZghVZwpaziBxUH0yhlI5Gn

---
---
---
# **Studienarbeit in der Vorlesung 'Applied Big Data Analyitics'**
# **Implementation: Percent Price Oscillator (PPO) anhand des Data-Warehouse-Models**
---
---
---
* ### **Erstellt:** Wintersemeseter 2019-2020
* ### **Dozent:** Prof. Dr. Sebastian Leuoth
* ### **Autor:** Jan Gaida
* ### **Email:** jan.gaida@hof-university.de
* ### **Git:** [trading_2019](https://github.com/sleuoth-hof/trading_2019), [JanGaida](https://github.com/JanGaida) 
---
---
### *Powered by:*
### ***Hochschule für Angewandte Wissenschaften Hof***
[![Logo: Hochschule Hof](https://www.uni-assist.de/fileadmin/_processed_/4/7/csm_hof-university_logo_308ee8b37b.jpg)](https://www.hof-university.de/)
---
---
---
*© 2019-2020 Jan Gaida, Prof. Dr. Sebastian Leuoth. All Rights Reserved.*

---
# Laufzeit-Parameter
---
"""

# Import
Pfad_Zu_Temporaeres_Warehouse = 'tmp_warehouse/dataframe.csv'

# Export
Pfad_Zu_Finalem_Warehouse_Verzeichnis = 'warehouse'
Export_CSV_Teile_Mergen = True
Temporaere_CSV_Teile_Nach_Merge_Loeschen = True # Nur ausgelöst wenn: Export_CSV_Teile_Mergen = True

"""---
# Import
---
"""

# PYSPARK
from pyspark import SparkContext
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.functions import input_file_name, col, collect_list, concat_ws, udf, from_unixtime, unix_timestamp
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.window import Window
from pyspark.sql.functions import date_format

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sqlContext = SQLContext(sc)

# OS & FILE-STRUCTURE-RELATED
import os
import shutil
from glob import glob

# NUMPY
import numpy
from numpy import array

"""---
# Vorbereitungen
---

Ordner-Struktur
---
---
"""

# Ordner erstellen
if not os.path.exists(Pfad_Zu_Finalem_Warehouse_Verzeichnis):
    os.mkdir(Pfad_Zu_Finalem_Warehouse_Verzeichnis)

"""Datenframe-Vorbereitungen
---
---
"""

# Wieder einlesen
data_output = spark.read.csv(Pfad_Zu_Temporaeres_Warehouse, inferSchema = True, header = True) \
    .withColumn("date", date_format(col("date"), "yyyy-MM-dd"))

# Zu filternde Tage
data_output_days = data_output.select(data_output.date).distinct()
data_output_days_ar = array([row.date for row in data_output_days.collect()])

"""----
# CSV-Export
----
*Die Daten sollen nach Tage sotiert ausgegeben werden (vgl. Besprechung 13.12.19)*

*Darstellung des Datums nach [ISO-8601-Norm](https://lmgtfy.com/?q=ISO-8601) (z.B.: 2019-09-07)*

Export
---
---
"""

# Export-Loop
for currentFilename in data_output_days_ar:
    if currentFilename is not None:
        # aktueller Output-Pfad
        currentPath = os.path.join(Pfad_Zu_Finalem_Warehouse_Verzeichnis, currentFilename)

        # aktuelle Output-Daten sammeln
        currentData = data_output.where(data_output.date == currentFilename)
        currentData.collect();

        # schreiben
        currentData.write.csv(currentPath, mode = 'overwrite', encoding = 'utf-8', header = True )

"""---
# Finaly
---
"""

print("Aufgabe abgeschlossen.")

sc.stop()

"""---
Mergen
---
---

via Shell:
```
hadoop dfs -get warehouse warehouse;
cd ~;
cd warehouse;
for dir in */ 
do
cd "$dir"
if [ -f ~/warehouse/"${dir::-1}".csv ]
then
rm ~/warehouse/"${dir::-1}".csv
fi
cat part-*.csv >> ~/warehouse/"${dir::-1}".csv
cd ..
rm -R "$dir"
done;
cd ..;
```
"""