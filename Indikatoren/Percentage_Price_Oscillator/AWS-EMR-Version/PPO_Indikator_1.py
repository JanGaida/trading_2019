# -*- coding: utf-8 -*-
"""PPO_Indikator_v7_EMR_Teil_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dhn9G6o2Fhrjsxqxlze-IiUQWOdKkeZy

---
---
---
# **Studienarbeit in der Vorlesung 'Applied Big Data Analyitics'**
# **Implementation: Percent Price Oscillator (PPO) anhand des Data-Warehouse-Models**
---
---
---
* ### **Erstellt:** Wintersemeseter 2019-2020
* ### **Dozent:** Prof. Dr. Sebastian Leuoth
* ### **Autor:** Jan Gaida
* ### **Email:** jan.gaida@hof-university.de
* ### **Git:** [trading_2019](https://github.com/sleuoth-hof/trading_2019), [JanGaida](https://github.com/JanGaida) 
---
---
### *Powered by:*
### ***Hochschule für Angewandte Wissenschaften Hof***
[![Logo: Hochschule Hof](https://www.uni-assist.de/fileadmin/_processed_/4/7/csm_hof-university_logo_308ee8b37b.jpg)](https://www.hof-university.de/)
---
---
---
*© 2019-2020 Jan Gaida, Prof. Dr. Sebastian Leuoth. All Rights Reserved.*

---
# Laufzeit-Parameter
---
"""

# Datenbasis
Pfad_Zu_CSV_Datem = './ABDA2019/testdaten/cryptominuteresolution/'
Filenamen_Zu_Laden = '*.csv'
Filename_Start_Index = 96
Filename_Laenge = 6

# Export
Pfad_Zu_Temporaeres_Warehouse_Verzeichnis = './tmp_warehouse' # Erfordert ggf. Anpassungen am Shell-Skript
Filename_Temporaere_Daten = 'dataframe' # Erfordert ggf. Anpassungen am Shell-Skript

"""---
# Import
---
"""

# PYSPARK
from pyspark import SparkContext
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.functions import input_file_name, col, collect_list, udf
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.window import Window
from pyspark.sql.functions import date_format

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sqlContext = SQLContext(sc)

# OS & FILE-STRUCTURE-RELATED
import os
import shutil
from glob import glob

"""---
# Anwendung PPO-Indikator
---

*PPO Implementation basierend auf: [Investopedia.com](https://www.investopedia.com/terms/p/ppo.asp#formula-and-calculation-for-ppo)*

Metadaten-Aggregation
---
---
"""

# Einlesen
path_to_read = os.path.join('./ABDA2019/testdaten/cryptominuteresolution', Filenamen_Zu_Laden)
df_spark = spark.read.csv(path_to_read, inferSchema = True, header = True)

# Aggregation
df_spark = df_spark \
    .withColumn('filepath', input_file_name()) \
    .withColumn('filename', (input_file_name()[Filename_Start_Index:Filename_Laenge]) ) \
    .withColumn('timestamp', df_spark['time']/1000) \
    .withColumn('date', (df_spark['time']/1000).cast('timestamp'))

# Erstellen der SQL-Tabelle
df_spark.createOrReplaceTempView("base_data")
df_spark_total_count = df_spark.count()

"""Tag-Filter
---
---
"""

# Tage-Filter
base_data_dayfiltered = spark.sql(
    "SELECT d.* FROM base_data d " \
    "WHERE (d.date, filename) IN (select max(m.date), filename " \
    "FROM base_data m " \
    "GROUP BY date_format(m.date, \"y-M-d\"), filename)"
)

# Erstellen der SQL-Tabelle
base_data_dayfiltered.createOrReplaceTempView("base_data_dayfiltered")
base_data_dayfiltered_total_count = base_data_dayfiltered.count()

# Min-Max-Tage-SQL-Tabelle:
base_data_dayfiltered_informationView = spark.sql(
    "SELECT filename, count(*), min(date), max(date) " \
    "FROM base_data_dayfiltered " \
    "group by filename"
)

"""EMA-UDF
---
---
Berrechnet nach: [tradistats.com](https://tradistats.com/exponentieller-gleitender-durchschnitt/)
"""

# EMA-UDF
# see https://tradistats.com/exponentieller-gleitender-durchschnitt/
def ema(ar):
    if len(ar) > 0:
       SF  = 2/ (len(ar)+1)
       SFi = 1 - SF
       my_ema = ar[0]
       for i in ar:
           my_ema = (i * SF) + (my_ema * SFi)
    return my_ema

ema_udf = udf(ema, DoubleType())

"""Windows
---
---
"""

# EMA-WINDOWS
win26 =  Window \
    .partitionBy("filename") \
    .orderBy("date") \
    .rowsBetween(-25, 0)

win12 = Window \
    .partitionBy("filename") \
    .orderBy("date") \
    .rowsBetween(-11, 0)

win9 = Window \
    .partitionBy("filename") \
    .orderBy("date") \
    .rowsBetween(-8, 0)

"""PPO-Berrechnungen
---
---
"""

# EMA_26 && EMA_12 CALCULATION
base_data_tmp_ema_1 = base_data_dayfiltered \
    .withColumn('win26_close_list', collect_list('close').over(win26)) \
    .withColumn('win12_close_list', collect_list('close').over(win12))

base_data_tmp_ema_2 = base_data_tmp_ema_1.select(
    "*",
    ema_udf(base_data_tmp_ema_1["win26_close_list"]).alias("EMA26"),
    ema_udf(base_data_tmp_ema_1["win12_close_list"]).alias("EMA12")
)

base_data_tmp_ema_2.createOrReplaceTempView("base_data_tmp_ema_2")

# PPO BERECHNUNG
base_data_tmp_ema_3 = spark.sql(
    "SELECT *, (((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))" \
    "FROM base_data_tmp_ema_2 " \
    "ORDER BY date ASC"
)

# SIGNAL BERECHNUNG
base_data_tmp_ema_4 = base_data_tmp_ema_3 \
    .withColumn('win9_ema12sub26_list', collect_list('(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))').over(win9))

base_data_tmp_ema_5 = base_data_tmp_ema_4.select(
    "*",
    base_data_tmp_ema_4['(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))'].alias("PPO"),
    ema_udf(base_data_tmp_ema_4["win9_ema12sub26_list"]).alias("SIGNAL")
)

base_data_tmp_ema_5.createOrReplaceTempView("base_data_tmp_ema_5")

# PPO-HISTOGRAM BERECHNUNG
base_data_tmp_ema_6 = spark.sql(
    "SELECT *, (PPO - SIGNAL) " \
    "FROM base_data_tmp_ema_5 " \
    "ORDER BY date ASC"
)

base_data_ema = base_data_tmp_ema_6.select(
    "*",
    base_data_tmp_ema_6['(PPO - SIGNAL)'].alias("PPO_HISTOGRAM")
)

base_data_ema.createOrReplaceTempView("base_data_ema")

"""---
# Analyse 
---
*PPO-Analyse ebenfalls basierend auf: [Investopedia.com](https://www.investopedia.com/terms/p/ppo.asp#what-the-indicator-tells-you)*

Windows
---
---
"""

# Analyse-Windows
win2 =  Window \
    .partitionBy("filename") \
    .orderBy("date") \
    .rowsBetween(-1, 0)

win3 = Window \
    .partitionBy("filename")\
    .orderBy("date") \
    .rowsBetween(-2, 0)

"""Funktionen
---
---
**→ ppo_trend_analysis** :

Auf den 'PPO' angewendet (window mind. 1):

> *When the PPO is above zero, that helps confirm an uptrend since the short-term EMA is above the longer-term EMA. When the PPO is below zero, the short-term EMA is below the longer-term EMA, which is an indication of a downtrend.*


**→ ppo_crossover_signal_analysis** :

Auf den 'PPO_HISTOGRAM' angewendet (window mind. 2):
> *The indicator generates a buy signal when the PPO line crosses above the signal line from below, and a sell signal occurs when the PPO line crosses  below the signal from above.*

Auf den 'PPO' angewendet (window mind. 2):
> *Centerline crossovers also generate trading signals. Traders consider a move from below to above the centerline as bullish, and a move from above to below the centerline as bearish.*

**→ ppo_technical_divergence_analysis:**

Auf den 'close' und 'PPO' (window mind. 2; ggf. 'close' mit 'open' tauschen):
> *Traders can also use the PPO to look for technical divergence between the indicator and price. For example, if the price of an asset makes a higher high, but the indicator makes a lower high, it may indicate the upward momentum is subsiding. Conversely, if an asset's price makes a lower low, but the indicator makes a higher low, it could suggest that the bears are losing their traction and the price could head higher soon.*
"""

# Analyse-UDFs

def ppo_trend_analysis(ar):
    output = ""
    count = len(ar)
    if count > 0:
      # Einzelabgleich
      if count == 1:
        current = ar[0]
        if current > 0:
          output = "▲ Aufwärtstrend"
        elif current < 0:
          output = "▼ Abwärtstrend"

      # Vergleich mit den davorherigenden; ggf. mit mehreren vorherigen
      elif count > 1:
        last_position = count - 1
        next_to_last_position = last_position - 1

        last = ar[last_position] 
        next_to_last = ar[next_to_last_position]

        if last > 0 and next_to_last > 0:
          output = "▲ Aufwärtstrend"
        elif last < 0 and next_to_last > 0:
          output = "↘ neuer Abwärtstrend"
        elif last < 0 and next_to_last < 0:
          output = "▼ Abwärtstrend"
        elif last > 0 and next_to_last < 0:
          output = "↗ neuer Aufwärtstrend"

    return output

def ppo_crossover_signal_analysis(ar):
    output = ""
    count = len(ar)
    # Vergleich des letzten Crossovers
    if count > 1:
        last_position = count - 1
        next_to_last_position = last_position - 1
        
        last = ar[last_position] 
        next_to_last = ar[next_to_last_position]

        if last <= 0 and next_to_last > 0:
          output = "Verkauf-Signal"
        elif last >= 0 and next_to_last < 0:
          output = "Kauf-Signal"

    return output

def ppo_technical_divergence_analysis(close_ar, ppo_ar):
    output = ""
    count_close = len(close_ar)
    count_ppo = len(ppo_ar)
    
    # Arrays müssen selbe Größe haben
    if count_close == count_ppo and count_close > 1:
      # delta berechnen
      pointer = count_close - 1
      delta_close = close_ar[pointer]
      delta_ppo = ppo_ar[pointer]
      pointer -= 1

      while pointer >= 0:
        delta_close -= close_ar[pointer]
        delta_ppo -= ppo_ar[pointer]
        pointer -= 1

      # steigung berechnen
      div_close = delta_close / count_close
      div_ppo = delta_ppo / count_ppo

      if div_close > div_ppo:
        output = "↧ Abflachend"
      else:
        output = "↥ Wachsend"

    return output

ppo_trend_analysis_udf = udf(ppo_trend_analysis, StringType())
ppo_crossover_signal_analysis_udf = udf(ppo_crossover_signal_analysis, StringType())
ppo_technical_divergence_analysis_udf = udf(ppo_technical_divergence_analysis, StringType())

"""Aggregation
---
---
"""

# Analyse-Aggregation

base_data_ema_tmp_eval = base_data_ema \
    .withColumn('win2_ppo_list', collect_list('PPO').over(win2)) \
    .withColumn('win2_ppoh_list', collect_list('PPO_Histogram').over(win2)) \
    .withColumn('win2_close_list', collect_list('close').over(win2))

base_data_ema_eval_result = base_data_ema_tmp_eval.select(
    "*",
    ppo_trend_analysis_udf(base_data_ema_tmp_eval["win2_ppo_list"]).alias("TREND"),
    ppo_crossover_signal_analysis_udf(base_data_ema_tmp_eval["win2_ppo_list"]).alias("WEAK_SIGNAL"),
    ppo_crossover_signal_analysis_udf(base_data_ema_tmp_eval["win2_ppoh_list"]).alias("STRONG_SIGNAL"),
    ppo_technical_divergence_analysis_udf(base_data_ema_tmp_eval["win2_close_list"], base_data_ema_tmp_eval["win2_ppo_list"]).alias("DIVERGENCE")
)

base_data_ema_eval_result.createOrReplaceTempView("base_data_ema_eval_result")

"""---
# Auswertung
---

*Für alle finalen Empfehlungen gilt eine Reichteweite von 0 (schwach) bis 3 (stark)*

**Halten**
---
---
*   **H0:** 
  * ▼ Abwärtstrend && ↧ Abflachend
  * ▼ Abwärtstrend && ↥ Wachsend
  * ↘ neuer Abwärtstrend && ↧ Abflachend
*   **H1:**
  * ↗ neuer Aufwärtstrend && ↧ Abflachend
*   **H2:**
  * ▲ Aufwärtstrend && ↧ Abflachend
  * ↘ neuer Abwärtstrend && ↥ Wachsend
*   **H3:**
  * ▲ Aufwärtstrend && ↥ Wachsend
  * ↗ neuer Aufwärtstrend && ↥ Wachsend
"""

# Halten-SQL-Statement
holdingCaseStatement = "CASE WHEN (TREND = \"↗ neuer Aufwärtstrend\" AND DIVERGENCE = \"↧ Abflachend\") THEN \"H1\"" +"\n" \
  "WHEN (TREND = \"▲ Aufwärtstrend\" = \"↧ Abflachend\") OR (TREND = \"↘ neuer Abwärtstrend\" AND DIVERGENCE = \"↥ Wachsend\") THEN \"H2\"" +"\n" \
  "WHEN (TREND = \"▲ Aufwärtstrend\" AND DIVERGENCE = \"↥ Wachsend\") OR (TREND = \"↗ neuer Aufwärtstrend\" AND DIVERGENCE = \"↥ Wachsend\") THEN \"H3\"" +"\n" \
  "ELSE \"H0\" END" +"\n"

"""**Kaufen**
---
---
*   **K0:** 
  * ▼ Abwärtstrend && ↧ Abflachend
  * ↘ neuer Abwärtstrend && ↧ Abflachend
*   **K1:**
  * ▼ Abwärtstrend && ↥ Wachsend
  * ↘ neuer Abwärtstrend && ↥ Wachsend
  * ↗ neuer Aufwärtstrend && ↧ Abflachend
*   **K2:**
  * ▲ Aufwärtstrend && ↧ Abflachend
  * ↗ neuer Aufwärtstrend && ↥ Wachsend
*   **K3:**
  * ▲ Aufwärtstrend && ↥ Wachsend
"""

# Kaufen-SQL-Statement
buyingCaseStatement = "CASE WHEN (TREND = \"▼ Abwärtstrend\" AND DIVERGENCE = \"↧ Abflachend\") OR (TREND = \"↘ neuer Abwärtstrend\" AND DIVERGENCE = \"↧ Abflachend\") THEN \"K0\"" +"\n" \
  "WHEN (TREND = \"▲ Aufwärtstrend\" AND DIVERGENCE = \"↧ Abflachend\") OR (TREND = \"↗ neuer Aufwärtstrend\" AND DIVERGENCE = \"↥ Wachsend\") THEN \"K2\"" +"\n" \
  "WHEN (TREND = \"▲ Aufwärtstrend\" AND DIVERGENCE = \"↥ Wachsend\") THEN \"K3\"" +"\n" \
  "ELSE \"K1\" END" +"\n"

"""**Verkaufen**
---
---
*   **V0:** 
  * ▲ Aufwärtstrend && ↥ Wachsend
  * ▲ Aufwärtstrend && ↧ Abflachend
*   **V1:**
  * ↗ neuer Aufwärtstrend && ↥ Wachsend
  * ↘ neuer Abwärtstrend && ↥ Wachsend
*   **V2:**
  * ▼ Abwärtstrend && ↥ Wachsend
  * ↗ neuer Aufwärtstrend && ↧ Abflachend
  * ↘ neuer Abwärtstrend && ↧ Abflachend
*   **V3:**
  * ▼ Abwärtstrend && ↧ Abflachend
"""

# Verkaufen-SQL-Statement
sellingCaseStatement = "CASE WHEN (TREND = \"▲ Aufwärtstrend\" AND DIVERGENCE = \"↥ Wachsend\")OR (TREND = \"▲ Aufwärtstrend\" AND DIVERGENCE = \"↧ Abflachend\") THEN \"V0\"" +"\n" \
  "WHEN (TREND = \"↗ neuer Aufwärtstrend\" = \"↥ Wachsend\" AND DIVERGENCE = \"↥ Wachsend\") OR (TREND = \"↘ neuer Abwärtstrend\" AND DIVERGENCE = \"↥ Wachsend\") THEN \"V1\"" +"\n" \
  "WHEN (TREND = \"▼ Abwärtstrend\" AND DIVERGENCE = \"↧ Abflachend\") THEN \"V3\" " +"\n" \
  "ELSE \"V2\" END" +"\n"

"""**Aggregation**
---
---
*vgl. dazu Investopedia-Link (s.h. Sections-Header)*

* Wenn beide Signale null sind ⇒ *halten*
* Ansonsten 'Strong-' > 'Weaksignal', für jeweils beide gilt:
  * KAUF-Signal ⇒ *kaufen*
  * VERKAUF-Signal ⇒ *verkaufen*

```
+----------------+----------------+------------+
|  WEAK_SIGNAL   | STRONG_SIGNAL  | EMPFEHLUNG |
+----------------+----------------+------------+
| null           | null           | HALTEN     |
| Kauf-Signal    | null           | KAUFEN     |
| Kauf-Signal    | Kauf-Signal    | KAUFEN     |
| ...            | ...            | ...        |
| Verkauf-Signal | null           | VERKAUFEN  |
| Verkauf-Signal | Verkauf-Signal | VERKAUFEN  |
| ...            | ...            | ...        |
| Kauf-Signal    | Verkauf-Signal | VERKAUFEN  |
| Verkauf-Signal | Kauf-Signal    | KAUFEN     |
+----------------+----------------+------------+
```
"""

# Combiniertes-SQL-Statement
final_result_statement = "SELECT *," \
" CASE" \
"   WHEN (WEAK_SIGNAL = \"\" AND STRONG_SIGNAL = \"\") THEN " + holdingCaseStatement + \
"   WHEN (STRONG_SIGNAL = \"\") THEN " \
"     CASE" \
"        WHEN (WEAK_SIGNAL = \"Kauf-Signal\") THEN " + buyingCaseStatement + \
"        ELSE " + sellingCaseStatement + \
"     END" \
"   ELSE" \
"     CASE" \
"        WHEN (STRONG_SIGNAL = \"Kauf-Signal\") THEN " + buyingCaseStatement + \
"        ELSE " + sellingCaseStatement + \
"     END"  \
" END AS PPO_RESULT" \
" FROM base_data_ema_eval_result"

data_final_eval_result = spark.sql(final_result_statement)
data_final_eval_result.createOrReplaceTempView("data_final_eval_result")

"""---
# Visualisierung
---
"""

# Gelöscht. Dieses File ist nur für die EMR-Umgebung gedacht, welche die vorhandene Visualisierung nicht unterstützt.
# s.h. 'PPO_Indikator_%VERSION%_COLAB.ipynb' für Graphen-Visualisierung

"""----
# TMP-CSV-Export
----

Ordner-Struktur
---
---
"""

# Temporäres Verzeichnis erstellen
if not os.path.exists(Pfad_Zu_Temporaeres_Warehouse_Verzeichnis):
    os.mkdir(Pfad_Zu_Temporaeres_Warehouse_Verzeichnis)

"""Vorbereitung
---
---
"""

# Output Daten sammeln
data_output = spark.sql(
  " select filename, Date(date) date, open, close, high, low, volume, PPO_RESULT" \
  " from data_final_eval_result "
)
data_output.collect();

"""Tmp-Export
---
---
"""

# Schreib-Funktion
outputSplitPath = os.path.join(Pfad_Zu_Temporaeres_Warehouse_Verzeichnis, Filename_Temporaere_Daten)
data_output.write.csv(outputSplitPath, mode = 'overwrite', encoding = 'utf-8', header = True)
outputMergedPath = os.path.join(Pfad_Zu_Temporaeres_Warehouse_Verzeichnis, (Filename_Temporaere_Daten + '.csv'))

"""---
# Finaly
---
"""

print("Aufgabe abgeschlossen.\n\nSchema:")
data_output.printSchema()
print("Preview:")
data_output.show()

sc.stop()

"""---
Mergen
---
---

Continue via Shell:
```
hadoop dfs -get warehouse warehouse;
cd ~;
if [ ! -d tmp_warehouse ]
then
mkdir tmp_warehouse
fi
hadoop dfs -get tmp_warehouse/dataframe tmp_warehouse/dataframe;
cd tmp_warehouse;
cd dataframe;
cat part-*.csv >> ~/tmp_warehouse/dataframe.csv;
cd ..;
rm -R dataframe;
cd ..;
hdfs dfs -put ./tmp_warehouse/dataframe.csv tmp_warehouse;
```
"""