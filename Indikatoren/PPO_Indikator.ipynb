{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_Indikator_v3-4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hLeICupUHzXk",
        "wy1-pSd3K8ex"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82zjv2aVvLo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "# **Studienarbeit in der Vorlesung 'Applied Big Data Analyitics'**\n",
        "# **Implementation: Percent Price Oscillator (PPO) anhand des Data-Warehouse-Models**\n",
        "---\n",
        "---\n",
        "---\n",
        "* ### **Erstellt:** Wintersemeseter 2019-2020\n",
        "* ### **Dozent:** Prof. Dr. Sebastian Leuoth\n",
        "* ### **Autor:** Jan Gaida\n",
        "* ### **Email:** jan.gaida@hof-university.de\n",
        "* ### **Git:** [trading_2019](https://github.com/sleuoth-hof/trading_2019), [JanGaida](https://github.com/JanGaida) \n",
        "---\n",
        "---\n",
        "### *Powered by:*\n",
        "### ***Hochschule für Angewandte Wissenschaften Hof***\n",
        "[![Logo: Hochschule Hof](https://www.uni-assist.de/fileadmin/_processed_/4/7/csm_hof-university_logo_308ee8b37b.jpg)](https://www.hof-university.de/)\n",
        "---\n",
        "---\n",
        "---\n",
        "*© 2019-2020 Jan Gaida, Prof. Dr. Sebastian Leuoth. All Rights Reserved.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLeICupUHzXk",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Datenbank\n",
        "---\n",
        "*Urspung: [More than 400 Cryptocurrency-Chartdata](https://www.kaggle.com/tencars/392-crypto-currency-pairs-at-minute-resolution/version/2)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gisP4TkgH8Ba",
        "colab_type": "code",
        "outputId": "111a3ef7-3e2e-4c4b-c864-f1f46022ebb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# clonen\n",
        "!git clone https://github.com/sleuoth/ABDA2019.git\n",
        "\n",
        "# entpacken 'btcusd'\n",
        "!unzip ABDA2019/testdaten/cryptominuteresolution/btcusd.csv.zip\n",
        "!mv btcusd.csv ABDA2019/testdaten/cryptominuteresolution/btcusd.csv\n",
        "\n",
        "# resultat\n",
        "!ls ABDA2019/testdaten/cryptominuteresolution"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ABDA2019'...\n",
            "remote: Enumerating objects: 424, done.\u001b[K\n",
            "remote: Total 424 (delta 0), reused 0 (delta 0), pack-reused 424\u001b[K\n",
            "Receiving objects: 100% (424/424), 486.44 MiB | 18.90 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "Checking out files: 100% (418/418), done.\n",
            "Archive:  ABDA2019/testdaten/cryptominuteresolution/btcusd.csv.zip\n",
            "  inflating: btcusd.csv              \n",
            "abseth.csv\tcndbtc.csv  foausd.csv\tmkrdai.csv  rdnusd.csv\tutkbtc.csv\n",
            "absusd.csv\tcndeth.csv  fsnbtc.csv\tmkreth.csv  repbtc.csv\tutketh.csv\n",
            "agibtc.csv\tcndusd.csv  fsneth.csv\tmkrusd.csv  repeth.csv\tutkusd.csv\n",
            "agieth.csv\tcnneth.csv  fsnusd.csv\tmlneth.csv  repusd.csv\tutneth.csv\n",
            "agiusd.csv\tcnnusd.csv  fttusd.csv\tmlnusd.csv  reqbtc.csv\tutnusd.csv\n",
            "aidbtc.csv\tcsxeth.csv  fttust.csv\tmnabtc.csv  reqeth.csv\tveebtc.csv\n",
            "aideth.csv\tcsxusd.csv  funbtc.csv\tmnaeth.csv  requsd.csv\tveeeth.csv\n",
            "aidusd.csv\tctxbtc.csv  funeth.csv\tmnausd.csv  rifbtc.csv\tveeusd.csv\n",
            "aiobtc.csv\tctxeth.csv  funusd.csv\tmtnbtc.csv  rifusd.csv\tvetbtc.csv\n",
            "aioeth.csv\tctxusd.csv  geneth.csv\tmtneth.csv  rlcbtc.csv\tveteth.csv\n",
            "aiousd.csv\tdadbtc.csv  genusd.csv\tmtnusd.csv  rlceth.csv\tvetusd.csv\n",
            "algbtc.csv\tdadeth.csv  gnoeth.csv\tncabtc.csv  rlcusd.csv\tvldeth.csv\n",
            "algusd.csv\tdadusd.csv  gnousd.csv\tncaeth.csv  rrbusd.csv\tvldusd.csv\n",
            "algust.csv\tdaibtc.csv  gntbtc.csv\tncausd.csv  rrbust.csv\tvsybtc.csv\n",
            "ampbtc.csv\tdaieth.csv  gnteth.csv\tnecbtc.csv  rrtbtc.csv\tvsyusd.csv\n",
            "ampusd.csv\tdaiusd.csv  gntusd.csv\tneceth.csv  rrtusd.csv\twaxbtc.csv\n",
            "ampust.csv\tdatbtc.csv  goteth.csv\tnecusd.csv  rteeth.csv\twaxeth.csv\n",
            "antbtc.csv\tdateth.csv  goteur.csv\tneobtc.csv  rteusd.csv\twaxusd.csv\n",
            "anteth.csv\tdatusd.csv  gotusd.csv\tneoeth.csv  sanbtc.csv\twbteth.csv\n",
            "antusd.csv\tdgbbtc.csv  gsdusd.csv\tneoeur.csv  saneth.csv\twbtusd.csv\n",
            "asteth.csv\tdgbusd.csv  gtxusd.csv\tneogbp.csv  sanusd.csv\twlousd.csv\n",
            "astusd.csv\tdgxeth.csv  gtxust.csv\tneojpy.csv  screth.csv\twloxlm.csv\n",
            "atmbtc.csv\tdgxusd.csv  hotbtc.csv\tneousd.csv  scrusd.csv\twprbtc.csv\n",
            "atmeth.csv\tdrneth.csv  hoteth.csv\tnioeth.csv  seebtc.csv\twpreth.csv\n",
            "atmusd.csv\tdrnusd.csv  hotusd.csv\tniousd.csv  seeeth.csv\twprusd.csv\n",
            "atobtc.csv\tdshbtc.csv  impeth.csv\todebtc.csv  seeusd.csv\twtceth.csv\n",
            "atoeth.csv\tdshusd.csv  impusd.csv\todeeth.csv  senbtc.csv\twtcusd.csv\n",
            "atousd.csv\tdtabtc.csv  inteth.csv\todeusd.csv  seneth.csv\txcheth.csv\n",
            "aucbtc.csv\tdtaeth.csv  intusd.csv\tokbbtc.csv  senusd.csv\txchusd.csv\n",
            "auceth.csv\tdtausd.csv  iosbtc.csv\tokbeth.csv  sngbtc.csv\txlmbtc.csv\n",
            "aucusd.csv\tdthbtc.csv  ioseth.csv\tokbusd.csv  sngeth.csv\txlmeth.csv\n",
            "avtbtc.csv\tdtheth.csv  iosusd.csv\tokbust.csv  sngusd.csv\txlmeur.csv\n",
            "avteth.csv\tdthusd.csv  iotbtc.csv\tomgbtc.csv  sntbtc.csv\txlmgbp.csv\n",
            "avtusd.csv\tdtxusd.csv  ioteth.csv\tomgdai.csv  snteth.csv\txlmjpy.csv\n",
            "babbtc.csv\tdtxust.csv  ioteur.csv\tomgeth.csv  sntusd.csv\txlmusd.csv\n",
            "babusd.csv\tedobtc.csv  iotgbp.csv\tomgusd.csv  spkbtc.csv\txmrbtc.csv\n",
            "babust.csv\tedoeth.csv  iotjpy.csv\tomnbtc.csv  spketh.csv\txmrusd.csv\n",
            "batbtc.csv\tedousd.csv  iotusd.csv\tomnusd.csv  spkusd.csv\txraeth.csv\n",
            "bateth.csv\telfbtc.csv  iqxbtc.csv\tonleth.csv  stjbtc.csv\txrausd.csv\n",
            "batusd.csv\telfeth.csv  iqxeos.csv\tonlusd.csv  stjeth.csv\txrpbtc.csv\n",
            "bbneth.csv\telfusd.csv  iqxusd.csv\torsbtc.csv  stjusd.csv\txrpusd.csv\n",
            "bbnusd.csv\tenjeth.csv  kanusd.csv\torseth.csv  swmeth.csv\txtzbtc.csv\n",
            "bcibtc.csv\tenjusd.csv  kanust.csv\torsusd.csv  swmusd.csv\txtzusd.csv\n",
            "bciusd.csv\teosbtc.csv  kncbtc.csv\tpaibtc.csv  tkneth.csv\txvgbtc.csv\n",
            "bftbtc.csv\teoseth.csv  knceth.csv\tpaiusd.csv  tknusd.csv\txvgeth.csv\n",
            "bfteth.csv\teoseur.csv  kncusd.csv\tpaseth.csv  tnbbtc.csv\txvgeur.csv\n",
            "bftusd.csv\teosgbp.csv  leobtc.csv\tpasusd.csv  tnbeth.csv\txvggbp.csv\n",
            "bntbtc.csv\teosjpy.csv  leoeos.csv\tpaxusd.csv  tnbusd.csv\txvgjpy.csv\n",
            "bnteth.csv\teosusd.csv  leoeth.csv\tpaxust.csv  trieth.csv\txvgusd.csv\n",
            "bntusd.csv\teosust.csv  leousd.csv\tpnketh.csv  triusd.csv\tyggeth.csv\n",
            "boxeth.csv\tessbtc.csv  leoust.csv\tpnkusd.csv  trxbtc.csv\tyggusd.csv\n",
            "boxusd.csv\tesseth.csv  looeth.csv\tpoabtc.csv  trxeth.csv\tyywbtc.csv\n",
            "bsvbtc.csv\tessusd.csv  loousd.csv\tpoaeth.csv  trxeur.csv\tyyweth.csv\n",
            "bsvusd.csv\tetcbtc.csv  lrcbtc.csv\tpoausd.csv  trxgbp.csv\tyywusd.csv\n",
            "btceur.csv\tetcusd.csv  lrceth.csv\tpoybtc.csv  trxjpy.csv\tzbtusd.csv\n",
            "btcgbp.csv\tethbtc.csv  lrcusd.csv\tpoyeth.csv  trxusd.csv\tzbtust.csv\n",
            "btcjpy.csv\tetheur.csv  ltcbtc.csv\tpoyusd.csv  tsdusd.csv\tzcnbtc.csv\n",
            "btcusd.csv\tethgbp.csv  ltcusd.csv\tqshbtc.csv  tsdust.csv\tzcneth.csv\n",
            "btcusd.csv.zip\tethjpy.csv  ltcust.csv\tqsheth.csv  udcusd.csv\tzcnusd.csv\n",
            "btcust.csv\tethusd.csv  lymbtc.csv\tqshusd.csv  udcust.csv\tzecbtc.csv\n",
            "btcxch.csv\tethust.csv  lymeth.csv\tqtmbtc.csv  ufreth.csv\tzecusd.csv\n",
            "btgbtc.csv\tetpbtc.csv  lymusd.csv\tqtmeth.csv  ufrusd.csv\tzilbtc.csv\n",
            "btgusd.csv\tetpeth.csv  maneth.csv\tqtmusd.csv  uosbtc.csv\tzileth.csv\n",
            "bttbtc.csv\tetpusd.csv  manusd.csv\trbtbtc.csv  uosusd.csv\tzilusd.csv\n",
            "bttusd.csv\teuseth.csv  mgoeth.csv\trbtusd.csv  uskbtc.csv\tzrxbtc.csv\n",
            "cbtbtc.csv\teususd.csv  mgousd.csv\trcnbtc.csv  uskeos.csv\tzrxdai.csv\n",
            "cbteth.csv\teuteur.csv  mitbtc.csv\trcneth.csv  usketh.csv\tzrxeth.csv\n",
            "cbtusd.csv\teutusd.csv  miteth.csv\trcnusd.csv  uskusd.csv\tzrxusd.csv\n",
            "clobtc.csv\tevtusd.csv  mitusd.csv\trdnbtc.csv  uskust.csv\n",
            "clousd.csv\tfoaeth.csv  mkrbtc.csv\trdneth.csv  ustusd.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy1-pSd3K8ex",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Weitere Frameworkinstallation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ig7hNjcLpoH",
        "colab_type": "code",
        "outputId": "f191dfa2-725e-48b6-9cdc-ab2171d0b89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# jdk\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# tree\n",
        "!apt-get install tree\n",
        "\n",
        "# spark-package\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "\n",
        "# findspark\n",
        "!pip install findspark\n",
        "\n",
        "# numpy\n",
        "!pip install numpy\n",
        "\n",
        "# timeseries library\n",
        "!pip install ts ts-flint"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (1,155 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 135143 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting findspark\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-1.3.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Collecting ts\n",
            "  Downloading https://files.pythonhosted.org/packages/26/dd/76551999e37032441d1190d7df27c30fc75adbb203dacb91a2a72e8b7202/ts-0.5.1.tar.gz\n",
            "Collecting ts-flint\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/f9/afab3946adefd527ca376aa71fdb6cd1ff787940c3f2467de382a8697c9d/ts_flint-0.6.0-py3-none-any.whl\n",
            "Building wheels for collected packages: ts\n",
            "  Building wheel for ts (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ts: filename=ts-0.5.1-cp36-none-any.whl size=14367 sha256=a216398a2c96a27521d0aa307f072d28fec83697fd3734b68836cbd40c859541\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/63/72/2341c4dfe0ade30e93b7e2a924abb7149ee245384fd3b9a9e4\n",
            "Successfully built ts\n",
            "Installing collected packages: ts, ts-flint\n",
            "Successfully installed ts-0.5.1 ts-flint-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qS6K26FmN1B",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGZaKQC5MlEN",
        "colab_type": "code",
        "outputId": "2444ea87-ea0c-41a6-bae7-943dd73b6ade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Für Pfade und andere OS-Funktionalität\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print(\">> Colab verbunden mit einer TPU\")\n",
        "else:\n",
        "  print(\">> Colab nicht verbunden mit einer TPU\")\n",
        "\n",
        "# Spark und Spark-SQL\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import input_file_name, col, collect_list, concat_ws, udf\n",
        "from pyspark.sql.types import DoubleType, StringType\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialisierung Spark und Spark-SQL\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "# Für einfache Weiterverarbeitung des Datenframes (z.B. Visualisierung mit Plotty)\n",
        "import pandas as pd\n",
        "from pandas import Timestamp\n",
        "\n",
        "# Für einfaches Handeln von Arrays aus Pandas-Datenframes\n",
        "import numpy\n",
        "\n",
        "# Visualisierung als Diagramm\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Für Darstellung von Datenframes\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Für File-Download\n",
        "from google.colab import files\n",
        "\n",
        "# Um Zip-Datein zu erstellen\n",
        "import shutil"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Colab verbunden mit einer TPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnoOYnD4M92t",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Anwendung PPO-Indikator\n",
        "---\n",
        "\n",
        "*PPO Implementation basierend auf: [Investopedia.com](https://www.investopedia.com/terms/p/ppo.asp#formula-and-calculation-for-ppo)*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We7RVeZaPoS3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "Console_Preview_Limit = 5  #@param {type: \"slider\", min: 1, max: 100}\n",
        "#@markdown 'Git' enstpricht den dem zuvor geladen Repository 'ABDA2019':\n",
        "Datenframe_Von = \"Git\" #@param [\"Git\", \"Pfad\", \"Sample: Intel (requieres './intc.csv')\"] {allow-input: false}\n",
        "#@markdown Nur benutzt wenn 'Datenframe_Von' auf 'Git' gesetzt ist: \n",
        "Filenamen_Zu_Laden = '*.csv'  #@param {type: \"string\"}\n",
        "#@markdown Nur benutzt wenn 'Datenframe_Von' auf 'Pfad' gesetzt ist:\n",
        "Filepfad_Zu_Laden = '*.csv'  #@param {type: \"string\"}\n",
        "Filenamen_Stellen_Von_Filepfad = 0  #@param {type: \"integer\"}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6I_jTzusuK6",
        "colab_type": "text"
      },
      "source": [
        "*Metadaten-Aggregation*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIWo98Hg1t1-",
        "colab_type": "code",
        "outputId": "1c34cba6-851a-4768-afd2-a7916ab7fc98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "#\n",
        "# Intialen Datenframe bilden\n",
        "#\n",
        "\n",
        "if Datenframe_Von == \"Git\":\n",
        "  df_spark = spark.read.csv('./ABDA2019/testdaten/cryptominuteresolution/' + Filenamen_Zu_Laden, inferSchema = True, header = True)\n",
        "  if Filenamen_Zu_Laden == '*.csv':\n",
        "    start = 57\n",
        "  else:\n",
        "    start = 59\n",
        "  end = 6\n",
        "\n",
        "elif Datenframe_Von == \"Pfad\":\n",
        "  df_spark = spark.read.csv( Filepfad_Zu_Laden, inferSchema = True, header = True)\n",
        "  start = Filepfad_Zu_Laden.count + 15 - Filenamen_Stellen_Von_Filepfad - 4\n",
        "  end = Filenamen_Stellen_Von_Filepfad\n",
        "\n",
        "elif Datenframe_Von == \"Sample: Intel (requieres './intc.csv')\":\n",
        "  df_spark = spark.read.csv('./intc.csv', inferSchema = True, header = True)\n",
        "  start = 17\n",
        "  end = 4\n",
        "\n",
        "df_spark = df_spark \\\n",
        "  .withColumn('filepath', input_file_name()) \\\n",
        "  .withColumn('filename', (input_file_name()[start:end]) ) \\\n",
        "  .withColumn('timestamp', df_spark['time']/1000) \\\n",
        "  .withColumn('date', (df_spark['time']/1000).cast('timestamp'))\n",
        "\n",
        "print(\"Type:\", type(df_spark), \"\\n\")\n",
        "print(\"Schema: \", end = '')\n",
        "df_spark.printSchema()\n",
        "df_spark_total_count = df_spark.count()\n",
        "print(\"Available rows:\", df_spark_total_count, \"\\n\\nDaten-Preview:\")\n",
        "df_spark.show(Console_Preview_Limit, False)\n",
        "\n",
        "# Erstellen der SQL-Tabelle \n",
        "df_spark.createOrReplaceTempView(\"base_data\")\n",
        "\n",
        "df_spark_informationView = spark.sql(\"SELECT filename, count(*), min(date), max(date)  FROM base_data group by filename\")\n",
        "print(\"Geladene Daten:\")\n",
        "df_spark_informationView.show(Console_Preview_Limit, False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type: <class 'pyspark.sql.dataframe.DataFrame'> \n",
            "\n",
            "Schema: root\n",
            " |-- time: long (nullable = true)\n",
            " |-- open: double (nullable = true)\n",
            " |-- close: double (nullable = true)\n",
            " |-- high: double (nullable = true)\n",
            " |-- low: double (nullable = true)\n",
            " |-- volume: double (nullable = true)\n",
            " |-- filepath: string (nullable = false)\n",
            " |-- filename: string (nullable = false)\n",
            " |-- timestamp: double (nullable = true)\n",
            " |-- date: timestamp (nullable = true)\n",
            "\n",
            "Available rows: 33909757 \n",
            "\n",
            "Daten-Preview:\n",
            "+-------------+-----+-----+-----+-----+-----------+------------------------------------------------------------------+--------+------------+-------------------+\n",
            "|time         |open |close|high |low  |volume     |filepath                                                          |filename|timestamp   |date               |\n",
            "+-------------+-----+-----+-----+-----+-----------+------------------------------------------------------------------+--------+------------+-------------------+\n",
            "|1364774820000|93.25|93.3 |93.3 |93.25|93.3       |file:/content/ABDA2019/testdaten/cryptominuteresolution/btcusd.csv|btcusd  |1.36477482E9|2013-04-01 00:07:00|\n",
            "|1364774880000|100.0|100.0|100.0|100.0|93.3       |file:/content/ABDA2019/testdaten/cryptominuteresolution/btcusd.csv|btcusd  |1.36477488E9|2013-04-01 00:08:00|\n",
            "|1364774940000|93.3 |93.3 |93.3 |93.3 |33.67686227|file:/content/ABDA2019/testdaten/cryptominuteresolution/btcusd.csv|btcusd  |1.36477494E9|2013-04-01 00:09:00|\n",
            "|1364775060000|93.35|93.47|93.47|93.35|20.0       |file:/content/ABDA2019/testdaten/cryptominuteresolution/btcusd.csv|btcusd  |1.36477506E9|2013-04-01 00:11:00|\n",
            "|1364775120000|93.47|93.47|93.47|93.47|2.02162704 |file:/content/ABDA2019/testdaten/cryptominuteresolution/btcusd.csv|btcusd  |1.36477512E9|2013-04-01 00:12:00|\n",
            "+-------------+-----+-----+-----+-----+-----------+------------------------------------------------------------------+--------+------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Geladene Daten:\n",
            "+--------+--------+-------------------+-------------------+\n",
            "|filename|count(1)|min(date)          |max(date)          |\n",
            "+--------+--------+-------------------+-------------------+\n",
            "|dtxusd  |9159    |2019-08-16 17:33:00|2019-10-01 23:35:00|\n",
            "|neceth  |146     |2019-06-30 15:13:00|2019-09-30 03:55:00|\n",
            "|omnusd  |691     |2018-10-30 20:25:00|2019-10-01 03:05:00|\n",
            "|btcusd  |2457265 |2013-04-01 00:07:00|2019-10-01 18:46:00|\n",
            "|yywusd  |119465  |2017-12-01 17:18:00|2019-10-01 19:47:00|\n",
            "+--------+--------+-------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7FZUcVKr7LV",
        "colab_type": "text"
      },
      "source": [
        "*Tag-Filter*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eS-opK4lKmF",
        "colab_type": "code",
        "outputId": "9e098c4b-ca29-44db-c363-f87fd1dfa6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "#\n",
        "# PRO-TAG-FILTER\n",
        "#\n",
        "\n",
        "base_data_dayfiltered = spark.sql(\n",
        "  \"\"\"SELECT d.* FROM base_data d  \n",
        "     where  (d.date, filename) in (select max(m.date), filename from base_data m group by date_format(m.date, \"y-M-d\"), filename)\n",
        "  \"\"\")\n",
        "\n",
        "# Erstellen der SQL-Tabelle\n",
        "base_data_dayfiltered.createOrReplaceTempView(\"base_data_dayfiltered\")\n",
        "\n",
        "base_data_dayfiltered_total_count = base_data_dayfiltered.count()\n",
        "#base_data_dayfiltered.show(Console_Preview_Limit, False)\n",
        "\n",
        "base_data_dayfiltered_informationView = spark.sql(\"SELECT filename, count(*), min(date), max(date)  FROM base_data_dayfiltered group by filename\")\n",
        "print(\"Available rows:\", base_data_dayfiltered_total_count, \"(lost\",(df_spark_total_count-base_data_dayfiltered_total_count), \"rows)\", \"\\n\\nVerfügbare Daten pro Tag:\")\n",
        "base_data_dayfiltered_informationView.show(Console_Preview_Limit)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available rows: 156579 (lost 33753178 rows) \n",
            "\n",
            "Verfügbare Daten pro Tag:\n",
            "+--------+--------+-------------------+-------------------+\n",
            "|filename|count(1)|          min(date)|          max(date)|\n",
            "+--------+--------+-------------------+-------------------+\n",
            "|  neceth|      18|2019-06-30 15:13:00|2019-09-30 03:55:00|\n",
            "|  dtxusd|      43|2019-08-16 23:55:00|2019-10-01 23:35:00|\n",
            "|  omnusd|     167|2018-10-30 23:44:00|2019-10-01 03:05:00|\n",
            "|  btcusd|    2211|2013-04-01 23:58:00|2019-10-01 18:46:00|\n",
            "|  yywusd|     664|2017-12-01 23:59:00|2019-10-01 19:47:00|\n",
            "|  sngeth|     325|2018-01-24 23:11:00|2019-09-26 21:04:00|\n",
            "|  neojpy|     541|2018-03-29 22:48:00|2019-10-01 22:36:00|\n",
            "|  dtabtc|     225|2018-07-12 23:41:00|2019-10-01 10:04:00|\n",
            "|  euteur|      78|2018-11-27 16:32:00|2019-10-01 07:48:00|\n",
            "|  gotusd|     359|2018-09-06 23:52:00|2019-10-01 14:06:00|\n",
            "|  pasusd|     253|2019-01-17 23:09:00|2019-10-01 22:17:00|\n",
            "|  tnbusd|     615|2018-01-08 23:59:00|2019-10-01 19:00:00|\n",
            "|  udcusd|     285|2018-12-04 23:15:00|2019-10-01 17:39:00|\n",
            "|  vldeth|     129|2018-11-19 23:22:00|2019-10-01 19:49:00|\n",
            "|  csxusd|     138|2018-10-18 23:33:00|2019-09-30 17:04:00|\n",
            "|  mitusd|     432|2018-04-26 23:42:00|2019-09-30 11:22:00|\n",
            "|  btceur|     856|2017-05-19 08:16:00|2019-10-01 19:26:00|\n",
            "|  wprusd|     253|2018-07-26 23:45:00|2019-09-30 18:52:00|\n",
            "|  atobtc|     160|2019-04-25 23:15:00|2019-10-01 23:36:00|\n",
            "|  enjusd|     248|2018-11-19 19:52:00|2019-09-29 09:45:00|\n",
            "+--------+--------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fw8u3lUr-s9",
        "colab_type": "text"
      },
      "source": [
        "*EMA-UDF*\n",
        "---\n",
        "---\n",
        "Berrechnet nach: [tradistats.com](https://tradistats.com/exponentieller-gleitender-durchschnitt/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpCbfCCjguIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# EMA-UDF\n",
        "#\n",
        "\n",
        "# see https://tradistats.com/exponentieller-gleitender-durchschnitt/\n",
        "def ema(ar):\n",
        "    if len(ar) > 0:\n",
        "       SF  = 2/ (len(ar)+1)\n",
        "       SFi = 1 - SF\n",
        "       my_ema = ar[0]\n",
        "       for i in ar:\n",
        "           my_ema = (i * SF) + (my_ema * SFi)\n",
        "    return my_ema\n",
        "\n",
        "ema_udf = udf(ema, DoubleType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8VN59M0sSbi",
        "colab_type": "text"
      },
      "source": [
        "*Windows*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEFHfQ4SjDnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# EMA-WINDOWS\n",
        "#\n",
        "win26 = (Window\n",
        "    .partitionBy(\"filename\")\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-25, 0))\n",
        "\n",
        "win12 = (Window\n",
        "    .partitionBy(\"filename\")\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-11, 0))\n",
        "\n",
        "win9 = (Window\n",
        "    .partitionBy(\"filename\")\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-8, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWm-U6FdsXKm",
        "colab_type": "text"
      },
      "source": [
        "*PPO- & Signal-Berechnung*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PVMjroXh1VE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b41399ea-2739-42f9-d9ff-11d02d00502a"
      },
      "source": [
        "#\n",
        "# EMA_26 && EMA_12 CALCULATION\n",
        "#\n",
        "\n",
        "base_data_tmp_ema_1 = base_data_dayfiltered \\\n",
        "  .withColumn('win26_close_list', collect_list('close').over(win26)) \\\n",
        "  .withColumn('win12_close_list', collect_list('close').over(win12))\n",
        "\n",
        "base_data_tmp_ema_2 = base_data_tmp_ema_1.select(\n",
        "  \"*\",\n",
        "  ema_udf(base_data_tmp_ema_1[\"win26_close_list\"]).alias(\"EMA26\"),\n",
        "  ema_udf(base_data_tmp_ema_1[\"win12_close_list\"]).alias(\"EMA12\")\n",
        ")\n",
        "\n",
        "base_data_tmp_ema_2.createOrReplaceTempView(\"base_data_tmp_ema_2\")\n",
        "base_data_tmp_ema_2.show(Console_Preview_Limit, False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+\n",
            "|time         |open  |close |high  |low   |volume|filepath                                                          |filename|timestamp   |date               |win26_close_list                    |win12_close_list                    |EMA26             |EMA12             |\n",
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+\n",
            "|1565999700000|5.005 |5.005 |5.005 |5.005 |0.0012|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.5659997E9 |2019-08-16 23:55:00|[5.005]                             |[5.005]                             |5.005             |5.005             |\n",
            "|1566086280000|4.881 |4.881 |4.881 |4.881 |0.6634|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56608628E9|2019-08-17 23:58:00|[5.005, 4.881]                      |[5.005, 4.881]                      |4.9223333333333334|4.9223333333333334|\n",
            "|1566172680000|4.9101|4.9101|4.9101|4.9101|0.6373|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56617268E9|2019-08-18 23:58:00|[5.005, 4.881, 4.9101]              |[5.005, 4.881, 4.9101]              |4.92655           |4.92655           |\n",
            "|1566258840000|4.901 |4.901 |4.901 |4.901 |0.6431|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56625884E9|2019-08-19 23:54:00|[5.005, 4.881, 4.9101, 4.901]       |[5.005, 4.881, 4.9101, 4.901]       |4.922768          |4.922768          |\n",
            "|1566345300000|4.875 |4.875 |4.875 |4.875 |0.6432|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.5663453E9 |2019-08-20 23:55:00|[5.005, 4.881, 4.9101, 4.901, 4.875]|[5.005, 4.881, 4.9101, 4.901, 4.875]|4.91224938271605  |4.91224938271605  |\n",
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRu2qFdgqCjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "bc91ed36-f4bf-4eaf-a751-ae376af62aa2"
      },
      "source": [
        "#\n",
        "# PPO BERECHNUNG\n",
        "#\n",
        "\n",
        "base_data_tmp_ema_3 = spark.sql(\n",
        "  \" select *, (((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))   \" \\\n",
        "  \" FROM base_data_tmp_ema_2 \" \\\n",
        "  \" order by filename asc, date desc \"\n",
        ")\n",
        "\n",
        "#\n",
        "# SIGNAL BERECHNUNG\n",
        "#\n",
        "base_data_tmp_ema_4 = base_data_tmp_ema_3 \\\n",
        "  .withColumn('win9_ema12sub26_list', collect_list('(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))').over(win9))\n",
        "\n",
        "\n",
        "base_data_tmp_ema_5 = base_data_tmp_ema_4.select(\n",
        "  \"*\",\n",
        "  base_data_tmp_ema_4['(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))'].alias(\"PPO\"),\n",
        "  ema_udf(base_data_tmp_ema_4[\"win9_ema12sub26_list\"]).alias(\"SIGNAL\")\n",
        ")\n",
        "\n",
        "base_data_tmp_ema_5.createOrReplaceTempView(\"base_data_tmp_ema_5\")\n",
        "base_data_tmp_ema_5.show(Console_Preview_Limit, False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+-------------------------------------------------+-------------------------+---+------+\n",
            "|time         |open  |close |high  |low   |volume|filepath                                                          |filename|timestamp   |date               |win26_close_list                    |win12_close_list                    |EMA26             |EMA12             |(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))|win9_ema12sub26_list     |PPO|SIGNAL|\n",
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+-------------------------------------------------+-------------------------+---+------+\n",
            "|1565999700000|5.005 |5.005 |5.005 |5.005 |0.0012|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.5659997E9 |2019-08-16 23:55:00|[5.005]                             |[5.005]                             |5.005             |5.005             |0.0                                              |[0.0]                    |0.0|0.0   |\n",
            "|1566086280000|4.881 |4.881 |4.881 |4.881 |0.6634|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56608628E9|2019-08-17 23:58:00|[5.005, 4.881]                      |[5.005, 4.881]                      |4.9223333333333334|4.9223333333333334|0.0                                              |[0.0, 0.0]               |0.0|0.0   |\n",
            "|1566172680000|4.9101|4.9101|4.9101|4.9101|0.6373|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56617268E9|2019-08-18 23:58:00|[5.005, 4.881, 4.9101]              |[5.005, 4.881, 4.9101]              |4.92655           |4.92655           |0.0                                              |[0.0, 0.0, 0.0]          |0.0|0.0   |\n",
            "|1566258840000|4.901 |4.901 |4.901 |4.901 |0.6431|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56625884E9|2019-08-19 23:54:00|[5.005, 4.881, 4.9101, 4.901]       |[5.005, 4.881, 4.9101, 4.901]       |4.922768          |4.922768          |0.0                                              |[0.0, 0.0, 0.0, 0.0]     |0.0|0.0   |\n",
            "|1566345300000|4.875 |4.875 |4.875 |4.875 |0.6432|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.5663453E9 |2019-08-20 23:55:00|[5.005, 4.881, 4.9101, 4.901, 4.875]|[5.005, 4.881, 4.9101, 4.901, 4.875]|4.91224938271605  |4.91224938271605  |0.0                                              |[0.0, 0.0, 0.0, 0.0, 0.0]|0.0|0.0   |\n",
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+-------------------------------------------------+-------------------------+---+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYMSR0WGsgUv",
        "colab_type": "text"
      },
      "source": [
        "*PPO_Histogram-Berechnung*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOvlkuUBwlmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b9fb7c25-53f8-4b40-f863-e7eec1d24576"
      },
      "source": [
        "#\n",
        "# PPO-HISTOGRAM BERECHNUNG\n",
        "#\n",
        "\n",
        "base_data_tmp_ema_6 = spark.sql(\n",
        "  \" select *, (PPO - SIGNAL)   \" \\\n",
        "  \" FROM base_data_tmp_ema_5 \" \\\n",
        "  \" order by filename asc, date desc \"\n",
        ")\n",
        "\n",
        "base_data_ema = base_data_tmp_ema_6.select(\n",
        "  \"*\",\n",
        "  base_data_tmp_ema_6['(PPO - SIGNAL)'].alias(\"PPO_HISTOGRAM\")\n",
        ")\n",
        "\n",
        "base_data_ema.createOrReplaceTempView(\"base_data_ema\")\n",
        "base_data_ema.show(Console_Preview_Limit, False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+--------+--------+--------+--------+-------------+------------------------------------------------------------------+--------+------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------+-------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+--------------------+\n",
            "|time         |open    |close   |high    |low     |volume       |filepath                                                          |filename|timestamp   |date               |win26_close_list                                                                                                                                                                                                                                                             |win12_close_list                                                                                                                     |EMA26               |EMA12                |(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))|win9_ema12sub26_list                                                                                                                                                                 |PPO               |SIGNAL             |(PPO - SIGNAL)      |PPO_HISTOGRAM       |\n",
            "+-------------+--------+--------+--------+--------+-------------+------------------------------------------------------------------+--------+------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------+-------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+--------------------+\n",
            "|1569920460000|5.929E-5|5.929E-5|5.929E-5|5.929E-5|467.19       |file:/content/ABDA2019/testdaten/cryptominuteresolution/abseth.csv|abseth  |1.56992046E9|2019-10-01 09:01:00|[6.844E-5, 6.7E-5, 6.518E-5, 6.887E-5, 7.49E-5, 7.241E-5, 7.202E-5, 6.917E-5, 6.878E-5, 6.757E-5, 6.75E-5, 6.846E-5, 6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5, 6.201E-5, 5.909E-5, 5.929E-5]|[5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5, 6.201E-5, 5.909E-5, 5.929E-5]|6.261377441678071E-5|5.916416186274444E-5 |-5.50935091546208                                |[-3.972939476751633, -5.220550033683788, -5.40133061659197, -5.937107649429317, -6.413791605354604, -5.009073651748249, -5.322072664881107, -5.416023859291335, -5.50935091546208]   |-5.50935091546208 |-5.24580019792635  |-0.2635507175357299 |-0.2635507175357299 |\n",
            "|1569870180000|5.908E-5|5.909E-5|5.909E-5|5.908E-5|1834.25      |file:/content/ABDA2019/testdaten/cryptominuteresolution/abseth.csv|abseth  |1.56987018E9|2019-09-30 19:03:00|[6.935E-5, 6.844E-5, 6.7E-5, 6.518E-5, 6.887E-5, 7.49E-5, 7.241E-5, 7.202E-5, 6.917E-5, 6.878E-5, 6.757E-5, 6.75E-5, 6.846E-5, 6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5, 6.201E-5, 5.909E-5]|[6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5, 6.201E-5, 5.909E-5]|6.301255266359391E-5|5.959977777698515E-5 |-5.416023859291335                               |[-3.867755509120363, -3.972939476751633, -5.220550033683788, -5.40133061659197, -5.937107649429317, -6.413791605354604, -5.009073651748249, -5.322072664881107, -5.416023859291335]  |-5.416023859291335|-5.16226557709555  |-0.25375828219578533|-0.25375828219578533|\n",
            "|1569794760000|6.199E-5|6.201E-5|6.201E-5|6.199E-5|10166.3556   |file:/content/ABDA2019/testdaten/cryptominuteresolution/abseth.csv|abseth  |1.56979476E9|2019-09-29 22:06:00|[7.213E-5, 6.935E-5, 6.844E-5, 6.7E-5, 6.518E-5, 6.887E-5, 7.49E-5, 7.241E-5, 7.202E-5, 6.917E-5, 6.878E-5, 6.757E-5, 6.75E-5, 6.846E-5, 6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5, 6.201E-5]|[6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5, 6.201E-5]|6.373228665233935E-5|6.0340408045711527E-5|-5.322072664881107                               |[-1.9579938997318376, -3.867755509120363, -3.972939476751633, -5.220550033683788, -5.40133061659197, -5.937107649429317, -6.413791605354604, -5.009073651748249, -5.322072664881107] |-5.322072664881107|-4.7784211762544135|-0.5436514886266934 |-0.5436514886266934 |\n",
            "|1569668880000|6.105E-5|6.105E-5|6.105E-5|6.105E-5|362.55       |file:/content/ABDA2019/testdaten/cryptominuteresolution/abseth.csv|abseth  |1.56966888E9|2019-09-28 11:08:00|[7.098E-5, 7.213E-5, 6.935E-5, 6.844E-5, 6.7E-5, 6.518E-5, 6.887E-5, 7.49E-5, 7.241E-5, 7.202E-5, 6.917E-5, 6.878E-5, 6.757E-5, 6.75E-5, 6.846E-5, 6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5]|[6.846E-5, 6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5, 6.105E-5]|6.370214899387665E-5|6.0511261433026964E-5|-5.009073651748249                               |[-3.1472166315684835, -1.9579938997318376, -3.867755509120363, -3.972939476751633, -5.220550033683788, -5.40133061659197, -5.937107649429317, -6.413791605354604, -5.009073651748249]|-5.009073651748249|-4.842026770539075 |-0.1670468812091741 |-0.1670468812091741 |\n",
            "|1569627900000|5.926E-5|5.926E-5|5.926E-5|5.926E-5|1720.65162642|file:/content/ABDA2019/testdaten/cryptominuteresolution/abseth.csv|abseth  |1.5696279E9 |2019-09-27 23:45:00|[7.424E-5, 7.098E-5, 7.213E-5, 6.935E-5, 6.844E-5, 6.7E-5, 6.518E-5, 6.887E-5, 7.49E-5, 7.241E-5, 7.202E-5, 6.917E-5, 6.878E-5, 6.757E-5, 6.75E-5, 6.846E-5, 6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5]|[6.75E-5, 6.846E-5, 6.548E-5, 6.141E-5, 5.853E-5, 5.853E-5, 5.625E-5, 5.849E-5, 5.6399999999999995E-5, 5.866E-5, 5.767E-5, 5.926E-5] |6.439033928340286E-5|6.026047710778462E-5 |-6.413791605354604                               |[-3.498220886308628, -3.1472166315684835, -1.9579938997318376, -3.867755509120363, -3.972939476751633, -5.220550033683788, -5.40133061659197, -5.937107649429317, -6.413791605354604]|-6.413791605354604|-4.859153792223726 |-1.5546378131308778 |-1.5546378131308778 |\n",
            "+-------------+--------+--------+--------+--------+-------------+------------------------------------------------------------------+--------+------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------+-------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGPOwO7c8VeE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Analyse \n",
        "---\n",
        "*PPO-Analyse ebenfalls basierend auf: [Investopedia.com](https://www.investopedia.com/terms/p/ppo.asp#what-the-indicator-tells-you)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAG4xnpZrakH",
        "colab_type": "text"
      },
      "source": [
        "*Windows*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN_jFOyG9dcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Analyse-Windows\n",
        "#\n",
        "\n",
        "win2 = (Window\n",
        "    .partitionBy(\"filename\")\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-1, 0))\n",
        "\n",
        "win3 = (Window\n",
        "    .partitionBy(\"filename\")\n",
        "    .orderBy(\"date\")\n",
        "    .rowsBetween(-2, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS8w0v_6rhd0",
        "colab_type": "text"
      },
      "source": [
        "*Funktionen*\n",
        "---\n",
        "---\n",
        "**→ ppo_trend_analysis** :\n",
        "\n",
        "Auf den 'PPO' angewendet (window mind. 1):\n",
        "\n",
        "> *When the PPO is above zero, that helps confirm an uptrend since the short-term EMA is above the longer-term EMA. When the PPO is below zero, the short-term EMA is below the longer-term EMA, which is an indication of a downtrend.*\n",
        "\n",
        "\n",
        "**→ ppo_crossover_signal_analysis** :\n",
        "\n",
        "Auf den 'PPO_HISTOGRAM' angewendet (window mind. 2):\n",
        "> *The indicator generates a buy signal when the PPO line crosses above the signal line from below, and a sell signal occurs when the PPO line crosses  below the signal from above.*\n",
        "\n",
        "Auf den 'PPO' angewendet (window mind. 2):\n",
        "> *Centerline crossovers also generate trading signals. Traders consider a move from below to above the centerline as bullish, and a move from above to below the centerline as bearish.*\n",
        "\n",
        "**→ ppo_technical_divergence_analysis:**\n",
        "\n",
        "Auf den 'close' und 'PPO' (window mind. 2; ggf. 'close' mit 'open' tauschen):\n",
        "> *Traders can also use the PPO to look for technical divergence between the indicator and price. For example, if the price of an asset makes a higher high, but the indicator makes a lower high, it may indicate the upward momentum is subsiding. Conversely, if an asset's price makes a lower low, but the indicator makes a higher low, it could suggest that the bears are losing their traction and the price could head higher soon.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUqIGok9ltS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Analyse-UDF\n",
        "#\n",
        "\n",
        "def ppo_trend_analysis(ar):\n",
        "    output = \"\"\n",
        "    count = len(ar)\n",
        "    if count > 0:\n",
        "      # Einzelabgleich\n",
        "      if count == 1:\n",
        "        current = ar[0]\n",
        "        if current > 0:\n",
        "          output = \"▲ Aufwärtstrend\" # \"Aufwärtstrend\"\n",
        "        elif current < 0:\n",
        "          output = \"▼ Abwärtstrend\" # \"Abwärtstrend\"\n",
        "\n",
        "      # Vergleich mit den davorherigenden; ggf. mit mehreren vorherigen\n",
        "      elif count > 1:\n",
        "        last_position = count - 1\n",
        "        next_to_last_position = last_position - 1\n",
        "\n",
        "        last = ar[last_position] \n",
        "        next_to_last = ar[next_to_last_position]\n",
        "\n",
        "        if last > 0 and next_to_last > 0:\n",
        "          output = \"▲ Aufwärtstrend\"\n",
        "        elif last < 0 and next_to_last > 0:\n",
        "          output = \"↘ neuer Abwärtstrend\"\n",
        "        elif last < 0 and next_to_last < 0:\n",
        "          output = \"▼ Abwärtstrend\"\n",
        "        elif last > 0 and next_to_last < 0:\n",
        "          output = \"↗ neuer Aufwärtstrend\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def ppo_crossover_signal_analysis(ar):\n",
        "    output = \"\"\n",
        "    count = len(ar)\n",
        "    # Vergleich des letzten Crossovers\n",
        "    if count > 1:\n",
        "        last_position = count - 1\n",
        "        next_to_last_position = last_position - 1\n",
        "        \n",
        "        last = ar[last_position] \n",
        "        next_to_last = ar[next_to_last_position]\n",
        "\n",
        "        if last <= 0 and next_to_last > 0:\n",
        "          output = \"Verkauf-Signal\"\n",
        "        elif last >= 0 and next_to_last < 0:\n",
        "          output = \"Kauf-Signal\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def ppo_technical_divergence_analysis(close_ar, ppo_ar):\n",
        "    output = \"\"\n",
        "    count_close = len(close_ar)\n",
        "    count_ppo = len(ppo_ar)\n",
        "    \n",
        "    # Arrays müssen selbe Größe haben\n",
        "    if count_close == count_ppo and count_close > 1:\n",
        "      # delta berechnen\n",
        "      pointer = count_close - 1\n",
        "      delta_close = close_ar[pointer]\n",
        "      delta_ppo = ppo_ar[pointer]\n",
        "      pointer -= 1\n",
        "\n",
        "      while pointer >= 0:\n",
        "        delta_close -= close_ar[pointer]\n",
        "        delta_ppo -= ppo_ar[pointer]\n",
        "        pointer -= 1\n",
        "\n",
        "      # steigung berechnen\n",
        "      div_close = delta_close / count_close\n",
        "      div_ppo = delta_ppo / count_ppo\n",
        "\n",
        "      if div_close > div_ppo:\n",
        "        output = \"↧ Abflachend\"\n",
        "      else:\n",
        "        output = \"↥ Wachsend\"\n",
        "\n",
        "    return output\n",
        "\n",
        "ppo_trend_analysis_udf = udf(ppo_trend_analysis, StringType())\n",
        "ppo_crossover_signal_analysis_udf = udf(ppo_crossover_signal_analysis, StringType())\n",
        "ppo_technical_divergence_analysis_udf = udf(ppo_technical_divergence_analysis, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwS8Vh_kro1G",
        "colab_type": "text"
      },
      "source": [
        "*Aggregation*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkRAgL94J5Px",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "18054ce4-d247-4aad-a42f-2e23d1659fda"
      },
      "source": [
        "#\n",
        "# SIGNAL ANALYSIS\n",
        "#\n",
        "\n",
        "base_data_ema_tmp_eval = base_data_ema \\\n",
        "  .withColumn('win2_ppo_list', collect_list('PPO').over(win2)) \\\n",
        "  .withColumn('win2_ppoh_list', collect_list('PPO_Histogram').over(win2)) \\\n",
        "  .withColumn('win2_close_list', collect_list('close').over(win2))\n",
        "\n",
        "base_data_ema_eval_result = base_data_ema_tmp_eval.select(\n",
        "  \"*\",\n",
        "  ppo_trend_analysis_udf(base_data_ema_tmp_eval[\"win2_ppo_list\"]).alias(\"TREND\"),\n",
        "  ppo_crossover_signal_analysis_udf(base_data_ema_tmp_eval[\"win2_ppo_list\"]).alias(\"WEAK_SIGNAL\"),\n",
        "  ppo_crossover_signal_analysis_udf(base_data_ema_tmp_eval[\"win2_ppoh_list\"]).alias(\"STRONG_SIGNAL\"),\n",
        "  ppo_technical_divergence_analysis_udf(base_data_ema_tmp_eval[\"win2_close_list\"], base_data_ema_tmp_eval[\"win2_ppo_list\"]).alias(\"DIVERGENCE\")\n",
        ")\n",
        "\n",
        "base_data_ema_eval_result.createOrReplaceTempView(\"base_data_ema_eval_result\")\n",
        "base_data_ema_eval_result.show(Console_Preview_Limit, False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+-------------------------------------------------+-------------------------+---+------+--------------+-------------+-------------+--------------+---------------+-----+-----------+-------------+------------+\n",
            "|time         |open  |close |high  |low   |volume|filepath                                                          |filename|timestamp   |date               |win26_close_list                    |win12_close_list                    |EMA26             |EMA12             |(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))|win9_ema12sub26_list     |PPO|SIGNAL|(PPO - SIGNAL)|PPO_HISTOGRAM|win2_ppo_list|win2_ppoh_list|win2_close_list|TREND|WEAK_SIGNAL|STRONG_SIGNAL|DIVERGENCE  |\n",
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+-------------------------------------------------+-------------------------+---+------+--------------+-------------+-------------+--------------+---------------+-----+-----------+-------------+------------+\n",
            "|1565999700000|5.005 |5.005 |5.005 |5.005 |0.0012|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.5659997E9 |2019-08-16 23:55:00|[5.005]                             |[5.005]                             |5.005             |5.005             |0.0                                              |[0.0]                    |0.0|0.0   |0.0           |0.0          |[0.0]        |[0.0]         |[5.005]        |     |           |             |            |\n",
            "|1566086280000|4.881 |4.881 |4.881 |4.881 |0.6634|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56608628E9|2019-08-17 23:58:00|[5.005, 4.881]                      |[5.005, 4.881]                      |4.9223333333333334|4.9223333333333334|0.0                                              |[0.0, 0.0]               |0.0|0.0   |0.0           |0.0          |[0.0, 0.0]   |[0.0, 0.0]    |[5.005, 4.881] |     |           |             |↥ Wachsend  |\n",
            "|1566172680000|4.9101|4.9101|4.9101|4.9101|0.6373|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56617268E9|2019-08-18 23:58:00|[5.005, 4.881, 4.9101]              |[5.005, 4.881, 4.9101]              |4.92655           |4.92655           |0.0                                              |[0.0, 0.0, 0.0]          |0.0|0.0   |0.0           |0.0          |[0.0, 0.0]   |[0.0, 0.0]    |[4.881, 4.9101]|     |           |             |↧ Abflachend|\n",
            "|1566258840000|4.901 |4.901 |4.901 |4.901 |0.6431|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.56625884E9|2019-08-19 23:54:00|[5.005, 4.881, 4.9101, 4.901]       |[5.005, 4.881, 4.9101, 4.901]       |4.922768          |4.922768          |0.0                                              |[0.0, 0.0, 0.0, 0.0]     |0.0|0.0   |0.0           |0.0          |[0.0, 0.0]   |[0.0, 0.0]    |[4.9101, 4.901]|     |           |             |↥ Wachsend  |\n",
            "|1566345300000|4.875 |4.875 |4.875 |4.875 |0.6432|file:/content/ABDA2019/testdaten/cryptominuteresolution/dtxusd.csv|dtxusd  |1.5663453E9 |2019-08-20 23:55:00|[5.005, 4.881, 4.9101, 4.901, 4.875]|[5.005, 4.881, 4.9101, 4.901, 4.875]|4.91224938271605  |4.91224938271605  |0.0                                              |[0.0, 0.0, 0.0, 0.0, 0.0]|0.0|0.0   |0.0           |0.0          |[0.0, 0.0]   |[0.0, 0.0]    |[4.901, 4.875] |     |           |             |↥ Wachsend  |\n",
            "+-------------+------+------+------+------+------+------------------------------------------------------------------+--------+------------+-------------------+------------------------------------+------------------------------------+------------------+------------------+-------------------------------------------------+-------------------------+---+------+--------------+-------------+-------------+--------------+---------------+-----+-----------+-------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCOrGQDoL53n",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Auswertung\n",
        "---\n",
        "\n",
        "*Für alle finalen Empfehlungen gilt eine Reichteweite von 0 (schwach) bis 3 (stark)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5DugOoQMQw1",
        "colab_type": "text"
      },
      "source": [
        "**Halten**\n",
        "---\n",
        "---\n",
        "*   **H0:** \n",
        "  * ▼ Abwärtstrend && ↧ Abflachend\n",
        "  * ▼ Abwärtstrend && ↥ Wachsend\n",
        "  * ↘ neuer Abwärtstrend && ↧ Abflachend\n",
        "*   **H1:**\n",
        "  * ↗ neuer Aufwärtstrend && ↧ Abflachend\n",
        "*   **H2:**\n",
        "  * ▲ Aufwärtstrend && ↧ Abflachend\n",
        "  * ↘ neuer Abwärtstrend && ↥ Wachsend\n",
        "*   **H3:**\n",
        "  * ▲ Aufwärtstrend && ↥ Wachsend\n",
        "  * ↗ neuer Aufwärtstrend && ↥ Wachsend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS-0YC6zq8Lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Halten-SQL-Statement\n",
        "#\n",
        "\n",
        "holdingCaseStatement = \"CASE WHEN (TREND = \\\"↗ neuer Aufwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"H1\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" = \\\"↧ Abflachend\\\") OR (TREND = \\\"↘ neuer Abwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"H2\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") OR (TREND = \\\"↗ neuer Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"H3\\\"\" +\"\\n\" \\\n",
        "\"ELSE \\\"H0\\\" END\" +\"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ibvDfPXqgXO",
        "colab_type": "text"
      },
      "source": [
        "**Kaufen**\n",
        "---\n",
        "---\n",
        "*   **K0:** \n",
        "  * ▼ Abwärtstrend && ↧ Abflachend\n",
        "  * ↘ neuer Abwärtstrend && ↧ Abflachend\n",
        "*   **K1:**\n",
        "  * ▼ Abwärtstrend && ↥ Wachsend\n",
        "  * ↘ neuer Abwärtstrend && ↥ Wachsend\n",
        "  * ↗ neuer Aufwärtstrend && ↧ Abflachend\n",
        "*   **K2:**\n",
        "  * ▲ Aufwärtstrend && ↧ Abflachend\n",
        "  * ↗ neuer Aufwärtstrend && ↥ Wachsend\n",
        "*   **K3:**\n",
        "  * ▲ Aufwärtstrend && ↥ Wachsend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkstKP8lq2xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Kaufen-SQL-Statement\n",
        "#\n",
        "\n",
        "buyingCaseStatement = \"CASE WHEN (TREND = \\\"▼ Abwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") OR (TREND = \\\"↘ neuer Abwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"K0\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") OR (TREND = \\\"↗ neuer Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"K2\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"K3\\\"\" +\"\\n\" \\\n",
        "\"ELSE \\\"K1\\\" END\" +\"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffbB0Dhqm38",
        "colab_type": "text"
      },
      "source": [
        "**Verkaufen**\n",
        "---\n",
        "---\n",
        "*   **V0:** \n",
        "  * ▲ Aufwärtstrend && ↥ Wachsend\n",
        "  * ▲ Aufwärtstrend && ↧ Abflachend\n",
        "*   **V1:**\n",
        "  * ↗ neuer Aufwärtstrend && ↥ Wachsend\n",
        "  * ↘ neuer Abwärtstrend && ↥ Wachsend\n",
        "*   **V2:**\n",
        "  * ▼ Abwärtstrend && ↥ Wachsend\n",
        "  * ↗ neuer Aufwärtstrend && ↧ Abflachend\n",
        "  * ↘ neuer Abwärtstrend && ↧ Abflachend\n",
        "*   **V3:**\n",
        "  * ▼ Abwärtstrend && ↧ Abflachend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAB7dievRveL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Verkaufen-SQL-Statement\n",
        "#\n",
        "\n",
        "sellingCaseStatement = \"CASE WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\")OR (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"V0\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"↗ neuer Aufwärtstrend\\\" = \\\"↥ Wachsend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") OR (TREND = \\\"↘ neuer Abwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"V1\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▼ Abwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"V3\\\" \" +\"\\n\" \\\n",
        "\"ELSE \\\"V2\\\" END\" +\"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Zg1CuVnB3_",
        "colab_type": "text"
      },
      "source": [
        "**Aggregation**\n",
        "---\n",
        "---\n",
        "*vgl. dazu Investopedia-Link (s.h. Sections-Header)*\n",
        "\n",
        "* Wenn beide Signale null sind ⇒ *halten*\n",
        "* Ansonsten 'Strong-' > 'Weaksignal', für jeweils beide gilt:\n",
        "  * KAUF-Signal ⇒ *kaufen*\n",
        "  * VERKAUF-Signal ⇒ *verkaufen*\n",
        "\n",
        "```\n",
        "+----------------+----------------+------------+\n",
        "|  WEAK_SIGNAL   | STRONG_SIGNAL  | EMPFEHLUNG |\n",
        "+----------------+----------------+------------+\n",
        "| null           | null           | HALTEN     |\n",
        "| Kauf-Signal    | null           | KAUFEN     |\n",
        "| Kauf-Signal    | Kauf-Signal    | KAUFEN     |\n",
        "| ...            | ...            | ...        |\n",
        "| Verkauf-Signal | null           | VERKAUFEN  |\n",
        "| Verkauf-Signal | Verkauf-Signal | VERKAUFEN  |\n",
        "| ...            | ...            | ...        |\n",
        "| Kauf-Signal    | Verkauf-Signal | VERKAUFEN  |\n",
        "| Verkauf-Signal | Kauf-Signal    | KAUFEN     |\n",
        "+----------------+----------------+------------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFoLl5e4VX3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Combiniertes-SQL-Statement\n",
        "#\n",
        "\n",
        "final_result_statement = \"SELECT *,\" \\\n",
        "\" CASE\" \\\n",
        "\"   WHEN (WEAK_SIGNAL = \\\"\\\" AND STRONG_SIGNAL = \\\"\\\") THEN \" + holdingCaseStatement + \\\n",
        "\"   WHEN (STRONG_SIGNAL = \\\"\\\") THEN \" \\\n",
        "\"     CASE\" \\\n",
        "\"        WHEN (WEAK_SIGNAL = \\\"Kauf-Signal\\\") THEN \" + buyingCaseStatement + \\\n",
        "\"        ELSE \" + sellingCaseStatement + \\\n",
        "\"     END\" \\\n",
        "\"   ELSE\" \\\n",
        "\"     CASE\" \\\n",
        "\"        WHEN (STRONG_SIGNAL = \\\"Kauf-Signal\\\") THEN \" + buyingCaseStatement + \\\n",
        "\"        ELSE \" + sellingCaseStatement + \\\n",
        "\"     END\"  \\\n",
        "\" END AS PPO_RESULT\" \\\n",
        "\" FROM base_data_ema_eval_result\"\n",
        "\n",
        "data_final_eval_result = spark.sql(final_result_statement)\n",
        "data_final_eval_result.createOrReplaceTempView(\"data_final_eval_result\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTbTV3Tq-yVQ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Visualisierung\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpv5aTZ5s9cr",
        "colab_type": "text"
      },
      "source": [
        "*Parameter*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvznmJ0OAg9A",
        "colab_type": "code",
        "outputId": "47dbc4ff-f744-48c8-f5d3-256a96d1b212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "#\n",
        "# Minimales bis Maximale Daten der geladenen Kruse\n",
        "#\n",
        "\n",
        "base_data_dayfiltered_informationView.show(Console_Preview_Limit)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------+-------------------+-------------------+\n",
            "|filename|count(1)|          min(date)|          max(date)|\n",
            "+--------+--------+-------------------+-------------------+\n",
            "|  neceth|      18|2019-06-30 15:13:00|2019-09-30 03:55:00|\n",
            "|  dtxusd|      43|2019-08-16 23:55:00|2019-10-01 23:35:00|\n",
            "|  omnusd|     167|2018-10-30 23:44:00|2019-10-01 03:05:00|\n",
            "|  btcusd|    2211|2013-04-01 23:58:00|2019-10-01 18:46:00|\n",
            "|  yywusd|     664|2017-12-01 23:59:00|2019-10-01 19:47:00|\n",
            "|  sngeth|     325|2018-01-24 23:11:00|2019-09-26 21:04:00|\n",
            "|  neojpy|     541|2018-03-29 22:48:00|2019-10-01 22:36:00|\n",
            "|  dtabtc|     225|2018-07-12 23:41:00|2019-10-01 10:04:00|\n",
            "|  euteur|      78|2018-11-27 16:32:00|2019-10-01 07:48:00|\n",
            "|  gotusd|     359|2018-09-06 23:52:00|2019-10-01 14:06:00|\n",
            "|  pasusd|     253|2019-01-17 23:09:00|2019-10-01 22:17:00|\n",
            "|  tnbusd|     615|2018-01-08 23:59:00|2019-10-01 19:00:00|\n",
            "|  udcusd|     285|2018-12-04 23:15:00|2019-10-01 17:39:00|\n",
            "|  vldeth|     129|2018-11-19 23:22:00|2019-10-01 19:49:00|\n",
            "|  csxusd|     138|2018-10-18 23:33:00|2019-09-30 17:04:00|\n",
            "|  mitusd|     432|2018-04-26 23:42:00|2019-09-30 11:22:00|\n",
            "|  btceur|     856|2017-05-19 08:16:00|2019-10-01 19:26:00|\n",
            "|  wprusd|     253|2018-07-26 23:45:00|2019-09-30 18:52:00|\n",
            "|  atobtc|     160|2019-04-25 23:15:00|2019-10-01 23:36:00|\n",
            "|  enjusd|     248|2018-11-19 19:52:00|2019-09-29 09:45:00|\n",
            "+--------+--------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N492DiY2_AMq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "Anzeige_Für_Alle_Gelesen_Daten = False #@param {type:\"boolean\"}\n",
        "#@markdown Parameter beziehen sich lediglich auf die Ausgabe:\n",
        "Anzeige_Limit = 365  #@param {type: \"slider\", min: 1, max: 10000}\n",
        "Anzeige_Zahlen_Genaugikeit = 3  #@param {type: \"slider\", min: 1, max: 10}\n",
        "Anzeige_Template = \"plotly_dark\" #@param ['ggplot2', 'seaborn', 'simple_white', 'plotly', 'plotly_white', 'plotly_dark', 'presentation', 'xgridoff', 'ygridoff', 'gridon', 'none'] {allow-input: false}\n",
        "#@markdown Start- & End-Tag haben keinen Einfluss auf die Exportierten Tage:\n",
        "Anzeige_Start_Tag = '2018-10-01' #@param {type:\"date\"}\n",
        "Anzeige_Ende_Tag = '2019-10-01' #@param {type:\"date\"}\n",
        "\n",
        "Anzeige_Zahlen_Genaugikeit_str = str(Anzeige_Zahlen_Genaugikeit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbbSlkCNtVYk",
        "colab_type": "text"
      },
      "source": [
        "*Vorbereitung*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXtWob2qAqPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f63f9f5f-ddf0-4c5a-c263-0ec38f550349"
      },
      "source": [
        "#\n",
        "# Datenvorbereitung\n",
        "#\n",
        "if Anzeige_Für_Alle_Gelesen_Daten:\n",
        "  data_eval_result = spark.sql(\n",
        "    \" select filename, Date(date), format_number(high, \"+Anzeige_Zahlen_Genaugikeit_str+\") high, format_number(low, \"+Anzeige_Zahlen_Genaugikeit_str+\") low, format_number(open, \"+Anzeige_Zahlen_Genaugikeit_str+\") open, format_number(close, \"+Anzeige_Zahlen_Genaugikeit_str+\") close\" \\\n",
        "    \" , format_number(PPO, \"+Anzeige_Zahlen_Genaugikeit_str+\") PPO, format_number(SIGNAL, \"+Anzeige_Zahlen_Genaugikeit_str+\") Signal, format_number(PPO_HISTOGRAM, \"+Anzeige_Zahlen_Genaugikeit_str+\") PPO_HISTOGRAM \" \\\n",
        "    \" , TREND, WEAK_SIGNAL, STRONG_SIGNAL, DIVERGENCE, PPO_RESULT \"\\\n",
        "    \" FROM data_final_eval_result \" \\\n",
        "    \" WHERE date >= '\"+ Anzeige_Start_Tag.strip() +\" 00:00:00' AND date <= '\"+ Anzeige_Ende_Tag.strip() +\"23:59:59' \" \\\n",
        "    \" order by filename asc, date asc \" \n",
        "  )\n",
        "\n",
        "  data_pd = data_eval_result.toPandas()\n",
        "\n",
        "  # Tausender-Zeichen entfernen: 1,000.001 -> 1000.001\n",
        "  data_pd.close = (data_pd['close'].replace('\\,','', regex = True).astype(float))\n",
        "  data_pd.open = (data_pd['open'].replace('\\,','', regex = True).astype(float))\n",
        "  data_pd.high = (data_pd['high'].replace('\\,','', regex = True).astype(float))\n",
        "  data_pd.low = (data_pd['low'].replace('\\,','', regex = True).astype(float))\n",
        "\n",
        "  data_pd.date = pd.to_datetime(data_pd.date)\n",
        "  data_pd.close = pd.to_numeric(data_pd.close)\n",
        "  data_pd.PPO = pd.to_numeric(data_pd.PPO)\n",
        "  data_pd.Signal = pd.to_numeric(data_pd.Signal)\n",
        "  data_pd.PPO_HISTOGRAM = pd.to_numeric(data_pd.PPO_HISTOGRAM)\n",
        "\n",
        "  data_pd.filename = (data_pd['filename']).astype(str)\n",
        "\n",
        "  # Vorbereiten der for-each-loop\n",
        "  data_pd_filenames = spark.sql(\"SELECT DISTINCT filename FROM data_final_eval_result\").toPandas().to_numpy()\n",
        "\n",
        "else:\n",
        "  print(\"Aufbereiotung der Daten zur Visualisierung durch Parameter 'Anzeige_Für_Alle_Gelesen_Daten' übersprungen.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aufbereiotung der Daten zur Visualisierung durch Parameter 'Anzeige_Für_Alle_Gelesen_Daten' übersprungen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RB_j1k5-MId",
        "colab_type": "text"
      },
      "source": [
        "Output\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37rKNTHlBrbI",
        "colab_type": "code",
        "outputId": "0fd35eb1-7c98-4c45-e716-1f12b485930a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "# Charts & Table\n",
        "#\n",
        "\n",
        "if Anzeige_Für_Alle_Gelesen_Daten:\n",
        "  hideInjectedFilenameColumn_CSS = \"<style>\" \\\n",
        "    \"table td:nth-child(1) { display:none;}\" \\\n",
        "    \"table.dataframe thead th:first-child {display: none;}\" \\\n",
        "    \"</style>\"\n",
        "\n",
        "  for currentFilenames in data_pd_filenames:\n",
        "    for currentFilename in currentFilenames:\n",
        "\n",
        "        current_data_pd = data_pd[data_pd.filename == currentFilename]\n",
        "\n",
        "        stock_chart = go.Figure(\n",
        "            data = [ go.Candlestick( \n",
        "                x = current_data_pd['date'],\n",
        "                open = current_data_pd['open'],\n",
        "                high = current_data_pd['high'],\n",
        "                low = current_data_pd['low'],\n",
        "                close = current_data_pd['close']\n",
        "                )]\n",
        "        )\n",
        "\n",
        "        ppo_chart = go.Figure(\n",
        "            data = [ go.Scatter(\n",
        "                x = current_data_pd['date'],\n",
        "                y = current_data_pd['PPO'],\n",
        "                mode = 'lines',\n",
        "                name = 'PPO'\n",
        "            ), go.Scatter(\n",
        "                x = current_data_pd['date'],\n",
        "                y = current_data_pd['Signal'],\n",
        "                mode = 'lines',\n",
        "                name = 'Signal'\n",
        "            )]\n",
        "        )\n",
        "\n",
        "        ppo_chart.add_trace(\n",
        "            go.Bar(\n",
        "                name = 'PPO-Histogram',\n",
        "                x = current_data_pd['date'],\n",
        "                y = current_data_pd['PPO_HISTOGRAM']\n",
        "            )\n",
        "        )\n",
        "\n",
        "        stock_chart.update_layout(\n",
        "            title='Chart für CSV-Datei \\'' + currentFilename + '\\'',\n",
        "            yaxis_title = 'Points',\n",
        "            template = Anzeige_Template,\n",
        "            xaxis_rangeslider_visible = False\n",
        "        )\n",
        "\n",
        "        ppo_chart.update_layout(\n",
        "            title='PPO-Indikator für CSV-Datei \\'' + currentFilename + '\\'',\n",
        "            yaxis_title = 'Changes in %',\n",
        "            template = Anzeige_Template\n",
        "        )\n",
        "\n",
        "        print(\"\\n\\nFilename: \" + currentFilename + \"\\nZeitraum: \" + Anzeige_Start_Tag + \" → \" + Anzeige_Ende_Tag + \"\\n\")\n",
        "        stock_chart.show()\n",
        "        ppo_chart.show()\n",
        "        print()\n",
        "        display(HTML(data_pd.to_html(index = False, max_rows = Anzeige_Limit) + hideInjectedFilenameColumn_CSS))\n",
        "else:\n",
        "  print(\"Visualisierung durch Parameter 'Anzeige_Für_Alle_Gelesen_Daten' übersprungen.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Visualisierung durch Parameter 'Anzeige_Für_Alle_Gelesen_Daten' übersprungen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjCN5PgZ2sgY",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "# CSV-Export\n",
        "----\n",
        "*Die Daten sollen nach Tage sotiert ausgegeben werden (vgl. Besprechung 13.12.19)*\n",
        "\n",
        "*Darstellung des Datums nach [ISO-8601-Norm](https://lmgtfy.com/?q=ISO-8601) (z.B.: 2019-09-07)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csUZHyk-6XYB",
        "colab_type": "text"
      },
      "source": [
        "Ordner-Struktur\n",
        "---\n",
        "\n",
        "**aktueller Pfad sollte ~/content' sein**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZEAcJHMgsKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@markdown Maximal einen Unterordner - Übergeordneter Ordner muss bereits existieren:\n",
        "Export_Mainordner_Pfad = 'ABDA2019'  #@param {type: \"string\"}\n",
        "Export_Subordner_Pfad = 'spark_warehouse'  #@param {type: \"string\"}\n",
        "Export_Ordnername = 'ppo_csv_export'  #@param {type: \"string\"}\n",
        "Download_Export_Ordner = True #@param {type:\"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crTttkJC3wWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# (Neuen) Ordner Erstellen\n",
        "#\n",
        "\n",
        "# Alten Ordner löschen\n",
        "if os.path.exists(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername)):\n",
        "  os.rmdir(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))\n",
        "\n",
        "# Ordner-Checks\n",
        "if not os.path.exists(Export_Mainordner_Pfad):\n",
        "  os.mkdir(Export_Mainordner_Pfad)\n",
        "\n",
        "if not os.path.exists(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad)):\n",
        "  os.mkdir(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad))\n",
        "\n",
        "if not os.path.exists(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername)):\n",
        "  os.mkdir(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOU6H1W84Cml",
        "colab_type": "text"
      },
      "source": [
        "Vorbereitung\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svwpxboa4EG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Outputt Daten sammeln\n",
        "#\n",
        "\n",
        "data_output = spark.sql(\n",
        "  \" select filename, Date(date) date, open, close, high, low, volume, PPO_RESULT\" \\\n",
        "  \" FROM data_final_eval_result \" \\\n",
        "  \" order by filename asc, date asc \" \n",
        ").toPandas()\n",
        "\n",
        "# Types definieren\n",
        "data_output.filename = (data_output['filename']).astype(str)\n",
        "data_output.date = pd.to_datetime(data_output.date)\n",
        "data_output.close = (data_output['close'].replace('\\,','', regex = True).astype(float))\n",
        "data_output.open = (data_output['open'].replace('\\,','', regex = True).astype(float))\n",
        "data_output.high = pd.to_numeric(data_output.high)\n",
        "data_output.low = pd.to_numeric(data_output.low)\n",
        "data_output.volume = pd.to_numeric(data_output.volume)\n",
        "data_output.PPO_RESULT = (data_output['PPO_RESULT']).astype(str)\n",
        "\n",
        "data_output_days = spark.sql(\"SELECT DISTINCT Date(date) date FROM data_final_eval_result\").toPandas()\n",
        "data_output_days.date = data_output_days.date = pd.to_datetime(data_output_days.date)\n",
        "\n",
        "data_distinct_days = data_output_days.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_S_nGOd1u5N",
        "colab_type": "text"
      },
      "source": [
        "Export\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOjNDRNcD8kx",
        "colab_type": "code",
        "outputId": "4190b409-99da-4cf3-e05b-9ed6c57c3afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#\n",
        "# Daten als csv exportieren\n",
        "#\n",
        "\n",
        "print(\"Zu exportierende Tage:\")\n",
        "numpy.set_printoptions(threshold = Console_Preview_Limit)\n",
        "print(data_distinct_days)\n",
        "print(\"\\nZu exportierender Datensatz:\")\n",
        "display(HTML(data_output.to_html(index = False, max_rows = Console_Preview_Limit)))\n",
        "\n",
        "for currentDays in data_distinct_days:\n",
        "  for currentDay in currentDays:\n",
        "\n",
        "      # Daten\n",
        "      currentOutput = data_output[data_output.date == currentDay]\n",
        "      # Zeitpunkt\n",
        "      currentTimestamp = Timestamp(currentDay)\n",
        "\n",
        "      # Anpassung des Monats zu 2 Stellen\n",
        "      if currentTimestamp.month > 9:\n",
        "        formattedMonth = str(currentTimestamp.month)\n",
        "      else:\n",
        "        formattedMonth = '0' + str(currentTimestamp.month)\n",
        "\n",
        "      # Anpassung des Tages zu 2 Stellen\n",
        "      if currentTimestamp.day > 9:\n",
        "        formattedDay = str(currentTimestamp.day)\n",
        "      else:\n",
        "        formattedDay = '0' + str(currentTimestamp.day)\n",
        "\n",
        "      # Filename\n",
        "      currentFilename = str(currentTimestamp.year) + \"-\" + formattedMonth + \"-\" + formattedDay + \".csv\"\n",
        "      \n",
        "      # Schreiben\n",
        "      currentOutput.to_csv(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername, currentFilename),\n",
        "                             sep = '\\t', encoding = 'utf-8', index = False, mode = 'w')\n",
        "        \n",
        "print(\"\\n\\nExport abgeschlossen, erstellte Dateien:\")\n",
        "os.listdir(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zu exportierende Tage:\n",
            "[['2019-06-04T00:00:00.000000000']\n",
            " ['2018-08-10T00:00:00.000000000']\n",
            " ['2019-05-08T00:00:00.000000000']\n",
            " ...\n",
            " ['2013-07-27T00:00:00.000000000']\n",
            " ['2013-07-22T00:00:00.000000000']\n",
            " ['2013-06-04T00:00:00.000000000']]\n",
            "\n",
            "Zu exportierender Datensatz:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>filename</th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>volume</th>\n",
              "      <th>PPO_RESULT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>abseth</td>\n",
              "      <td>2018-08-09</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>3563.0</td>\n",
              "      <td>H0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>abseth</td>\n",
              "      <td>2018-08-10</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>300.0</td>\n",
              "      <td>H0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>zrxusd</td>\n",
              "      <td>2019-09-30</td>\n",
              "      <td>0.205310</td>\n",
              "      <td>0.205310</td>\n",
              "      <td>0.205310</td>\n",
              "      <td>0.205310</td>\n",
              "      <td>500.0</td>\n",
              "      <td>H3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>zrxusd</td>\n",
              "      <td>2019-10-01</td>\n",
              "      <td>0.213130</td>\n",
              "      <td>0.213140</td>\n",
              "      <td>0.213140</td>\n",
              "      <td>0.213130</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>H0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Export abgeschlossen, erstellte Dateien:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2014-01-01.csv',\n",
              " '2013-11-17.csv',\n",
              " '2014-02-12.csv',\n",
              " '2015-12-06.csv',\n",
              " '2014-09-11.csv',\n",
              " '2016-06-17.csv',\n",
              " '2019-02-19.csv',\n",
              " '2015-09-30.csv',\n",
              " '2016-10-24.csv',\n",
              " '2015-07-29.csv',\n",
              " '2018-12-19.csv',\n",
              " '2019-06-01.csv',\n",
              " '2013-11-19.csv',\n",
              " '2016-12-06.csv',\n",
              " '2014-08-01.csv',\n",
              " '2016-12-21.csv',\n",
              " '2015-08-22.csv',\n",
              " '2018-01-28.csv',\n",
              " '2014-08-03.csv',\n",
              " '2014-06-23.csv',\n",
              " '2014-09-23.csv',\n",
              " '2015-07-16.csv',\n",
              " '2018-03-09.csv',\n",
              " '2016-01-07.csv',\n",
              " '2017-02-17.csv',\n",
              " '2013-06-26.csv',\n",
              " '2017-09-22.csv',\n",
              " '2019-06-08.csv',\n",
              " '2014-07-03.csv',\n",
              " '2019-06-30.csv',\n",
              " '2017-01-05.csv',\n",
              " '2018-07-29.csv',\n",
              " '2015-09-11.csv',\n",
              " '2016-07-04.csv',\n",
              " '2016-03-06.csv',\n",
              " '2014-11-13.csv',\n",
              " '2018-12-27.csv',\n",
              " '2015-02-18.csv',\n",
              " '2013-07-26.csv',\n",
              " '2017-05-02.csv',\n",
              " '2014-08-28.csv',\n",
              " '2016-02-03.csv',\n",
              " '2014-09-26.csv',\n",
              " '2017-10-23.csv',\n",
              " '2019-08-28.csv',\n",
              " '2013-07-15.csv',\n",
              " '2014-02-05.csv',\n",
              " '2019-06-11.csv',\n",
              " '2018-04-01.csv',\n",
              " '2014-05-27.csv',\n",
              " '2014-04-22.csv',\n",
              " '2018-03-23.csv',\n",
              " '2014-07-06.csv',\n",
              " '2017-03-14.csv',\n",
              " '2017-04-23.csv',\n",
              " '2015-02-16.csv',\n",
              " '2016-12-09.csv',\n",
              " '2016-06-03.csv',\n",
              " '2015-06-16.csv',\n",
              " '2013-07-12.csv',\n",
              " '2018-11-22.csv',\n",
              " '2014-09-06.csv',\n",
              " '2016-03-27.csv',\n",
              " '2015-08-13.csv',\n",
              " '2019-05-18.csv',\n",
              " '2019-04-24.csv',\n",
              " '2019-08-22.csv',\n",
              " '2015-12-11.csv',\n",
              " '2017-02-04.csv',\n",
              " '2014-05-30.csv',\n",
              " '2016-03-03.csv',\n",
              " '2017-08-26.csv',\n",
              " '2016-03-31.csv',\n",
              " '2018-04-23.csv',\n",
              " '2013-11-16.csv',\n",
              " '2017-01-21.csv',\n",
              " '2019-09-18.csv',\n",
              " '2019-07-13.csv',\n",
              " '2015-01-16.csv',\n",
              " '2016-12-24.csv',\n",
              " '2015-06-30.csv',\n",
              " '2019-09-20.csv',\n",
              " '2017-06-27.csv',\n",
              " '2018-09-30.csv',\n",
              " '2017-11-02.csv',\n",
              " '2013-11-25.csv',\n",
              " '2014-10-13.csv',\n",
              " '2015-03-31.csv',\n",
              " '2019-03-30.csv',\n",
              " '2016-11-03.csv',\n",
              " '2019-08-02.csv',\n",
              " '2018-04-20.csv',\n",
              " '2016-09-13.csv',\n",
              " '2017-12-31.csv',\n",
              " '2018-11-20.csv',\n",
              " '2014-01-07.csv',\n",
              " '2014-06-12.csv',\n",
              " '2018-03-01.csv',\n",
              " '2017-01-06.csv',\n",
              " '2015-09-07.csv',\n",
              " '2016-10-20.csv',\n",
              " '2015-04-22.csv',\n",
              " '2016-11-23.csv',\n",
              " '2018-08-30.csv',\n",
              " '2016-09-18.csv',\n",
              " '2016-09-02.csv',\n",
              " '2017-08-22.csv',\n",
              " '2014-07-28.csv',\n",
              " '2019-06-21.csv',\n",
              " '2018-10-04.csv',\n",
              " '2016-08-11.csv',\n",
              " '2013-07-07.csv',\n",
              " '2018-02-10.csv',\n",
              " '2013-11-09.csv',\n",
              " '2019-01-25.csv',\n",
              " '2019-06-29.csv',\n",
              " '2015-08-16.csv',\n",
              " '2016-07-17.csv',\n",
              " '2017-08-11.csv',\n",
              " '2014-05-20.csv',\n",
              " '2014-02-03.csv',\n",
              " '2019-08-04.csv',\n",
              " '2015-09-13.csv',\n",
              " '2014-07-02.csv',\n",
              " '2017-12-30.csv',\n",
              " '2019-05-28.csv',\n",
              " '2013-06-18.csv',\n",
              " '2016-12-11.csv',\n",
              " '2016-06-14.csv',\n",
              " '2016-12-31.csv',\n",
              " '2016-10-13.csv',\n",
              " '2017-10-11.csv',\n",
              " '2014-07-22.csv',\n",
              " '2014-08-15.csv',\n",
              " '2015-02-14.csv',\n",
              " '2016-07-30.csv',\n",
              " '2013-10-23.csv',\n",
              " '2014-06-16.csv',\n",
              " '2015-02-19.csv',\n",
              " '2018-02-26.csv',\n",
              " '2014-03-05.csv',\n",
              " '2013-09-27.csv',\n",
              " '2018-12-20.csv',\n",
              " '2014-01-02.csv',\n",
              " '2013-12-15.csv',\n",
              " '2019-02-06.csv',\n",
              " '2016-02-01.csv',\n",
              " '2014-09-08.csv',\n",
              " '2015-04-13.csv',\n",
              " '2018-02-12.csv',\n",
              " '2017-03-24.csv',\n",
              " '2013-08-29.csv',\n",
              " '2019-06-05.csv',\n",
              " '2018-03-25.csv',\n",
              " '2019-05-04.csv',\n",
              " '2017-01-31.csv',\n",
              " '2017-12-22.csv',\n",
              " '2017-07-07.csv',\n",
              " '2013-04-04.csv',\n",
              " '2017-08-09.csv',\n",
              " '2018-09-22.csv',\n",
              " '2016-02-07.csv',\n",
              " '2019-04-25.csv',\n",
              " '2014-02-06.csv',\n",
              " '2019-09-03.csv',\n",
              " '2016-08-17.csv',\n",
              " '2013-10-04.csv',\n",
              " '2018-12-07.csv',\n",
              " '2014-03-24.csv',\n",
              " '2014-06-22.csv',\n",
              " '2013-11-30.csv',\n",
              " '2018-06-14.csv',\n",
              " '2019-08-25.csv',\n",
              " '2016-11-24.csv',\n",
              " '2016-02-18.csv',\n",
              " '2017-06-09.csv',\n",
              " '2018-10-29.csv',\n",
              " '2014-08-20.csv',\n",
              " '2017-06-14.csv',\n",
              " '2015-07-06.csv',\n",
              " '2016-12-25.csv',\n",
              " '2019-07-23.csv',\n",
              " '2017-12-10.csv',\n",
              " '2019-01-01.csv',\n",
              " '2017-03-13.csv',\n",
              " '2017-08-24.csv',\n",
              " '2014-04-13.csv',\n",
              " '2018-07-28.csv',\n",
              " '2015-11-06.csv',\n",
              " '2015-12-28.csv',\n",
              " '2018-05-14.csv',\n",
              " '2013-09-06.csv',\n",
              " '2018-06-29.csv',\n",
              " '2017-04-30.csv',\n",
              " '2018-09-15.csv',\n",
              " '2013-08-13.csv',\n",
              " '2013-11-20.csv',\n",
              " '2013-10-29.csv',\n",
              " '2017-04-03.csv',\n",
              " '2018-05-17.csv',\n",
              " '2019-07-01.csv',\n",
              " '2018-06-11.csv',\n",
              " '2015-10-30.csv',\n",
              " '2013-11-08.csv',\n",
              " '2014-07-13.csv',\n",
              " '2013-08-15.csv',\n",
              " '2015-09-26.csv',\n",
              " '2018-03-22.csv',\n",
              " '2017-06-22.csv',\n",
              " '2015-07-04.csv',\n",
              " '2018-09-29.csv',\n",
              " '2014-06-13.csv',\n",
              " '2015-07-20.csv',\n",
              " '2014-03-12.csv',\n",
              " '2016-11-04.csv',\n",
              " '2015-07-03.csv',\n",
              " '2017-10-13.csv',\n",
              " '2019-02-27.csv',\n",
              " '2015-07-10.csv',\n",
              " '2019-01-03.csv',\n",
              " '2013-11-07.csv',\n",
              " '2014-03-29.csv',\n",
              " '2014-02-24.csv',\n",
              " '2019-07-04.csv',\n",
              " '2019-05-16.csv',\n",
              " '2017-12-17.csv',\n",
              " '2017-04-11.csv',\n",
              " '2018-06-17.csv',\n",
              " '2014-08-31.csv',\n",
              " '2019-04-11.csv',\n",
              " '2013-04-29.csv',\n",
              " '2017-03-10.csv',\n",
              " '2019-09-30.csv',\n",
              " '2014-08-02.csv',\n",
              " '2015-01-28.csv',\n",
              " '2018-05-20.csv',\n",
              " '2013-08-06.csv',\n",
              " '2014-09-21.csv',\n",
              " '2018-02-08.csv',\n",
              " '2018-01-19.csv',\n",
              " '2015-09-18.csv',\n",
              " '2017-04-15.csv',\n",
              " '2014-10-07.csv',\n",
              " '2015-02-01.csv',\n",
              " '2018-04-15.csv',\n",
              " '2015-12-26.csv',\n",
              " '2018-01-17.csv',\n",
              " '2017-03-03.csv',\n",
              " '2016-03-02.csv',\n",
              " '2017-04-12.csv',\n",
              " '2014-09-15.csv',\n",
              " '2015-07-01.csv',\n",
              " '2017-06-12.csv',\n",
              " '2018-06-06.csv',\n",
              " '2017-10-17.csv',\n",
              " '2015-11-14.csv',\n",
              " '2014-11-12.csv',\n",
              " '2018-11-03.csv',\n",
              " '2014-06-10.csv',\n",
              " '2018-06-13.csv',\n",
              " '2015-01-10.csv',\n",
              " '2013-04-15.csv',\n",
              " '2017-05-05.csv',\n",
              " '2013-10-11.csv',\n",
              " '2017-05-24.csv',\n",
              " '2019-01-31.csv',\n",
              " '2018-02-01.csv',\n",
              " '2017-01-29.csv',\n",
              " '2019-04-22.csv',\n",
              " '2019-09-19.csv',\n",
              " '2018-08-07.csv',\n",
              " '2013-04-26.csv',\n",
              " '2015-09-03.csv',\n",
              " '2018-12-16.csv',\n",
              " '2017-04-16.csv',\n",
              " '2015-02-26.csv',\n",
              " '2016-06-26.csv',\n",
              " '2019-01-07.csv',\n",
              " '2017-06-26.csv',\n",
              " '2018-05-25.csv',\n",
              " '2015-08-12.csv',\n",
              " '2017-12-27.csv',\n",
              " '2018-05-26.csv',\n",
              " '2013-09-11.csv',\n",
              " '2014-11-15.csv',\n",
              " '2018-10-07.csv',\n",
              " '2014-07-24.csv',\n",
              " '2014-06-14.csv',\n",
              " '2018-01-24.csv',\n",
              " '2016-06-04.csv',\n",
              " '2013-10-14.csv',\n",
              " '2014-02-14.csv',\n",
              " '2019-06-20.csv',\n",
              " '2015-09-16.csv',\n",
              " '2017-08-16.csv',\n",
              " '2016-07-07.csv',\n",
              " '2017-03-28.csv',\n",
              " '2017-06-13.csv',\n",
              " '2019-07-18.csv',\n",
              " '2017-06-06.csv',\n",
              " '2017-05-01.csv',\n",
              " '2014-08-12.csv',\n",
              " '2016-10-27.csv',\n",
              " '2018-11-26.csv',\n",
              " '2018-07-05.csv',\n",
              " '2017-04-07.csv',\n",
              " '2016-01-08.csv',\n",
              " '2014-09-14.csv',\n",
              " '2017-05-26.csv',\n",
              " '2013-08-12.csv',\n",
              " '2013-07-14.csv',\n",
              " '2016-03-13.csv',\n",
              " '2016-08-02.csv',\n",
              " '2014-11-09.csv',\n",
              " '2017-09-29.csv',\n",
              " '2016-09-30.csv',\n",
              " '2019-01-22.csv',\n",
              " '2016-09-10.csv',\n",
              " '2019-08-16.csv',\n",
              " '2019-09-10.csv',\n",
              " '2017-11-13.csv',\n",
              " '2017-03-26.csv',\n",
              " '2014-03-21.csv',\n",
              " '2014-11-10.csv',\n",
              " '2015-10-14.csv',\n",
              " '2018-10-11.csv',\n",
              " '2019-09-13.csv',\n",
              " '2016-02-25.csv',\n",
              " '2016-03-26.csv',\n",
              " '2017-10-16.csv',\n",
              " '2013-06-03.csv',\n",
              " '2019-01-17.csv',\n",
              " '2013-04-11.csv',\n",
              " '2017-10-02.csv',\n",
              " '2018-01-03.csv',\n",
              " '2019-04-08.csv',\n",
              " '2018-08-31.csv',\n",
              " '2016-06-28.csv',\n",
              " '2014-01-14.csv',\n",
              " '2015-01-11.csv',\n",
              " '2014-01-10.csv',\n",
              " '2018-08-27.csv',\n",
              " '2016-08-15.csv',\n",
              " '2018-01-01.csv',\n",
              " '2013-04-27.csv',\n",
              " '2015-12-03.csv',\n",
              " '2019-02-15.csv',\n",
              " '2017-03-23.csv',\n",
              " '2013-06-22.csv',\n",
              " '2018-12-17.csv',\n",
              " '2018-10-28.csv',\n",
              " '2014-09-02.csv',\n",
              " '2016-09-11.csv',\n",
              " '2017-09-06.csv',\n",
              " '2014-04-05.csv',\n",
              " '2015-09-29.csv',\n",
              " '2016-07-16.csv',\n",
              " '2014-10-12.csv',\n",
              " '2015-08-09.csv',\n",
              " '2013-12-09.csv',\n",
              " '2016-11-11.csv',\n",
              " '2014-07-17.csv',\n",
              " '2015-10-25.csv',\n",
              " '2016-12-10.csv',\n",
              " '2013-05-18.csv',\n",
              " '2017-08-05.csv',\n",
              " '2015-11-13.csv',\n",
              " '2018-08-15.csv',\n",
              " '2018-08-03.csv',\n",
              " '2019-03-10.csv',\n",
              " '2017-01-20.csv',\n",
              " '2015-12-21.csv',\n",
              " '2019-05-09.csv',\n",
              " '2014-02-28.csv',\n",
              " '2015-12-12.csv',\n",
              " '2016-01-10.csv',\n",
              " '2019-04-10.csv',\n",
              " '2018-09-17.csv',\n",
              " '2014-02-04.csv',\n",
              " '2013-05-12.csv',\n",
              " '2013-05-08.csv',\n",
              " '2014-07-23.csv',\n",
              " '2016-08-14.csv',\n",
              " '2019-08-01.csv',\n",
              " '2016-10-16.csv',\n",
              " '2014-05-18.csv',\n",
              " '2017-06-08.csv',\n",
              " '2015-08-15.csv',\n",
              " '2017-11-06.csv',\n",
              " '2019-03-04.csv',\n",
              " '2013-08-08.csv',\n",
              " '2015-01-02.csv',\n",
              " '2018-04-03.csv',\n",
              " '2018-09-26.csv',\n",
              " '2018-01-15.csv',\n",
              " '2015-04-15.csv',\n",
              " '2018-07-07.csv',\n",
              " '2017-11-27.csv',\n",
              " '2018-06-04.csv',\n",
              " '2019-10-01.csv',\n",
              " '2016-01-09.csv',\n",
              " '2015-09-24.csv',\n",
              " '2016-10-15.csv',\n",
              " '2014-03-19.csv',\n",
              " '2017-02-09.csv',\n",
              " '2018-10-24.csv',\n",
              " '2015-10-26.csv',\n",
              " '2016-09-27.csv',\n",
              " '2019-09-15.csv',\n",
              " '2019-01-27.csv',\n",
              " '2013-08-26.csv',\n",
              " '2016-11-02.csv',\n",
              " '2015-11-09.csv',\n",
              " '2015-02-03.csv',\n",
              " '2016-09-22.csv',\n",
              " '2013-06-28.csv',\n",
              " '2014-09-28.csv',\n",
              " '2018-01-25.csv',\n",
              " '2016-10-12.csv',\n",
              " '2014-10-25.csv',\n",
              " '2013-10-10.csv',\n",
              " '2014-01-15.csv',\n",
              " '2019-02-28.csv',\n",
              " '2019-03-26.csv',\n",
              " '2017-07-25.csv',\n",
              " '2017-04-04.csv',\n",
              " '2016-11-08.csv',\n",
              " '2019-05-19.csv',\n",
              " '2018-09-25.csv',\n",
              " '2016-11-14.csv',\n",
              " '2016-10-04.csv',\n",
              " '2016-03-19.csv',\n",
              " '2017-07-24.csv',\n",
              " '2018-04-02.csv',\n",
              " '2013-08-28.csv',\n",
              " '2017-08-28.csv',\n",
              " '2015-11-01.csv',\n",
              " '2015-01-25.csv',\n",
              " '2019-04-02.csv',\n",
              " '2013-09-16.csv',\n",
              " '2017-11-19.csv',\n",
              " '2014-06-18.csv',\n",
              " '2015-04-27.csv',\n",
              " '2019-09-29.csv',\n",
              " '2014-01-16.csv',\n",
              " '2018-06-02.csv',\n",
              " '2018-10-15.csv',\n",
              " '2017-09-11.csv',\n",
              " '2018-06-23.csv',\n",
              " '2019-03-03.csv',\n",
              " '2013-10-19.csv',\n",
              " '2017-10-08.csv',\n",
              " '2017-10-10.csv',\n",
              " '2017-05-19.csv',\n",
              " '2014-01-11.csv',\n",
              " '2015-01-21.csv',\n",
              " '2019-05-17.csv',\n",
              " '2016-11-16.csv',\n",
              " '2018-12-18.csv',\n",
              " '2014-07-05.csv',\n",
              " '2015-07-19.csv',\n",
              " '2015-01-31.csv',\n",
              " '2017-09-07.csv',\n",
              " '2018-03-07.csv',\n",
              " '2015-04-08.csv',\n",
              " '2018-11-01.csv',\n",
              " '2018-05-03.csv',\n",
              " '2015-08-01.csv',\n",
              " '2014-11-14.csv',\n",
              " '2014-06-27.csv',\n",
              " '2015-10-15.csv',\n",
              " '2015-03-12.csv',\n",
              " '2014-09-10.csv',\n",
              " '2015-09-28.csv',\n",
              " '2013-09-01.csv',\n",
              " '2019-07-17.csv',\n",
              " '2018-09-20.csv',\n",
              " '2014-11-02.csv',\n",
              " '2017-04-27.csv',\n",
              " '2015-08-02.csv',\n",
              " '2016-01-12.csv',\n",
              " '2014-07-12.csv',\n",
              " '2017-04-24.csv',\n",
              " '2017-01-07.csv',\n",
              " '2019-02-11.csv',\n",
              " '2014-03-02.csv',\n",
              " '2018-03-18.csv',\n",
              " '2017-03-04.csv',\n",
              " '2017-07-04.csv',\n",
              " '2018-07-18.csv',\n",
              " '2019-02-03.csv',\n",
              " '2013-09-29.csv',\n",
              " '2015-11-07.csv',\n",
              " '2018-02-02.csv',\n",
              " '2017-09-09.csv',\n",
              " '2016-07-27.csv',\n",
              " '2018-08-18.csv',\n",
              " '2015-07-27.csv',\n",
              " '2018-02-04.csv',\n",
              " '2019-05-13.csv',\n",
              " '2015-11-22.csv',\n",
              " '2018-07-01.csv',\n",
              " '2019-06-22.csv',\n",
              " '2017-08-04.csv',\n",
              " '2017-12-04.csv',\n",
              " '2017-10-24.csv',\n",
              " '2014-01-18.csv',\n",
              " '2014-06-20.csv',\n",
              " '2016-10-31.csv',\n",
              " '2017-05-18.csv',\n",
              " '2017-06-29.csv',\n",
              " '2018-04-04.csv',\n",
              " '2015-08-10.csv',\n",
              " '2019-05-30.csv',\n",
              " '2017-10-20.csv',\n",
              " '2014-06-07.csv',\n",
              " '2018-08-16.csv',\n",
              " '2019-09-21.csv',\n",
              " '2015-04-17.csv',\n",
              " '2018-02-17.csv',\n",
              " '2015-04-11.csv',\n",
              " '2017-04-28.csv',\n",
              " '2019-01-19.csv',\n",
              " '2017-06-25.csv',\n",
              " '2016-10-07.csv',\n",
              " '2018-07-03.csv',\n",
              " '2019-03-18.csv',\n",
              " '2014-04-14.csv',\n",
              " '2019-04-30.csv',\n",
              " '2017-08-18.csv',\n",
              " '2015-08-24.csv',\n",
              " '2015-02-13.csv',\n",
              " '2014-02-17.csv',\n",
              " '2018-06-05.csv',\n",
              " '2015-12-29.csv',\n",
              " '2016-10-28.csv',\n",
              " '2014-05-09.csv',\n",
              " '2016-01-24.csv',\n",
              " '2018-07-10.csv',\n",
              " '2016-09-24.csv',\n",
              " '2016-06-21.csv',\n",
              " '2018-11-08.csv',\n",
              " '2013-07-22.csv',\n",
              " '2017-11-14.csv',\n",
              " '2017-10-22.csv',\n",
              " '2015-12-24.csv',\n",
              " '2018-05-23.csv',\n",
              " '2018-02-07.csv',\n",
              " '2018-11-25.csv',\n",
              " '2019-09-25.csv',\n",
              " '2019-04-26.csv',\n",
              " '2017-05-20.csv',\n",
              " '2014-06-19.csv',\n",
              " '2015-04-12.csv',\n",
              " '2019-04-28.csv',\n",
              " '2019-07-21.csv',\n",
              " '2013-08-07.csv',\n",
              " '2016-03-21.csv',\n",
              " '2014-07-07.csv',\n",
              " '2018-10-23.csv',\n",
              " '2017-06-17.csv',\n",
              " '2015-04-07.csv',\n",
              " '2018-11-16.csv',\n",
              " '2014-08-22.csv',\n",
              " '2013-04-10.csv',\n",
              " '2019-07-28.csv',\n",
              " '2013-09-08.csv',\n",
              " '2016-09-28.csv',\n",
              " '2018-04-06.csv',\n",
              " '2016-10-05.csv',\n",
              " '2013-10-06.csv',\n",
              " '2016-01-01.csv',\n",
              " '2017-02-22.csv',\n",
              " '2019-03-09.csv',\n",
              " '2014-03-10.csv',\n",
              " '2019-02-04.csv',\n",
              " '2018-12-01.csv',\n",
              " '2019-07-22.csv',\n",
              " '2017-11-29.csv',\n",
              " '2016-08-16.csv',\n",
              " '2016-02-16.csv',\n",
              " '2016-03-24.csv',\n",
              " '2017-06-04.csv',\n",
              " '2017-04-19.csv',\n",
              " '2017-02-06.csv',\n",
              " '2017-07-08.csv',\n",
              " '2017-02-13.csv',\n",
              " '2016-03-18.csv',\n",
              " '2014-07-29.csv',\n",
              " '2015-03-30.csv',\n",
              " '2017-08-14.csv',\n",
              " '2018-04-30.csv',\n",
              " '2015-02-24.csv',\n",
              " '2015-01-22.csv',\n",
              " '2013-06-15.csv',\n",
              " '2014-05-14.csv',\n",
              " '2014-02-07.csv',\n",
              " '2014-01-21.csv',\n",
              " '2014-05-28.csv',\n",
              " '2014-09-05.csv',\n",
              " '2013-10-31.csv',\n",
              " '2019-05-29.csv',\n",
              " '2017-05-15.csv',\n",
              " '2019-01-29.csv',\n",
              " '2018-06-19.csv',\n",
              " '2017-09-24.csv',\n",
              " '2013-08-27.csv',\n",
              " '2018-03-29.csv',\n",
              " '2013-08-30.csv',\n",
              " '2014-03-03.csv',\n",
              " '2016-01-22.csv',\n",
              " '2017-06-23.csv',\n",
              " '2017-09-03.csv',\n",
              " '2017-05-25.csv',\n",
              " '2017-05-22.csv',\n",
              " '2017-01-03.csv',\n",
              " '2017-12-29.csv',\n",
              " '2017-02-16.csv',\n",
              " '2013-11-29.csv',\n",
              " '2013-07-10.csv',\n",
              " '2017-08-25.csv',\n",
              " '2019-03-28.csv',\n",
              " '2016-06-27.csv',\n",
              " '2015-06-21.csv',\n",
              " '2019-04-16.csv',\n",
              " '2019-06-07.csv',\n",
              " '2018-08-01.csv',\n",
              " '2014-02-08.csv',\n",
              " '2013-08-19.csv',\n",
              " '2015-04-04.csv',\n",
              " '2018-03-14.csv',\n",
              " '2019-03-02.csv',\n",
              " '2018-03-26.csv',\n",
              " '2017-11-21.csv',\n",
              " '2014-07-26.csv',\n",
              " '2016-07-05.csv',\n",
              " '2014-01-08.csv',\n",
              " '2019-05-26.csv',\n",
              " '2016-10-21.csv',\n",
              " '2013-10-05.csv',\n",
              " '2014-08-06.csv',\n",
              " '2016-07-06.csv',\n",
              " '2016-10-06.csv',\n",
              " '2019-09-26.csv',\n",
              " '2015-03-22.csv',\n",
              " '2016-03-01.csv',\n",
              " '2019-04-27.csv',\n",
              " '2013-09-03.csv',\n",
              " '2017-01-16.csv',\n",
              " '2019-01-30.csv',\n",
              " '2017-03-18.csv',\n",
              " '2014-05-10.csv',\n",
              " '2016-01-11.csv',\n",
              " '2017-01-30.csv',\n",
              " '2015-11-02.csv',\n",
              " '2017-03-15.csv',\n",
              " '2014-05-17.csv',\n",
              " '2017-01-09.csv',\n",
              " '2017-05-03.csv',\n",
              " '2014-03-27.csv',\n",
              " '2019-07-30.csv',\n",
              " '2018-08-02.csv',\n",
              " '2017-07-23.csv',\n",
              " '2016-06-10.csv',\n",
              " '2018-06-07.csv',\n",
              " '2017-08-07.csv',\n",
              " '2017-11-04.csv',\n",
              " '2018-11-10.csv',\n",
              " '2015-01-06.csv',\n",
              " '2016-08-30.csv',\n",
              " '2015-06-18.csv',\n",
              " '2019-05-02.csv',\n",
              " '2016-02-13.csv',\n",
              " '2018-10-08.csv',\n",
              " '2017-09-04.csv',\n",
              " '2014-05-11.csv',\n",
              " '2019-02-08.csv',\n",
              " '2014-09-07.csv',\n",
              " '2018-06-28.csv',\n",
              " '2018-11-30.csv',\n",
              " '2019-07-14.csv',\n",
              " '2016-12-13.csv',\n",
              " '2019-09-24.csv',\n",
              " '2014-11-16.csv',\n",
              " '2019-07-11.csv',\n",
              " '2016-08-31.csv',\n",
              " '2018-06-22.csv',\n",
              " '2016-10-10.csv',\n",
              " '2018-04-07.csv',\n",
              " '2016-02-24.csv',\n",
              " '2018-01-16.csv',\n",
              " '2016-03-30.csv',\n",
              " '2019-03-24.csv',\n",
              " '2013-05-21.csv',\n",
              " '2016-12-23.csv',\n",
              " '2018-09-28.csv',\n",
              " '2019-02-14.csv',\n",
              " '2017-03-27.csv',\n",
              " '2015-06-12.csv',\n",
              " '2013-12-03.csv',\n",
              " '2013-07-08.csv',\n",
              " '2014-11-17.csv',\n",
              " '2016-01-15.csv',\n",
              " '2015-06-27.csv',\n",
              " '2014-10-24.csv',\n",
              " '2015-01-14.csv',\n",
              " '2017-05-23.csv',\n",
              " '2015-08-21.csv',\n",
              " '2013-08-04.csv',\n",
              " '2018-03-11.csv',\n",
              " '2013-09-02.csv',\n",
              " '2014-03-30.csv',\n",
              " '2019-07-27.csv',\n",
              " '2018-08-04.csv',\n",
              " '2017-12-13.csv',\n",
              " '2018-10-21.csv',\n",
              " '2018-04-18.csv',\n",
              " '2013-05-11.csv',\n",
              " '2014-04-30.csv',\n",
              " '2017-07-03.csv',\n",
              " '2018-09-18.csv',\n",
              " '2019-06-03.csv',\n",
              " '2018-01-04.csv',\n",
              " '2015-09-04.csv',\n",
              " '2017-12-21.csv',\n",
              " '2016-02-20.csv',\n",
              " '2019-08-13.csv',\n",
              " '2015-06-06.csv',\n",
              " '2017-07-09.csv',\n",
              " '2018-09-21.csv',\n",
              " '2013-11-15.csv',\n",
              " '2019-02-26.csv',\n",
              " '2019-08-17.csv',\n",
              " '2016-09-07.csv',\n",
              " '2019-09-05.csv',\n",
              " '2018-11-23.csv',\n",
              " '2019-04-13.csv',\n",
              " '2014-06-11.csv',\n",
              " '2015-06-04.csv',\n",
              " '2014-04-28.csv',\n",
              " '2018-08-28.csv',\n",
              " '2013-05-22.csv',\n",
              " '2016-09-12.csv',\n",
              " '2013-05-26.csv',\n",
              " '2017-02-11.csv',\n",
              " '2013-08-22.csv',\n",
              " '2013-11-22.csv',\n",
              " '2018-12-24.csv',\n",
              " '2013-06-02.csv',\n",
              " '2015-07-09.csv',\n",
              " '2014-10-11.csv',\n",
              " '2019-01-14.csv',\n",
              " '2017-01-11.csv',\n",
              " '2017-12-26.csv',\n",
              " '2017-01-23.csv',\n",
              " '2016-08-18.csv',\n",
              " '2016-03-12.csv',\n",
              " '2019-08-07.csv',\n",
              " '2016-06-25.csv',\n",
              " '2016-09-19.csv',\n",
              " '2014-06-06.csv',\n",
              " '2018-04-27.csv',\n",
              " '2018-12-11.csv',\n",
              " '2015-10-10.csv',\n",
              " '2017-03-08.csv',\n",
              " '2017-06-28.csv',\n",
              " '2018-03-10.csv',\n",
              " '2018-11-02.csv',\n",
              " '2018-09-08.csv',\n",
              " '2014-05-02.csv',\n",
              " '2018-04-28.csv',\n",
              " '2014-10-06.csv',\n",
              " '2015-07-21.csv',\n",
              " '2013-07-27.csv',\n",
              " '2014-11-19.csv',\n",
              " '2016-07-31.csv',\n",
              " '2017-10-05.csv',\n",
              " '2016-01-20.csv',\n",
              " '2018-05-18.csv',\n",
              " '2013-08-11.csv',\n",
              " '2019-07-02.csv',\n",
              " '2014-03-25.csv',\n",
              " '2018-07-31.csv',\n",
              " '2019-05-12.csv',\n",
              " '2015-03-11.csv',\n",
              " '2015-10-23.csv',\n",
              " '2017-07-21.csv',\n",
              " '2016-11-22.csv',\n",
              " '2019-08-12.csv',\n",
              " '2014-07-11.csv',\n",
              " '2018-07-02.csv',\n",
              " '2018-09-11.csv',\n",
              " '2013-09-12.csv',\n",
              " '2013-05-09.csv',\n",
              " '2016-11-25.csv',\n",
              " '2014-10-28.csv',\n",
              " '2017-03-31.csv',\n",
              " '2014-09-01.csv',\n",
              " '2016-10-17.csv',\n",
              " '2015-07-08.csv',\n",
              " '2018-10-22.csv',\n",
              " '2015-08-17.csv',\n",
              " '2018-03-30.csv',\n",
              " '2017-06-03.csv',\n",
              " '2014-01-23.csv',\n",
              " '2015-03-24.csv',\n",
              " '2017-11-17.csv',\n",
              " '2017-11-07.csv',\n",
              " '2016-01-06.csv',\n",
              " '2013-07-30.csv',\n",
              " '2018-05-10.csv',\n",
              " '2013-07-13.csv',\n",
              " '2015-08-18.csv',\n",
              " '2015-09-12.csv',\n",
              " '2014-08-19.csv',\n",
              " '2014-10-17.csv',\n",
              " '2018-09-01.csv',\n",
              " '2016-06-20.csv',\n",
              " '2013-04-28.csv',\n",
              " '2013-04-18.csv',\n",
              " '2019-03-08.csv',\n",
              " '2018-09-02.csv',\n",
              " '2016-07-19.csv',\n",
              " '2017-08-21.csv',\n",
              " '2019-08-09.csv',\n",
              " '2016-10-23.csv',\n",
              " '2019-08-06.csv',\n",
              " '2018-07-26.csv',\n",
              " '2015-06-02.csv',\n",
              " '2017-05-21.csv',\n",
              " '2015-12-07.csv',\n",
              " '2016-11-09.csv',\n",
              " '2018-07-27.csv',\n",
              " '2014-06-08.csv',\n",
              " '2016-12-18.csv',\n",
              " '2018-05-12.csv',\n",
              " '2016-09-21.csv',\n",
              " '2014-10-01.csv',\n",
              " '2018-07-14.csv',\n",
              " '2015-06-05.csv',\n",
              " '2015-06-01.csv',\n",
              " '2016-12-16.csv',\n",
              " '2013-07-05.csv',\n",
              " '2013-09-15.csv',\n",
              " '2014-02-13.csv',\n",
              " '2019-01-12.csv',\n",
              " '2014-02-26.csv',\n",
              " '2014-05-13.csv',\n",
              " '2017-09-18.csv',\n",
              " '2014-04-19.csv',\n",
              " '2014-10-03.csv',\n",
              " '2016-01-28.csv',\n",
              " '2014-10-10.csv',\n",
              " '2014-08-16.csv',\n",
              " '2017-11-12.csv',\n",
              " '2017-01-26.csv',\n",
              " '2014-09-30.csv',\n",
              " '2018-03-17.csv',\n",
              " '2014-06-15.csv',\n",
              " '2013-11-18.csv',\n",
              " '2017-10-03.csv',\n",
              " '2019-03-07.csv',\n",
              " '2016-12-17.csv',\n",
              " '2018-04-13.csv',\n",
              " '2017-08-10.csv',\n",
              " '2013-11-10.csv',\n",
              " '2016-07-24.csv',\n",
              " '2014-08-07.csv',\n",
              " '2017-05-08.csv',\n",
              " '2014-04-27.csv',\n",
              " '2018-06-26.csv',\n",
              " '2015-10-08.csv',\n",
              " '2018-12-22.csv',\n",
              " '2014-06-21.csv',\n",
              " '2013-10-13.csv',\n",
              " '2016-07-12.csv',\n",
              " '2018-10-31.csv',\n",
              " '2015-09-02.csv',\n",
              " '2015-08-07.csv',\n",
              " '2013-10-28.csv',\n",
              " '2014-01-04.csv',\n",
              " '2015-12-31.csv',\n",
              " '2018-10-10.csv',\n",
              " '2018-08-09.csv',\n",
              " '2017-03-30.csv',\n",
              " '2013-08-05.csv',\n",
              " '2018-01-30.csv',\n",
              " '2016-06-06.csv',\n",
              " '2017-05-27.csv',\n",
              " '2014-02-02.csv',\n",
              " '2018-09-24.csv',\n",
              " '2013-10-03.csv',\n",
              " '2015-04-30.csv',\n",
              " '2013-08-23.csv',\n",
              " '2015-06-28.csv',\n",
              " '2014-04-17.csv',\n",
              " '2018-03-24.csv',\n",
              " '2017-02-27.csv',\n",
              " '2016-09-29.csv',\n",
              " '2017-02-02.csv',\n",
              " '2013-12-18.csv',\n",
              " '2017-12-08.csv',\n",
              " '2019-03-06.csv',\n",
              " '2017-02-12.csv',\n",
              " '2013-09-10.csv',\n",
              " '2017-12-24.csv',\n",
              " '2013-05-31.csv',\n",
              " '2017-10-28.csv',\n",
              " '2018-02-24.csv',\n",
              " '2019-04-04.csv',\n",
              " '2015-06-22.csv',\n",
              " '2013-04-07.csv',\n",
              " '2019-08-27.csv',\n",
              " '2019-04-05.csv',\n",
              " '2017-06-24.csv',\n",
              " '2014-02-21.csv',\n",
              " '2015-01-13.csv',\n",
              " '2016-06-05.csv',\n",
              " '2018-07-08.csv',\n",
              " '2014-10-20.csv',\n",
              " '2018-01-29.csv',\n",
              " '2017-01-13.csv',\n",
              " '2017-08-02.csv',\n",
              " '2013-05-16.csv',\n",
              " '2019-02-16.csv',\n",
              " '2019-03-25.csv',\n",
              " '2014-07-15.csv',\n",
              " '2016-10-22.csv',\n",
              " '2015-04-06.csv',\n",
              " '2016-07-28.csv',\n",
              " '2014-04-20.csv',\n",
              " '2018-08-29.csv',\n",
              " '2017-11-25.csv',\n",
              " '2017-09-25.csv',\n",
              " '2015-10-03.csv',\n",
              " '2014-02-01.csv',\n",
              " '2015-08-23.csv',\n",
              " '2015-10-21.csv',\n",
              " '2019-02-20.csv',\n",
              " '2015-07-14.csv',\n",
              " '2017-03-29.csv',\n",
              " '2016-12-22.csv',\n",
              " '2018-11-05.csv',\n",
              " '2017-02-26.csv',\n",
              " '2015-01-27.csv',\n",
              " '2018-09-23.csv',\n",
              " '2018-09-07.csv',\n",
              " '2018-06-10.csv',\n",
              " '2015-10-11.csv',\n",
              " '2014-05-29.csv',\n",
              " '2013-04-20.csv',\n",
              " '2019-06-23.csv',\n",
              " '2018-02-09.csv',\n",
              " '2017-01-24.csv',\n",
              " '2017-11-10.csv',\n",
              " '2016-08-25.csv',\n",
              " '2018-08-21.csv',\n",
              " '2013-07-01.csv',\n",
              " '2019-01-05.csv',\n",
              " '2016-02-04.csv',\n",
              " '2019-05-24.csv',\n",
              " '2017-10-01.csv',\n",
              " '2019-06-06.csv',\n",
              " '2013-11-26.csv',\n",
              " '2019-03-11.csv',\n",
              " '2018-04-11.csv',\n",
              " '2013-10-27.csv',\n",
              " '2016-07-09.csv',\n",
              " '2016-11-12.csv',\n",
              " '2014-03-09.csv',\n",
              " '2016-09-01.csv',\n",
              " '2013-12-16.csv',\n",
              " '2015-04-20.csv',\n",
              " '2013-06-07.csv',\n",
              " '2019-07-12.csv',\n",
              " '2015-06-07.csv',\n",
              " '2015-01-05.csv',\n",
              " '2019-07-31.csv',\n",
              " '2017-10-27.csv',\n",
              " '2013-08-21.csv',\n",
              " '2019-05-03.csv',\n",
              " '2014-02-25.csv',\n",
              " '2015-03-08.csv',\n",
              " '2017-11-20.csv',\n",
              " '2015-01-08.csv',\n",
              " '2019-04-12.csv',\n",
              " '2014-03-11.csv',\n",
              " '2013-04-09.csv',\n",
              " '2017-11-05.csv',\n",
              " '2016-09-05.csv',\n",
              " '2013-05-29.csv',\n",
              " '2019-02-05.csv',\n",
              " '2018-09-03.csv',\n",
              " '2016-01-21.csv',\n",
              " '2013-07-04.csv',\n",
              " '2018-03-08.csv',\n",
              " '2017-02-07.csv',\n",
              " '2015-06-19.csv',\n",
              " '2014-01-29.csv',\n",
              " '2019-01-28.csv',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOvGVIG4sMLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if Download_Export_Ordner:\n",
        "  shutil.make_archive('Exportdaten_PPO', 'zip', os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))\n",
        "  files.download('/content/Exportdaten_PPO.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvM2pNY5m68h",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Vergleichbare Charts\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_fCKAEpnBXE",
        "colab_type": "text"
      },
      "source": [
        "**BTCUSD**: [traidingview.com](https://de.tradingview.com/chart/ecCxGiMv/) *(login erforderlich)*\n",
        "\n",
        "```\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| TraidingView.com:                                                 |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "|             | 2019-10-01 | 2019-09-30 | 2019-08-10  | 2019-08-07  |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| EMA26       |    9305.74 |    9384.68 |    10848.14 |    10630.29 |\n",
        "| EMA12       |    8734.78 |    8810.38 |    11193.74 |    10886.97 |\n",
        "| PPO         |      -6.14 |      -6.12 |        3.19 |        2.41 |\n",
        "| SIGNAL      |      -4.56 |      -4.17 |        1.63 |        0.13 |\n",
        "| PPO_History |      -1.57 |      -1.95 |        1.55 |        2.28 |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| CALCULATED:                                                       |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "|             | 2019-10-01 | 2019-09-30 | 2019-08-10  | 2019-08-07  |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| EMA26       |  9327.3017 |  9446.6421 |  10651.5067 |  10673.4093 |\n",
        "| EMA12       |  8751.5960 |  8842.4748 |  11163.8865 |  10785.1409 |\n",
        "| PPO         |     -6.172 |    -6.3956 |      4.8104 |      1.0468 |\n",
        "| SIGNAL      |    -4.4147 |    -4.0178 |      0.9102 |     -1.8162 |\n",
        "| PPO_History |    -1.7576 |    -2.3778 |      3.9002 |      2.8631 |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asHXGi907ytk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**intc.csv (utf-8)** *--> eher ungeeignetes Beispiel*\n",
        "\n",
        " see [School.StockCharts.com](https://school.stockcharts.com/doku.php?id=technical_indicators:price_oscillators_ppo) \n",
        "\n",
        "```\n",
        "time,open,close,high,low,volume\n",
        "1271203140000,21.16,20.16,20.16,20.16,0.0\n",
        "1271289540000,20.49,20.49,20.49,20.49,0.0\n",
        "1271375940000,20.74,20.74,20.74,20.74,0.0\n",
        "1271462340000,20.77,20.77,20.77,20.77,0.0\n",
        "1271721540000,20.53,20.53,20.53,20.53,0.0\n",
        "1271807940000,19.61,19.61,19.61,19.61,0.0\n",
        "1271894340000,20.02,20.02,20.02,20.02,0.0\n",
        "1271980740000,19.70,19.70,19.70,19.70,0.0\n",
        "1272067140000,19.94,19.94,19.94,19.94,0.0\n",
        "1272326340000,19.62,19.62,19.62,19.62,0.0\n",
        "1272412740000,19.11,19.11,19.11,19.11,0.0\n",
        "1272499140000,19.32,19.32,19.32,19.32,0.0\n",
        "1272585540000,19.61,19.61,19.61,19.61,0.0\n",
        "1272671940000,19.54,19.54,19.54,19.54,0.0\n",
        "1272931140000,18.89,18.89,18.89,18.89,0.0\n",
        "1273017540000,19.33,19.33,19.33,19.33,0.0\n",
        "1273103940000,19.21,19.21,19.21,19.21,0.0\n",
        "1273190340000,19.51,19.51,19.51,19.51,0.0\n",
        "1273276740000,19.55,19.55,19.55,19.55,0.0\n",
        "1273535940000,19.92,19.92,19.92,19.92,0.0\n",
        "1273622340000,20.29,20.29,20.29,20.29,0.0\n",
        "1273708740000,20.58,20.58,20.58,20.58,0.0\n",
        "1273795140000,20.52,20.52,20.52,20.52,0.0\n",
        "1273881540000,20.69,20.69,20.69,20.69,0.0\n",
        "1274140740000,20.67,20.67,20.67,20.67,0.0\n",
        "1274227140000,20.72,20.72,20.72,20.72,0.0\n",
        "1274313540000,20.25,20.25,20.25,20.25,0.0\n",
        "1274399940000,20.56,20.56,20.56,20.56,0.0\n",
        "1274486340000,20.49,20.49,20.49,20.49,0.0\n",
        "1274745540000,20.39,20.39,20.39,20.39,0.0\n",
        "\n",
        "```\n",
        "\n"
      ]
    }
  ]
}