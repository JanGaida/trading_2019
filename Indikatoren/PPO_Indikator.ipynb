{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_Indikator_v5-6.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hLeICupUHzXk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82zjv2aVvLo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "# **Studienarbeit in der Vorlesung 'Applied Big Data Analyitics'**\n",
        "# **Implementation: Percent Price Oscillator (PPO) anhand des Data-Warehouse-Models**\n",
        "---\n",
        "---\n",
        "---\n",
        "* ### **Erstellt:** Wintersemeseter 2019-2020\n",
        "* ### **Dozent:** Prof. Dr. Sebastian Leuoth\n",
        "* ### **Autor:** Jan Gaida\n",
        "* ### **Email:** jan.gaida@hof-university.de\n",
        "* ### **Git:** [trading_2019](https://github.com/sleuoth-hof/trading_2019), [JanGaida](https://github.com/JanGaida) \n",
        "---\n",
        "---\n",
        "### *Powered by:*\n",
        "### ***Hochschule für Angewandte Wissenschaften Hof***\n",
        "[![Logo: Hochschule Hof](https://www.uni-assist.de/fileadmin/_processed_/4/7/csm_hof-university_logo_308ee8b37b.jpg)](https://www.hof-university.de/)\n",
        "---\n",
        "---\n",
        "---\n",
        "*© 2019-2020 Jan Gaida, Prof. Dr. Sebastian Leuoth. All Rights Reserved.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0U5tkKJWlK",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Laufzeit-Parameter\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5bIqUvXKi_8",
        "colab_type": "text"
      },
      "source": [
        "*Datenbasis Parameter*\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ApeOGsTKnf1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown > ---\n",
        "#@markdown > **Datenursprung** *─ definiert die zuwählende Datenbasis*\n",
        "#@markdown > --- ---\n",
        "Datenframe_Von = \"ABDA2019-Repository\" #@param [\"ABDA2019-Repository\", \"Zu wählender Pfad\", \"Beispiel: Intel (erfordert eine manuell zu erstellende 'content/intc.csv'-Datei)\"] {allow-input: false}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Zu ladende Dateien** *─ nur benutzt wenn 'Datenframe_Von' auf 'ABDA2019-Repository' eingestellt ist*\n",
        "#@markdown > --- ---\n",
        "Filenamen_Zu_Laden = '*.csv'  #@param {type: \"string\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Zu ladender Pfad** *─ nur benutzt wenn 'Datenframe_Von' auf 'Zu wählender Pfad' eingestellt ist*\n",
        "#@markdown > --- ---\n",
        "Filepfad_Zu_Laden = '*.csv'  #@param {type: \"string\"}\n",
        "#@markdown > ---\n",
        "#@markdown > **Anzahl der Zeichen im Chartnamen** *─ nur benutzt wenn 'Datenframe_Von' auf 'Zu wählender Pfad' eingestellt ist*\n",
        "#@markdown > --- ---\n",
        "Filenamen_Stellen_Von_Filepfad = 0  #@param {type: \"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryoW7Bt2KMhh",
        "colab_type": "text"
      },
      "source": [
        "*Anzeige Parameter*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYjZte1jKSE8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown > ---\n",
        "#@markdown > **Preview**\n",
        "#@markdown > --- ---\n",
        "Anzeige_Aktiviert = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Maximal anzuzeigende Zeilen im Preview**\n",
        "#@markdown > --- ---\n",
        "Anzeige_Limit = 10  #@param {type: \"slider\", min: 1, max: 10000}\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Anzuzeigende Ziffern nach der Kommastelle**\n",
        "#@markdown > --- ---\n",
        "Anzeige_Zahlen_Genaugikeit = 3  #@param {type: \"slider\", min: 1, max: 10}\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Template der verwendeten Diagramme** *─ vgl. Plotly*\n",
        "#@markdown > --- ---\n",
        "Anzeige_Template = \"plotly_dark\" #@param ['ggplot2', 'seaborn', 'simple_white', 'plotly', 'plotly_white', 'plotly_dark', 'presentation', 'xgridoff', 'ygridoff', 'gridon', 'none'] {allow-input: false}\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Anzuzeigender Zeitraum** *─ keine Auswirkung auf den Export*\n",
        "#@markdown > --- ---\n",
        "Anzeige_Start_Tag = '2013-04-01' #@param {type:\"date\"}\n",
        "Anzeige_Ende_Tag = '2019-10-01' #@param {type:\"date\"}\n",
        "\n",
        "Anzeige_Zahlen_Genaugikeit_str = str(Anzeige_Zahlen_Genaugikeit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT0MahuyQ3VE",
        "colab_type": "text"
      },
      "source": [
        "*Export Parameter*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIPD-14nJZiI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@markdown > ---\n",
        "#@markdown > **Name des Export-Root-Ordners** *─ vgl. 'content/Export_Mainordner_Pfad/Export_Subordner_Pfad/Export_Ordnername'*\n",
        "#@markdown > --- ---\n",
        "Export_Mainordner_Pfad = 'ABDA2019'  #@param {type: \"string\"}\n",
        "#@markdown > ---\n",
        "#@markdown > **Name des Unterordner des Export-Root-Ordners** *─ vgl. 'content/Export_Mainordner_Pfad/Export_Subordner_Pfad/Export_Ordnername'*\n",
        "#@markdown > --- ---\n",
        "Export_Subordner_Pfad = 'spark_warehouse'  #@param {type: \"string\"}\n",
        "#@markdown > ---\n",
        "#@markdown > **Ordnername in dem erstellte CSV-Datein geschrieben werden** *─ vgl. 'content/Export_Mainordner_Pfad/Export_Subordner_Pfad/Export_Ordnername'*\n",
        "#@markdown > --- ---\n",
        "Export_Ordnername = 'ppo_csv_export'  #@param {type: \"string\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Ob zum Export die Pandas-Api benutzt werden soll**\n",
        "#@markdown > --- ---\n",
        "Pandas_CSV_Export = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Ob nach Fertigstellung die erstellten Daten heruntergeladen werden sollen**\n",
        "#@markdown > --- ---\n",
        "Download_Export_Ordner = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Ob die Erstellten CSV-Part-Files zusammengeführt werden sollen**\n",
        "#@markdown > --- ---\n",
        "Merge_Export_Part = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown > ---\n",
        "#@markdown > **Ob die Erstellten CSV-Part-Files gelöscht werden sollen** *- erfordert 'Merge_Export_Part'*\n",
        "#@markdown > --- ---\n",
        "Delete_Export_Part = True #@param {type:\"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLeICupUHzXk",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Datenbank\n",
        "---\n",
        "*Urspung: [More than 400 Cryptocurrency-Chartdata](https://www.kaggle.com/tencars/392-crypto-currency-pairs-at-minute-resolution/version/2)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gisP4TkgH8Ba",
        "colab_type": "code",
        "outputId": "4b86902e-f194-4272-dd4b-71b835c08da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# clonen\n",
        "!git clone https://github.com/sleuoth/ABDA2019.git\n",
        "\n",
        "# entpacken 'btcusd'\n",
        "!unzip ABDA2019/testdaten/cryptominuteresolution/btcusd.csv.zip\n",
        "!mv btcusd.csv ABDA2019/testdaten/cryptominuteresolution/btcusd.csv\n",
        "\n",
        "# resultat\n",
        "!ls ABDA2019/testdaten/cryptominuteresolution"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ABDA2019' already exists and is not an empty directory.\n",
            "Archive:  ABDA2019/testdaten/cryptominuteresolution/btcusd.csv.zip\n",
            "  inflating: btcusd.csv              \n",
            "abseth.csv\tcndbtc.csv  foausd.csv\tmkrdai.csv  rdnusd.csv\tutkbtc.csv\n",
            "absusd.csv\tcndeth.csv  fsnbtc.csv\tmkreth.csv  repbtc.csv\tutketh.csv\n",
            "agibtc.csv\tcndusd.csv  fsneth.csv\tmkrusd.csv  repeth.csv\tutkusd.csv\n",
            "agieth.csv\tcnneth.csv  fsnusd.csv\tmlneth.csv  repusd.csv\tutneth.csv\n",
            "agiusd.csv\tcnnusd.csv  fttusd.csv\tmlnusd.csv  reqbtc.csv\tutnusd.csv\n",
            "aidbtc.csv\tcsxeth.csv  fttust.csv\tmnabtc.csv  reqeth.csv\tveebtc.csv\n",
            "aideth.csv\tcsxusd.csv  funbtc.csv\tmnaeth.csv  requsd.csv\tveeeth.csv\n",
            "aidusd.csv\tctxbtc.csv  funeth.csv\tmnausd.csv  rifbtc.csv\tveeusd.csv\n",
            "aiobtc.csv\tctxeth.csv  funusd.csv\tmtnbtc.csv  rifusd.csv\tvetbtc.csv\n",
            "aioeth.csv\tctxusd.csv  geneth.csv\tmtneth.csv  rlcbtc.csv\tveteth.csv\n",
            "aiousd.csv\tdadbtc.csv  genusd.csv\tmtnusd.csv  rlceth.csv\tvetusd.csv\n",
            "algbtc.csv\tdadeth.csv  gnoeth.csv\tncabtc.csv  rlcusd.csv\tvldeth.csv\n",
            "algusd.csv\tdadusd.csv  gnousd.csv\tncaeth.csv  rrbusd.csv\tvldusd.csv\n",
            "algust.csv\tdaibtc.csv  gntbtc.csv\tncausd.csv  rrbust.csv\tvsybtc.csv\n",
            "ampbtc.csv\tdaieth.csv  gnteth.csv\tnecbtc.csv  rrtbtc.csv\tvsyusd.csv\n",
            "ampusd.csv\tdaiusd.csv  gntusd.csv\tneceth.csv  rrtusd.csv\twaxbtc.csv\n",
            "ampust.csv\tdatbtc.csv  goteth.csv\tnecusd.csv  rteeth.csv\twaxeth.csv\n",
            "antbtc.csv\tdateth.csv  goteur.csv\tneobtc.csv  rteusd.csv\twaxusd.csv\n",
            "anteth.csv\tdatusd.csv  gotusd.csv\tneoeth.csv  sanbtc.csv\twbteth.csv\n",
            "antusd.csv\tdgbbtc.csv  gsdusd.csv\tneoeur.csv  saneth.csv\twbtusd.csv\n",
            "asteth.csv\tdgbusd.csv  gtxusd.csv\tneogbp.csv  sanusd.csv\twlousd.csv\n",
            "astusd.csv\tdgxeth.csv  gtxust.csv\tneojpy.csv  screth.csv\twloxlm.csv\n",
            "atmbtc.csv\tdgxusd.csv  hotbtc.csv\tneousd.csv  scrusd.csv\twprbtc.csv\n",
            "atmeth.csv\tdrneth.csv  hoteth.csv\tnioeth.csv  seebtc.csv\twpreth.csv\n",
            "atmusd.csv\tdrnusd.csv  hotusd.csv\tniousd.csv  seeeth.csv\twprusd.csv\n",
            "atobtc.csv\tdshbtc.csv  impeth.csv\todebtc.csv  seeusd.csv\twtceth.csv\n",
            "atoeth.csv\tdshusd.csv  impusd.csv\todeeth.csv  senbtc.csv\twtcusd.csv\n",
            "atousd.csv\tdtabtc.csv  inteth.csv\todeusd.csv  seneth.csv\txcheth.csv\n",
            "aucbtc.csv\tdtaeth.csv  intusd.csv\tokbbtc.csv  senusd.csv\txchusd.csv\n",
            "auceth.csv\tdtausd.csv  iosbtc.csv\tokbeth.csv  sngbtc.csv\txlmbtc.csv\n",
            "aucusd.csv\tdthbtc.csv  ioseth.csv\tokbusd.csv  sngeth.csv\txlmeth.csv\n",
            "avtbtc.csv\tdtheth.csv  iosusd.csv\tokbust.csv  sngusd.csv\txlmeur.csv\n",
            "avteth.csv\tdthusd.csv  iotbtc.csv\tomgbtc.csv  sntbtc.csv\txlmgbp.csv\n",
            "avtusd.csv\tdtxusd.csv  ioteth.csv\tomgdai.csv  snteth.csv\txlmjpy.csv\n",
            "babbtc.csv\tdtxust.csv  ioteur.csv\tomgeth.csv  sntusd.csv\txlmusd.csv\n",
            "babusd.csv\tedobtc.csv  iotgbp.csv\tomgusd.csv  spkbtc.csv\txmrbtc.csv\n",
            "babust.csv\tedoeth.csv  iotjpy.csv\tomnbtc.csv  spketh.csv\txmrusd.csv\n",
            "batbtc.csv\tedousd.csv  iotusd.csv\tomnusd.csv  spkusd.csv\txraeth.csv\n",
            "bateth.csv\telfbtc.csv  iqxbtc.csv\tonleth.csv  stjbtc.csv\txrausd.csv\n",
            "batusd.csv\telfeth.csv  iqxeos.csv\tonlusd.csv  stjeth.csv\txrpbtc.csv\n",
            "bbneth.csv\telfusd.csv  iqxusd.csv\torsbtc.csv  stjusd.csv\txrpusd.csv\n",
            "bbnusd.csv\tenjeth.csv  kanusd.csv\torseth.csv  swmeth.csv\txtzbtc.csv\n",
            "bcibtc.csv\tenjusd.csv  kanust.csv\torsusd.csv  swmusd.csv\txtzusd.csv\n",
            "bciusd.csv\teosbtc.csv  kncbtc.csv\tpaibtc.csv  tkneth.csv\txvgbtc.csv\n",
            "bftbtc.csv\teoseth.csv  knceth.csv\tpaiusd.csv  tknusd.csv\txvgeth.csv\n",
            "bfteth.csv\teoseur.csv  kncusd.csv\tpaseth.csv  tnbbtc.csv\txvgeur.csv\n",
            "bftusd.csv\teosgbp.csv  leobtc.csv\tpasusd.csv  tnbeth.csv\txvggbp.csv\n",
            "bntbtc.csv\teosjpy.csv  leoeos.csv\tpaxusd.csv  tnbusd.csv\txvgjpy.csv\n",
            "bnteth.csv\teosusd.csv  leoeth.csv\tpaxust.csv  trieth.csv\txvgusd.csv\n",
            "bntusd.csv\teosust.csv  leousd.csv\tpnketh.csv  triusd.csv\tyggeth.csv\n",
            "boxeth.csv\tessbtc.csv  leoust.csv\tpnkusd.csv  trxbtc.csv\tyggusd.csv\n",
            "boxusd.csv\tesseth.csv  looeth.csv\tpoabtc.csv  trxeth.csv\tyywbtc.csv\n",
            "bsvbtc.csv\tessusd.csv  loousd.csv\tpoaeth.csv  trxeur.csv\tyyweth.csv\n",
            "bsvusd.csv\tetcbtc.csv  lrcbtc.csv\tpoausd.csv  trxgbp.csv\tyywusd.csv\n",
            "btceur.csv\tetcusd.csv  lrceth.csv\tpoybtc.csv  trxjpy.csv\tzbtusd.csv\n",
            "btcgbp.csv\tethbtc.csv  lrcusd.csv\tpoyeth.csv  trxusd.csv\tzbtust.csv\n",
            "btcjpy.csv\tetheur.csv  ltcbtc.csv\tpoyusd.csv  tsdusd.csv\tzcnbtc.csv\n",
            "btcusd.csv\tethgbp.csv  ltcusd.csv\tqshbtc.csv  tsdust.csv\tzcneth.csv\n",
            "btcusd.csv.zip\tethjpy.csv  ltcust.csv\tqsheth.csv  udcusd.csv\tzcnusd.csv\n",
            "btcust.csv\tethusd.csv  lymbtc.csv\tqshusd.csv  udcust.csv\tzecbtc.csv\n",
            "btcxch.csv\tethust.csv  lymeth.csv\tqtmbtc.csv  ufreth.csv\tzecusd.csv\n",
            "btgbtc.csv\tetpbtc.csv  lymusd.csv\tqtmeth.csv  ufrusd.csv\tzilbtc.csv\n",
            "btgusd.csv\tetpeth.csv  maneth.csv\tqtmusd.csv  uosbtc.csv\tzileth.csv\n",
            "bttbtc.csv\tetpusd.csv  manusd.csv\trbtbtc.csv  uosusd.csv\tzilusd.csv\n",
            "bttusd.csv\teuseth.csv  mgoeth.csv\trbtusd.csv  uskbtc.csv\tzrxbtc.csv\n",
            "cbtbtc.csv\teususd.csv  mgousd.csv\trcnbtc.csv  uskeos.csv\tzrxdai.csv\n",
            "cbteth.csv\teuteur.csv  mitbtc.csv\trcneth.csv  usketh.csv\tzrxeth.csv\n",
            "cbtusd.csv\teutusd.csv  miteth.csv\trcnusd.csv  uskusd.csv\tzrxusd.csv\n",
            "clobtc.csv\tevtusd.csv  mitusd.csv\trdnbtc.csv  uskust.csv\n",
            "clousd.csv\tfoaeth.csv  mkrbtc.csv\trdneth.csv  ustusd.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy1-pSd3K8ex",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Weitere Frameworkinstallation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ig7hNjcLpoH",
        "colab_type": "code",
        "outputId": "a16f3145-2128-4f4c-f712-ee1713c4f24c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# jdk\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# tree\n",
        "!apt-get install tree\n",
        "\n",
        "# spark-package\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "\n",
        "# findspark\n",
        "!pip install findspark\n",
        "\n",
        "# numpy\n",
        "!pip install numpy\n",
        "\n",
        "# timeseries library\n",
        "!pip install ts ts-flint"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tree is already the newest version (1.7.0-5).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.5)\n",
            "Requirement already satisfied: ts in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: ts-flint in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qS6K26FmN1B",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGZaKQC5MlEN",
        "colab_type": "code",
        "outputId": "122d0d55-fa0a-49fe-f830-f3127daf3ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Für Pfade und andere OS-Funktionalität\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print(\">> Colab verbunden mit einer TPU\")\n",
        "else:\n",
        "  print(\">> Colab nicht verbunden mit einer TPU\")\n",
        "\n",
        "# Spark und Spark-SQL\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import input_file_name, col, collect_list, concat_ws, udf\n",
        "from pyspark.sql.types import DoubleType, StringType\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialisierung Spark und Spark-SQL\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "# Für einfache Weiterverarbeitung des Datenframes (z.B. Visualisierung mit Plotty)\n",
        "import pandas as pd\n",
        "from pandas import Timestamp\n",
        "\n",
        "# Für einfaches Handeln von Arrays aus Pandas-Datenframes\n",
        "import numpy\n",
        "from numpy import array\n",
        "\n",
        "# Visualisierung als Diagramm\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Für Darstellung von Datenframes\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Für File-Download\n",
        "from google.colab import files\n",
        "\n",
        "# Um Zip-Datein zu erstellen\n",
        "import shutil\n",
        "\n",
        "# Für den merge\n",
        "from glob import glob"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Colab verbunden mit einer TPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnoOYnD4M92t",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Anwendung PPO-Indikator\n",
        "---\n",
        "\n",
        "*PPO Implementation basierend auf: [Investopedia.com](https://www.investopedia.com/terms/p/ppo.asp#formula-and-calculation-for-ppo)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6I_jTzusuK6",
        "colab_type": "text"
      },
      "source": [
        "*Metadaten-Aggregation*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIWo98Hg1t1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Intialen Datenframe bilden\n",
        "#\n",
        "\n",
        "if Datenframe_Von == \"ABDA2019-Repository\":\n",
        "  df_spark = spark.read.csv('./ABDA2019/testdaten/cryptominuteresolution/' + Filenamen_Zu_Laden, inferSchema = True, header = True)\n",
        "  if Filenamen_Zu_Laden == '*.csv':\n",
        "    start = 57\n",
        "  else:\n",
        "    start = 59\n",
        "  end = 6\n",
        "\n",
        "elif Datenframe_Von == \"Zu wählender Pfad\":\n",
        "  df_spark = spark.read.csv( Filepfad_Zu_Laden, inferSchema = True, header = True)\n",
        "  start = Filepfad_Zu_Laden.count + 15 - Filenamen_Stellen_Von_Filepfad - 4\n",
        "  end = Filenamen_Stellen_Von_Filepfad\n",
        "\n",
        "elif Datenframe_Von == \"Beispiel: Intel (erfordert eine manuell zu erstellende 'content/intc.csv'-Datei)\":\n",
        "  df_spark = spark.read.csv('./intc.csv', inferSchema = True, header = True)\n",
        "  start = 17\n",
        "  end = 4\n",
        "\n",
        "df_spark = df_spark \\\n",
        "  .withColumn('filepath', input_file_name()) \\\n",
        "  .withColumn('filename', (input_file_name()[start:end]) ) \\\n",
        "  .withColumn('timestamp', df_spark['time']/1000) \\\n",
        "  .withColumn('date', (df_spark['time']/1000).cast('timestamp'))\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  print(\"Type:\", type(df_spark), \"\\n\")\n",
        "  print(\"Schema: \", end = '')\n",
        "  df_spark.printSchema()\n",
        "  df_spark_total_count = df_spark.count()\n",
        "  print(\"Available rows:\", df_spark_total_count, \"\\n\\nDaten-Preview:\")\n",
        "  #df_spark.show(Anzeige_Limit, False) # Durch Darstellung mittels PandasDataframe ersetzt/optimiert\n",
        "  display(HTML(df_spark.toPandas().to_html(index = False, max_rows = Anzeige_Limit)))\n",
        "\n",
        "# Erstellen der SQL-Tabelle \n",
        "df_spark.createOrReplaceTempView(\"base_data\")\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  df_spark_informationView = spark.sql(\"SELECT filename, count(*), min(date), max(date)  FROM base_data group by filename\")\n",
        "  print(\"Geladene Daten:\")\n",
        "  #df_spark_informationView.show(Anzeige_Limit, False) # Durch Darstellung mittels PandasDataframe ersetzt/optimiert\n",
        "  display(HTML(df_spark_informationView.toPandas().to_html(index = False, max_rows = Anzeige_Limit)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7FZUcVKr7LV",
        "colab_type": "text"
      },
      "source": [
        "*Tag-Filter*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eS-opK4lKmF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "19754e58-dd00-4174-a847-a40a71ae7d6c"
      },
      "source": [
        "#\n",
        "# PRO-TAG-FILTER\n",
        "#\n",
        "\n",
        "base_data_dayfiltered = spark.sql(\n",
        "  \"SELECT d.* FROM base_data d \" \\\n",
        "  \" where  (d.date, filename) in (select max(m.date), filename from base_data m group by date_format(m.date, \\\"y-M-d\\\"), filename)\" )\n",
        "\n",
        "# Erstellen der SQL-Tabelle\n",
        "base_data_dayfiltered.createOrReplaceTempView(\"base_data_dayfiltered\")\n",
        "\n",
        "base_data_dayfiltered_total_count = base_data_dayfiltered.count()\n",
        "\n",
        "base_data_dayfiltered_informationView = spark.sql(\"SELECT filename, count(*), min(date), max(date)  FROM base_data_dayfiltered group by filename\")\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  print(\"Available rows:\", base_data_dayfiltered_total_count, \"(lost\",(df_spark_total_count-base_data_dayfiltered_total_count), \"rows)\", \"\\n\\nVerfügbare Daten pro Tag:\")\n",
        "  #base_data_dayfiltered_informationView.show(Anzeige_Limit) # Durch Darstellung mittels PandasDataframe ersetzt/optimiert\n",
        "  display(HTML(base_data_dayfiltered_informationView.toPandas().to_html(index = False, max_rows = Anzeige_Limit)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-4ac8c964f4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbase_data_dayfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base_data_dayfiltered\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbase_data_dayfiltered_total_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_data_dayfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbase_data_dayfiltered_informationView\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT filename, count(*), min(date), max(date)  FROM base_data_dayfiltered group by filename\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fw8u3lUr-s9",
        "colab_type": "text"
      },
      "source": [
        "*EMA-UDF*\n",
        "---\n",
        "---\n",
        "Berrechnet nach: [tradistats.com](https://tradistats.com/exponentieller-gleitender-durchschnitt/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpCbfCCjguIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# EMA-UDF\n",
        "#\n",
        "\n",
        "# see https://tradistats.com/exponentieller-gleitender-durchschnitt/\n",
        "def ema(ar):\n",
        "    if len(ar) > 0:\n",
        "       SF  = 2/ (len(ar)+1)\n",
        "       SFi = 1 - SF\n",
        "       my_ema = ar[0]\n",
        "       for i in ar:\n",
        "           my_ema = (i * SF) + (my_ema * SFi)\n",
        "    return my_ema\n",
        "\n",
        "ema_udf = udf(ema, DoubleType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8VN59M0sSbi",
        "colab_type": "text"
      },
      "source": [
        "*Windows*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEFHfQ4SjDnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# EMA-WINDOWS\n",
        "#\n",
        "win26 =  Window \\\n",
        "    .partitionBy(\"filename\") \\\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-25, 0)\n",
        "\n",
        "win12 = Window \\\n",
        "    .partitionBy(\"filename\") \\\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-11, 0)\n",
        "\n",
        "win9 = Window \\\n",
        "    .partitionBy(\"filename\") \\\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-8, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWm-U6FdsXKm",
        "colab_type": "text"
      },
      "source": [
        "*PPO- & Signal-Berechnung*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PVMjroXh1VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# EMA_26 && EMA_12 CALCULATION\n",
        "#\n",
        "\n",
        "base_data_tmp_ema_1 = base_data_dayfiltered \\\n",
        "  .withColumn('win26_close_list', collect_list('close').over(win26)) \\\n",
        "  .withColumn('win12_close_list', collect_list('close').over(win12))\n",
        "\n",
        "base_data_tmp_ema_2 = base_data_tmp_ema_1.select(\n",
        "  \"*\",\n",
        "  ema_udf(base_data_tmp_ema_1[\"win26_close_list\"]).alias(\"EMA26\"),\n",
        "  ema_udf(base_data_tmp_ema_1[\"win12_close_list\"]).alias(\"EMA12\")\n",
        ")\n",
        "\n",
        "base_data_tmp_ema_2.createOrReplaceTempView(\"base_data_tmp_ema_2\")\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  base_data_tmp_ema_2.show(Anzeige_Limit, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRu2qFdgqCjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# PPO BERECHNUNG\n",
        "#\n",
        "\n",
        "base_data_tmp_ema_3 = spark.sql(\n",
        "  \" select *, (((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE)) \" \\\n",
        "  \" FROM base_data_tmp_ema_2 \" \\\n",
        "  \" order by filename asc, date desc \"\n",
        ")\n",
        "\n",
        "#\n",
        "# SIGNAL BERECHNUNG\n",
        "#\n",
        "base_data_tmp_ema_4 = base_data_tmp_ema_3 \\\n",
        "  .withColumn('win9_ema12sub26_list', collect_list('(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))').over(win9))\n",
        "\n",
        "\n",
        "base_data_tmp_ema_5 = base_data_tmp_ema_4.select(\n",
        "  \"*\",\n",
        "  base_data_tmp_ema_4['(((EMA12 - EMA26) / EMA26) * CAST(100 AS DOUBLE))'].alias(\"PPO\"),\n",
        "  ema_udf(base_data_tmp_ema_4[\"win9_ema12sub26_list\"]).alias(\"SIGNAL\")\n",
        ")\n",
        "\n",
        "base_data_tmp_ema_5.createOrReplaceTempView(\"base_data_tmp_ema_5\")\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  base_data_tmp_ema_5.show(Anzeige_Limit, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYMSR0WGsgUv",
        "colab_type": "text"
      },
      "source": [
        "*PPO_Histogram-Berechnung*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOvlkuUBwlmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# PPO-HISTOGRAM BERECHNUNG\n",
        "#\n",
        "\n",
        "base_data_tmp_ema_6 = spark.sql(\n",
        "  \" select *, (PPO - SIGNAL)   \" \\\n",
        "  \" FROM base_data_tmp_ema_5 \" \\\n",
        "  \" order by filename asc, date desc \"\n",
        ")\n",
        "\n",
        "base_data_ema = base_data_tmp_ema_6.select(\n",
        "  \"*\",\n",
        "  base_data_tmp_ema_6['(PPO - SIGNAL)'].alias(\"PPO_HISTOGRAM\")\n",
        ")\n",
        "\n",
        "base_data_ema.createOrReplaceTempView(\"base_data_ema\")\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  base_data_ema.show(Anzeige_Limit, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGPOwO7c8VeE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Analyse \n",
        "---\n",
        "*PPO-Analyse ebenfalls basierend auf: [Investopedia.com](https://www.investopedia.com/terms/p/ppo.asp#what-the-indicator-tells-you)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAG4xnpZrakH",
        "colab_type": "text"
      },
      "source": [
        "*Windows*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN_jFOyG9dcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Analyse-Windows\n",
        "#\n",
        "\n",
        "win2 =  Window \\\n",
        "    .partitionBy(\"filename\") \\\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-1, 0)\n",
        "\n",
        "win3 = Window \\\n",
        "    .partitionBy(\"filename\")\\\n",
        "    .orderBy(\"date\") \\\n",
        "    .rowsBetween(-2, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS8w0v_6rhd0",
        "colab_type": "text"
      },
      "source": [
        "*Funktionen*\n",
        "---\n",
        "---\n",
        "**→ ppo_trend_analysis** :\n",
        "\n",
        "Auf den 'PPO' angewendet (window mind. 1):\n",
        "\n",
        "> *When the PPO is above zero, that helps confirm an uptrend since the short-term EMA is above the longer-term EMA. When the PPO is below zero, the short-term EMA is below the longer-term EMA, which is an indication of a downtrend.*\n",
        "\n",
        "\n",
        "**→ ppo_crossover_signal_analysis** :\n",
        "\n",
        "Auf den 'PPO_HISTOGRAM' angewendet (window mind. 2):\n",
        "> *The indicator generates a buy signal when the PPO line crosses above the signal line from below, and a sell signal occurs when the PPO line crosses  below the signal from above.*\n",
        "\n",
        "Auf den 'PPO' angewendet (window mind. 2):\n",
        "> *Centerline crossovers also generate trading signals. Traders consider a move from below to above the centerline as bullish, and a move from above to below the centerline as bearish.*\n",
        "\n",
        "**→ ppo_technical_divergence_analysis:**\n",
        "\n",
        "Auf den 'close' und 'PPO' (window mind. 2; ggf. 'close' mit 'open' tauschen):\n",
        "> *Traders can also use the PPO to look for technical divergence between the indicator and price. For example, if the price of an asset makes a higher high, but the indicator makes a lower high, it may indicate the upward momentum is subsiding. Conversely, if an asset's price makes a lower low, but the indicator makes a higher low, it could suggest that the bears are losing their traction and the price could head higher soon.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUqIGok9ltS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Analyse-UDF\n",
        "#\n",
        "\n",
        "def ppo_trend_analysis(ar):\n",
        "    output = \"\"\n",
        "    count = len(ar)\n",
        "    if count > 0:\n",
        "      # Einzelabgleich\n",
        "      if count == 1:\n",
        "        current = ar[0]\n",
        "        if current > 0:\n",
        "          output = \"▲ Aufwärtstrend\" # \"Aufwärtstrend\"\n",
        "        elif current < 0:\n",
        "          output = \"▼ Abwärtstrend\" # \"Abwärtstrend\"\n",
        "\n",
        "      # Vergleich mit den davorherigenden; ggf. mit mehreren vorherigen\n",
        "      elif count > 1:\n",
        "        last_position = count - 1\n",
        "        next_to_last_position = last_position - 1\n",
        "\n",
        "        last = ar[last_position] \n",
        "        next_to_last = ar[next_to_last_position]\n",
        "\n",
        "        if last > 0 and next_to_last > 0:\n",
        "          output = \"▲ Aufwärtstrend\"\n",
        "        elif last < 0 and next_to_last > 0:\n",
        "          output = \"↘ neuer Abwärtstrend\"\n",
        "        elif last < 0 and next_to_last < 0:\n",
        "          output = \"▼ Abwärtstrend\"\n",
        "        elif last > 0 and next_to_last < 0:\n",
        "          output = \"↗ neuer Aufwärtstrend\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def ppo_crossover_signal_analysis(ar):\n",
        "    output = \"\"\n",
        "    count = len(ar)\n",
        "    # Vergleich des letzten Crossovers\n",
        "    if count > 1:\n",
        "        last_position = count - 1\n",
        "        next_to_last_position = last_position - 1\n",
        "        \n",
        "        last = ar[last_position] \n",
        "        next_to_last = ar[next_to_last_position]\n",
        "\n",
        "        if last <= 0 and next_to_last > 0:\n",
        "          output = \"Verkauf-Signal\"\n",
        "        elif last >= 0 and next_to_last < 0:\n",
        "          output = \"Kauf-Signal\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def ppo_technical_divergence_analysis(close_ar, ppo_ar):\n",
        "    output = \"\"\n",
        "    count_close = len(close_ar)\n",
        "    count_ppo = len(ppo_ar)\n",
        "    \n",
        "    # Arrays müssen selbe Größe haben\n",
        "    if count_close == count_ppo and count_close > 1:\n",
        "      # delta berechnen\n",
        "      pointer = count_close - 1\n",
        "      delta_close = close_ar[pointer]\n",
        "      delta_ppo = ppo_ar[pointer]\n",
        "      pointer -= 1\n",
        "\n",
        "      while pointer >= 0:\n",
        "        delta_close -= close_ar[pointer]\n",
        "        delta_ppo -= ppo_ar[pointer]\n",
        "        pointer -= 1\n",
        "\n",
        "      # steigung berechnen\n",
        "      div_close = delta_close / count_close\n",
        "      div_ppo = delta_ppo / count_ppo\n",
        "\n",
        "      if div_close > div_ppo:\n",
        "        output = \"↧ Abflachend\"\n",
        "      else:\n",
        "        output = \"↥ Wachsend\"\n",
        "\n",
        "    return output\n",
        "\n",
        "ppo_trend_analysis_udf = udf(ppo_trend_analysis, StringType())\n",
        "ppo_crossover_signal_analysis_udf = udf(ppo_crossover_signal_analysis, StringType())\n",
        "ppo_technical_divergence_analysis_udf = udf(ppo_technical_divergence_analysis, StringType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwS8Vh_kro1G",
        "colab_type": "text"
      },
      "source": [
        "*Aggregation*\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkRAgL94J5Px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# SIGNAL ANALYSIS\n",
        "#\n",
        "\n",
        "base_data_ema_tmp_eval = base_data_ema \\\n",
        "  .withColumn('win2_ppo_list', collect_list('PPO').over(win2)) \\\n",
        "  .withColumn('win2_ppoh_list', collect_list('PPO_Histogram').over(win2)) \\\n",
        "  .withColumn('win2_close_list', collect_list('close').over(win2))\n",
        "\n",
        "base_data_ema_eval_result = base_data_ema_tmp_eval.select(\n",
        "  \"*\",\n",
        "  ppo_trend_analysis_udf(base_data_ema_tmp_eval[\"win2_ppo_list\"]).alias(\"TREND\"),\n",
        "  ppo_crossover_signal_analysis_udf(base_data_ema_tmp_eval[\"win2_ppo_list\"]).alias(\"WEAK_SIGNAL\"),\n",
        "  ppo_crossover_signal_analysis_udf(base_data_ema_tmp_eval[\"win2_ppoh_list\"]).alias(\"STRONG_SIGNAL\"),\n",
        "  ppo_technical_divergence_analysis_udf(base_data_ema_tmp_eval[\"win2_close_list\"], base_data_ema_tmp_eval[\"win2_ppo_list\"]).alias(\"DIVERGENCE\")\n",
        ")\n",
        "\n",
        "base_data_ema_eval_result.createOrReplaceTempView(\"base_data_ema_eval_result\")\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  base_data_ema_eval_result.show(Anzeige_Limit, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCOrGQDoL53n",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Auswertung\n",
        "---\n",
        "\n",
        "*Für alle finalen Empfehlungen gilt eine Reichteweite von 0 (schwach) bis 3 (stark)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5DugOoQMQw1",
        "colab_type": "text"
      },
      "source": [
        "**Halten**\n",
        "---\n",
        "---\n",
        "*   **H0:** \n",
        "  * ▼ Abwärtstrend && ↧ Abflachend\n",
        "  * ▼ Abwärtstrend && ↥ Wachsend\n",
        "  * ↘ neuer Abwärtstrend && ↧ Abflachend\n",
        "*   **H1:**\n",
        "  * ↗ neuer Aufwärtstrend && ↧ Abflachend\n",
        "*   **H2:**\n",
        "  * ▲ Aufwärtstrend && ↧ Abflachend\n",
        "  * ↘ neuer Abwärtstrend && ↥ Wachsend\n",
        "*   **H3:**\n",
        "  * ▲ Aufwärtstrend && ↥ Wachsend\n",
        "  * ↗ neuer Aufwärtstrend && ↥ Wachsend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS-0YC6zq8Lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Halten-SQL-Statement\n",
        "#\n",
        "\n",
        "holdingCaseStatement = \"CASE WHEN (TREND = \\\"↗ neuer Aufwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"H1\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" = \\\"↧ Abflachend\\\") OR (TREND = \\\"↘ neuer Abwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"H2\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") OR (TREND = \\\"↗ neuer Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"H3\\\"\" +\"\\n\" \\\n",
        "\"ELSE \\\"H0\\\" END\" +\"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ibvDfPXqgXO",
        "colab_type": "text"
      },
      "source": [
        "**Kaufen**\n",
        "---\n",
        "---\n",
        "*   **K0:** \n",
        "  * ▼ Abwärtstrend && ↧ Abflachend\n",
        "  * ↘ neuer Abwärtstrend && ↧ Abflachend\n",
        "*   **K1:**\n",
        "  * ▼ Abwärtstrend && ↥ Wachsend\n",
        "  * ↘ neuer Abwärtstrend && ↥ Wachsend\n",
        "  * ↗ neuer Aufwärtstrend && ↧ Abflachend\n",
        "*   **K2:**\n",
        "  * ▲ Aufwärtstrend && ↧ Abflachend\n",
        "  * ↗ neuer Aufwärtstrend && ↥ Wachsend\n",
        "*   **K3:**\n",
        "  * ▲ Aufwärtstrend && ↥ Wachsend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkstKP8lq2xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Kaufen-SQL-Statement\n",
        "#\n",
        "\n",
        "buyingCaseStatement = \"CASE WHEN (TREND = \\\"▼ Abwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") OR (TREND = \\\"↘ neuer Abwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"K0\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") OR (TREND = \\\"↗ neuer Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"K2\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"K3\\\"\" +\"\\n\" \\\n",
        "\"ELSE \\\"K1\\\" END\" +\"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffbB0Dhqm38",
        "colab_type": "text"
      },
      "source": [
        "**Verkaufen**\n",
        "---\n",
        "---\n",
        "*   **V0:** \n",
        "  * ▲ Aufwärtstrend && ↥ Wachsend\n",
        "  * ▲ Aufwärtstrend && ↧ Abflachend\n",
        "*   **V1:**\n",
        "  * ↗ neuer Aufwärtstrend && ↥ Wachsend\n",
        "  * ↘ neuer Abwärtstrend && ↥ Wachsend\n",
        "*   **V2:**\n",
        "  * ▼ Abwärtstrend && ↥ Wachsend\n",
        "  * ↗ neuer Aufwärtstrend && ↧ Abflachend\n",
        "  * ↘ neuer Abwärtstrend && ↧ Abflachend\n",
        "*   **V3:**\n",
        "  * ▼ Abwärtstrend && ↧ Abflachend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAB7dievRveL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Verkaufen-SQL-Statement\n",
        "#\n",
        "\n",
        "sellingCaseStatement = \"CASE WHEN (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\")OR (TREND = \\\"▲ Aufwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"V0\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"↗ neuer Aufwärtstrend\\\" = \\\"↥ Wachsend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") OR (TREND = \\\"↘ neuer Abwärtstrend\\\" AND DIVERGENCE = \\\"↥ Wachsend\\\") THEN \\\"V1\\\"\" +\"\\n\" \\\n",
        "\"WHEN (TREND = \\\"▼ Abwärtstrend\\\" AND DIVERGENCE = \\\"↧ Abflachend\\\") THEN \\\"V3\\\" \" +\"\\n\" \\\n",
        "\"ELSE \\\"V2\\\" END\" +\"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Zg1CuVnB3_",
        "colab_type": "text"
      },
      "source": [
        "**Aggregation**\n",
        "---\n",
        "---\n",
        "*vgl. dazu Investopedia-Link (s.h. Sections-Header)*\n",
        "\n",
        "* Wenn beide Signale null sind ⇒ *halten*\n",
        "* Ansonsten 'Strong-' > 'Weaksignal', für jeweils beide gilt:\n",
        "  * KAUF-Signal ⇒ *kaufen*\n",
        "  * VERKAUF-Signal ⇒ *verkaufen*\n",
        "\n",
        "```\n",
        "+----------------+----------------+------------+\n",
        "|  WEAK_SIGNAL   | STRONG_SIGNAL  | EMPFEHLUNG |\n",
        "+----------------+----------------+------------+\n",
        "| null           | null           | HALTEN     |\n",
        "| Kauf-Signal    | null           | KAUFEN     |\n",
        "| Kauf-Signal    | Kauf-Signal    | KAUFEN     |\n",
        "| ...            | ...            | ...        |\n",
        "| Verkauf-Signal | null           | VERKAUFEN  |\n",
        "| Verkauf-Signal | Verkauf-Signal | VERKAUFEN  |\n",
        "| ...            | ...            | ...        |\n",
        "| Kauf-Signal    | Verkauf-Signal | VERKAUFEN  |\n",
        "| Verkauf-Signal | Kauf-Signal    | KAUFEN     |\n",
        "+----------------+----------------+------------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFoLl5e4VX3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Combiniertes-SQL-Statement\n",
        "#\n",
        "\n",
        "final_result_statement = \"SELECT *,\" \\\n",
        "\" CASE\" \\\n",
        "\"   WHEN (WEAK_SIGNAL = \\\"\\\" AND STRONG_SIGNAL = \\\"\\\") THEN \" + holdingCaseStatement + \\\n",
        "\"   WHEN (STRONG_SIGNAL = \\\"\\\") THEN \" \\\n",
        "\"     CASE\" \\\n",
        "\"        WHEN (WEAK_SIGNAL = \\\"Kauf-Signal\\\") THEN \" + buyingCaseStatement + \\\n",
        "\"        ELSE \" + sellingCaseStatement + \\\n",
        "\"     END\" \\\n",
        "\"   ELSE\" \\\n",
        "\"     CASE\" \\\n",
        "\"        WHEN (STRONG_SIGNAL = \\\"Kauf-Signal\\\") THEN \" + buyingCaseStatement + \\\n",
        "\"        ELSE \" + sellingCaseStatement + \\\n",
        "\"     END\"  \\\n",
        "\" END AS PPO_RESULT\" \\\n",
        "\" FROM base_data_ema_eval_result\"\n",
        "\n",
        "data_final_eval_result = spark.sql(final_result_statement)\n",
        "data_final_eval_result.createOrReplaceTempView(\"data_final_eval_result\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTbTV3Tq-yVQ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Visualisierung\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpv5aTZ5s9cr",
        "colab_type": "text"
      },
      "source": [
        "*Parameter*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvznmJ0OAg9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Minimales bis Maximale Datum der geladenen Kurse\n",
        "#\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  #base_data_dayfiltered_informationView.show(Anzeige_Limit) # Durch Darstellung mittels PandasDataframe ersetzt/optimiert\n",
        "  display(HTML(base_data_dayfiltered_informationView.toPandas().to_html(index = False, max_rows = Anzeige_Limit)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbbSlkCNtVYk",
        "colab_type": "text"
      },
      "source": [
        "*Vorbereitung*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXtWob2qAqPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Datenvorbereitung\n",
        "#\n",
        "if Anzeige_Aktiviert:\n",
        "  # Daten von Auswertung in Dataframe auswählen\n",
        "  data_eval_result = spark.sql(\n",
        "    \" select filename, Date(date), format_number(high, \"+Anzeige_Zahlen_Genaugikeit_str+\") high, format_number(low, \"+Anzeige_Zahlen_Genaugikeit_str+\") low, format_number(open, \"+Anzeige_Zahlen_Genaugikeit_str+\") open, format_number(close, \"+Anzeige_Zahlen_Genaugikeit_str+\") close\" \\\n",
        "    \" , format_number(PPO, \"+Anzeige_Zahlen_Genaugikeit_str+\") PPO, format_number(SIGNAL, \"+Anzeige_Zahlen_Genaugikeit_str+\") SIGNAL, format_number(PPO_HISTOGRAM, \"+Anzeige_Zahlen_Genaugikeit_str+\") PPO_HISTOGRAM \" \\\n",
        "    \" , TREND, WEAK_SIGNAL, STRONG_SIGNAL, DIVERGENCE, PPO_RESULT \"\\\n",
        "    \" FROM data_final_eval_result \" \\\n",
        "    \" WHERE date >= '\"+ Anzeige_Start_Tag.strip() +\" 00:00:00' AND date <= '\"+ Anzeige_Ende_Tag.strip() +\"23:59:59' \" \\\n",
        "    \" order by filename asc, date asc \" \n",
        "  )\n",
        "\n",
        "  # Vorbereiten der for-each-loop\n",
        "  data_eval_filenames = spark.sql(\"select distinct filename from data_final_eval_result\")\n",
        "  data_eval_filenames_ar = [row.filename for row in data_eval_filenames.collect()]\n",
        "\n",
        "else:\n",
        "  print(\"Aufbereitung der Daten zur Visualisierung durch Parameter 'Anzeige_Aktiviert' übersprungen.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RB_j1k5-MId",
        "colab_type": "text"
      },
      "source": [
        "Output\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37rKNTHlBrbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Charts & Table\n",
        "#\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  \n",
        "  # Zusätzliches CSS in der Finalen Darstellung:\n",
        "  hideInjectedFilenameColumn_CSS = \"<style>\" \\\n",
        "    \"table td:nth-child(1) { display:none;}\" \\\n",
        "    \"table.dataframe thead th:first-child {display: none;}\" \\\n",
        "    \"</style>\"\n",
        "\n",
        "  # Darstellungs-Loop für alle Filenames:\n",
        "  for currentFilename in data_eval_filenames_ar:\n",
        "\n",
        "    # Gefilteter Dataframe und dessen Collection:\n",
        "    current_data_df = data_eval_result.where(data_eval_result['filename'] == currentFilename) # alle Columns ausgewählt für informativere Dataframe-Darstellung\n",
        "    current_data_collection = current_data_df.collect()\n",
        "\n",
        "    # Aufbau der Charts:\n",
        "    current_data_df_dates = [Timestamp(row.date) for row in current_data_collection]\n",
        "\n",
        "    current_stock_chart = go.Figure(\n",
        "      data = [ go.Candlestick(\n",
        "          x = current_data_df_dates,\n",
        "          open = [row.open for row in current_data_collection],\n",
        "          high = [row.high for row in current_data_collection],\n",
        "          low = [row.low for row in current_data_collection],\n",
        "          close = [row.close for row in current_data_collection]\n",
        "      )]\n",
        "    )\n",
        "\n",
        "    current_ppo_chart = go.Figure(\n",
        "        data = [ go.Scatter(\n",
        "            x = current_data_df_dates,\n",
        "            y = [row.PPO for row in current_data_collection],\n",
        "            mode = 'lines',\n",
        "            name = 'PPO'\n",
        "        ), go.Scatter(\n",
        "            x = current_data_df_dates,\n",
        "            y = [row.SIGNAL for row in current_data_collection],\n",
        "            mode = 'lines',\n",
        "            name = 'Signal'\n",
        "        ), go.Bar(\n",
        "            x = current_data_df_dates,\n",
        "            y = [row.PPO_HISTOGRAM for row in current_data_collection],\n",
        "            name = 'PPO-Histogram'\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    # Chart Styling:\n",
        "    current_stock_chart.update_layout(\n",
        "        title='Chart für CSV-Datei \\'' + currentFilename + '\\'',\n",
        "        yaxis_title = 'Punkte',\n",
        "        template = Anzeige_Template,\n",
        "        xaxis_rangeslider_visible = False\n",
        "    )\n",
        "\n",
        "    current_ppo_chart.update_layout(\n",
        "        title='PPO-Indikator für CSV-Datei \\'' + currentFilename + '\\'',\n",
        "        yaxis_title = 'Änderung in %',\n",
        "        template = Anzeige_Template\n",
        "    )\n",
        "\n",
        "    # Ausgabe:\n",
        "    print(\"\\n\\nFilename: \" + currentFilename + \"\\nZeitraum: \" + Anzeige_Start_Tag + \" → \" + Anzeige_Ende_Tag + \"\\n\")\n",
        "    current_stock_chart.show()\n",
        "    current_ppo_chart.show()\n",
        "    print()\n",
        "    #current_data_df.show() # Durch Darstellung mittels PandasDataframe ersetzt/optimiert\n",
        "    display(HTML(current_data_df.toPandas().to_html(index = False, max_rows = Anzeige_Limit) + hideInjectedFilenameColumn_CSS))\n",
        "    print()\n",
        "\n",
        "else:\n",
        "  print(\"Visualisierung durch Parameter 'Anzeige_Aktiviert' übersprungen.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjCN5PgZ2sgY",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "# CSV-Export\n",
        "----\n",
        "*Die Daten sollen nach Tage sotiert ausgegeben werden (vgl. Besprechung 13.12.19)*\n",
        "\n",
        "*Darstellung des Datums nach [ISO-8601-Norm](https://lmgtfy.com/?q=ISO-8601) (z.B.: 2019-09-07)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csUZHyk-6XYB",
        "colab_type": "text"
      },
      "source": [
        "Ordner-Struktur\n",
        "---\n",
        "\n",
        "**aktueller Pfad sollte '~/content' sein**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crTttkJC3wWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# (Neuen) Ordner Erstellen\n",
        "#\n",
        "\n",
        "# Alten Ordner löschen:\n",
        "if os.path.exists(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername)):\n",
        "  shutil.rmtree(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))\n",
        "\n",
        "# Ordner-Checks (ggf. Erstellen der Ordner):\n",
        "if not os.path.exists(Export_Mainordner_Pfad):\n",
        "  os.mkdir(Export_Mainordner_Pfad)\n",
        "\n",
        "if not os.path.exists(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad)):\n",
        "  os.mkdir(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad))\n",
        "\n",
        "if not os.path.exists(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername)):\n",
        "  os.mkdir(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOU6H1W84Cml",
        "colab_type": "text"
      },
      "source": [
        "Vorbereitung\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svwpxboa4EG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Outputt Daten sammeln\n",
        "#\n",
        "\n",
        "# Zu speicherende Daten als Dataframe auswählen:\n",
        "data_output = spark.sql(\n",
        "  \" select filename, Date(date) date, open, close, high, low, volume, PPO_RESULT\" \\\n",
        "  \" FROM data_final_eval_result \" \\\n",
        "  \" order by filename asc, date asc \" \n",
        ")\n",
        "data_output.collect();\n",
        "\n",
        "# Zu filternde Tage:\n",
        "data_output_days = spark.sql(\"select distinct Date(date) date FROM data_final_eval_result\")\n",
        "data_output_days_ar = array([Timestamp(row.date) for row in data_output_days.collect()]) # Für die Ausgabe als Numpy-Array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_S_nGOd1u5N",
        "colab_type": "text"
      },
      "source": [
        "Export\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOjNDRNcD8kx",
        "colab_type": "code",
        "outputId": "dea154c1-e29c-4de5-a9f0-56236dadfb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#\n",
        "# Daten als csv exportieren\n",
        "#\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  print(\"Zu exportierende Tage (Anzahl = \" + str(data_output_days_ar.size) + \"):\\n\")\n",
        "  numpy.set_printoptions(threshold = Anzeige_Limit)\n",
        "  print(data_output_days_ar)\n",
        "  print(\"\\nZu exportierender Datensatz:\")\n",
        "  display(HTML(data_output.toPandas().to_html(index = False, max_rows = Anzeige_Limit)))\n",
        "\n",
        "for currentDay in data_output_days_ar:\n",
        "    # Datenframe für den aktuellen Tag:\n",
        "    currentOutput = data_output.where(data_output.date == currentDay)\n",
        "\n",
        "    # Anpassung des Monats zu 2 Stellen\n",
        "    if currentDay.month > 9:\n",
        "      formattedMonth = str(currentDay.month)\n",
        "    else:\n",
        "      formattedMonth = '0' + str(currentDay.month)\n",
        "\n",
        "    # Anpassung des Tages zu 2 Stellen\n",
        "    if currentDay.day > 9:\n",
        "      formattedDay = str(currentDay.day)\n",
        "    else:\n",
        "      formattedDay = '0' + str(currentDay.day)\n",
        "\n",
        "    # Filename & Filepfad\n",
        "    currentFilename = str(currentDay.year) + \"-\" + formattedMonth + \"-\" + formattedDay\n",
        "    currentPath = os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername, currentFilename)\n",
        "    destinationPath = os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername, (currentFilename + \".csv\") )\n",
        "\n",
        "    # Schreiben\n",
        "    if Pandas_CSV_Export:\n",
        "      currentOutput.toPandas().to_csv(currentPath, encoding = 'utf-8', index = False, mode = 'w')\n",
        "    else:\n",
        "      currentOutput.write.csv(currentPath, mode = 'overwrite', encoding = 'utf-8', header = True )\n",
        "\n",
        "if Anzeige_Aktiviert:\n",
        "  print(\"\\n\\nExport abgeschlossen, erstellte Dateien:\")\n",
        "  os.listdir(os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))\n",
        "else:\n",
        "  print(\"\\n\\nExport abgeschlossen.\")\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Export abgeschlossen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QenC4NDjP4hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "filename = \"main.csv\"\n",
        "\n",
        "with open(filename, 'a') as singleFile:\n",
        "    first_csv = True\n",
        "    for csv in glob('ABDA2019/spark_warehouse/ppo_csv_export/2019-06-30/*.csv'):\n",
        "        if csv == filename:\n",
        "            pass\n",
        "        else:\n",
        "            header = True\n",
        "            for line in open(csv, 'r'):\n",
        "                if first_csv and header:\n",
        "                    singleFile.write(line)\n",
        "                    first_csv = False\n",
        "                    header = False\n",
        "                elif header:\n",
        "                    header = False\n",
        "                else:\n",
        "                    singleFile.write(line)\n",
        "    singleFile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt1Pl_6uTZXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if Merge_Export_Part:\n",
        "  for currentDay in data_output_days_ar:\n",
        "    # Anpassung des Monats zu 2 Stellen\n",
        "    if currentDay.month > 9:\n",
        "      formattedMonth = str(currentDay.month)\n",
        "    else:\n",
        "      formattedMonth = '0' + str(currentDay.month)\n",
        "\n",
        "    # Anpassung des Tages zu 2 Stellen\n",
        "    if currentDay.day > 9:\n",
        "      formattedDay = str(currentDay.day)\n",
        "    else:\n",
        "      formattedDay = '0' + str(currentDay.day)\n",
        "\n",
        "    # Filename & Filepfad\n",
        "    currentFilename = str(currentDay.year) + \"-\" + formattedMonth + \"-\" + formattedDay\n",
        "    currentPath = os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername, currentFilename)\n",
        "    exportFilename = os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername, (currentFilename + \".csv\"))\n",
        "\n",
        "\n",
        "    # Mergen | Credits: https://stackoverflow.com/a/59108574\n",
        "    with open(exportFilename, 'a') as singleFile:\n",
        "      first_csv = True\n",
        "\n",
        "      for csv in glob((currentPath + '/*.csv')):\n",
        "          if csv == filename:\n",
        "              pass\n",
        "          else:\n",
        "              header = True\n",
        "\n",
        "              for line in open(csv, 'r'):\n",
        "                  if first_csv and header:\n",
        "                      singleFile.write(line)\n",
        "                      first_csv = False\n",
        "                      header = False\n",
        "                  elif header:\n",
        "                      header = False\n",
        "                  else:\n",
        "                      singleFile.write(line)\n",
        "\n",
        "      singleFile.close()\n",
        "\n",
        "    #\n",
        "    # Part-Ordner ggf. löschen\n",
        "    #\n",
        "    if Delete_Export_Part:\n",
        "      if os.path.exists(currentPath):\n",
        "        shutil.rmtree(currentPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOvGVIG4sMLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Daten ggf. als Download anbieten\n",
        "#\n",
        "\n",
        "if Download_Export_Ordner:\n",
        "  shutil.make_archive('Exportdaten_PPO', 'zip', os.path.join(Export_Mainordner_Pfad, Export_Subordner_Pfad, Export_Ordnername))\n",
        "  files.download('/content/Exportdaten_PPO.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvM2pNY5m68h",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Vergleichbare Charts\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_fCKAEpnBXE",
        "colab_type": "text"
      },
      "source": [
        "**BTCUSD**: [traidingview.com](https://de.tradingview.com/chart/ecCxGiMv/) *(login erforderlich)*\n",
        "\n",
        "```\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| TraidingView.com:                                                 |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "|             | 2019-10-01 | 2019-09-30 | 2019-08-10  | 2019-08-07  |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| EMA26       |    9305.74 |    9384.68 |    10848.14 |    10630.29 |\n",
        "| EMA12       |    8734.78 |    8810.38 |    11193.74 |    10886.97 |\n",
        "| PPO         |      -6.14 |      -6.12 |        3.19 |        2.41 |\n",
        "| SIGNAL      |      -4.56 |      -4.17 |        1.63 |        0.13 |\n",
        "| PPO_History |      -1.57 |      -1.95 |        1.55 |        2.28 |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| CALCULATED:                                                       |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "|             | 2019-10-01 | 2019-09-30 | 2019-08-10  | 2019-08-07  |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "| EMA26       |  9327.3017 |  9446.6421 |  10651.5067 |  10673.4093 |\n",
        "| EMA12       |  8751.5960 |  8842.4748 |  11163.8865 |  10785.1409 |\n",
        "| PPO         |     -6.172 |    -6.3956 |      4.8104 |      1.0468 |\n",
        "| SIGNAL      |    -4.4147 |    -4.0178 |      0.9102 |     -1.8162 |\n",
        "| PPO_History |    -1.7576 |    -2.3778 |      3.9002 |      2.8631 |\n",
        "+-------------+------------+------------+-------------+-------------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asHXGi907ytk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**intc.csv (utf-8)** *--> eher ungeeignetes Beispiel*\n",
        "\n",
        " see [School.StockCharts.com](https://school.stockcharts.com/doku.php?id=technical_indicators:price_oscillators_ppo) \n",
        "\n",
        "```\n",
        "time,open,close,high,low,volume\n",
        "1271203140000,21.16,20.16,20.16,20.16,0.0\n",
        "1271289540000,20.49,20.49,20.49,20.49,0.0\n",
        "1271375940000,20.74,20.74,20.74,20.74,0.0\n",
        "1271462340000,20.77,20.77,20.77,20.77,0.0\n",
        "1271721540000,20.53,20.53,20.53,20.53,0.0\n",
        "1271807940000,19.61,19.61,19.61,19.61,0.0\n",
        "1271894340000,20.02,20.02,20.02,20.02,0.0\n",
        "1271980740000,19.70,19.70,19.70,19.70,0.0\n",
        "1272067140000,19.94,19.94,19.94,19.94,0.0\n",
        "1272326340000,19.62,19.62,19.62,19.62,0.0\n",
        "1272412740000,19.11,19.11,19.11,19.11,0.0\n",
        "1272499140000,19.32,19.32,19.32,19.32,0.0\n",
        "1272585540000,19.61,19.61,19.61,19.61,0.0\n",
        "1272671940000,19.54,19.54,19.54,19.54,0.0\n",
        "1272931140000,18.89,18.89,18.89,18.89,0.0\n",
        "1273017540000,19.33,19.33,19.33,19.33,0.0\n",
        "1273103940000,19.21,19.21,19.21,19.21,0.0\n",
        "1273190340000,19.51,19.51,19.51,19.51,0.0\n",
        "1273276740000,19.55,19.55,19.55,19.55,0.0\n",
        "1273535940000,19.92,19.92,19.92,19.92,0.0\n",
        "1273622340000,20.29,20.29,20.29,20.29,0.0\n",
        "1273708740000,20.58,20.58,20.58,20.58,0.0\n",
        "1273795140000,20.52,20.52,20.52,20.52,0.0\n",
        "1273881540000,20.69,20.69,20.69,20.69,0.0\n",
        "1274140740000,20.67,20.67,20.67,20.67,0.0\n",
        "1274227140000,20.72,20.72,20.72,20.72,0.0\n",
        "1274313540000,20.25,20.25,20.25,20.25,0.0\n",
        "1274399940000,20.56,20.56,20.56,20.56,0.0\n",
        "1274486340000,20.49,20.49,20.49,20.49,0.0\n",
        "1274745540000,20.39,20.39,20.39,20.39,0.0\n",
        "\n",
        "```\n",
        "\n"
      ]
    }
  ]
}