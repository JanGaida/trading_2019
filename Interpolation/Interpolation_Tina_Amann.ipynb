{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interpolation_Full(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZz4gn4FFOyt",
        "colab_type": "text"
      },
      "source": [
        "# Umgebung vorbereiten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHP-l4mtELhW",
        "colab_type": "code",
        "outputId": "3d529856-9563-4e39-f561-8e00b41cec68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "  # Repo klonen\n",
        "\n",
        "!git clone https://github.com/sleuoth/ABDA2019.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ABDA2019'...\n",
            "remote: Enumerating objects: 424, done.\u001b[K\n",
            "remote: Total 424 (delta 0), reused 0 (delta 0), pack-reused 424\u001b[K\n",
            "Receiving objects: 100% (424/424), 486.44 MiB | 18.62 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "Checking out files: 100% (418/418), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0YFIEt7EWNV",
        "colab_type": "code",
        "outputId": "0516dded-07b8-429d-bfe7-66de723e5e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Das ausführen, wenn die Dateien aus dem Repo verwendet werden sollen:\n",
        "\n",
        "#!ls ABDA2019/testdaten/cryptominuteresolution\n",
        "\n",
        "!unzip ABDA2019/testdaten/cryptominuteresolution/btcusd.csv.zip\n",
        "!mv btcusd.csv ABDA2019/testdaten/cryptominuteresolution/btcusd.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ABDA2019/testdaten/cryptominuteresolution/btcusd.csv.zip\n",
            "  inflating: btcusd.csv              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrQ-xBH5E-S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Das ausführen, wenn eine importierte Datei verwendet werden soll:\n",
        "\n",
        "#!ls ABDA2019\n",
        "#!unzip ABDA2019/cryptores.zip    #Dateipfad noch angeben"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIOBkqGgENR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spark einbinden\n",
        "\n",
        "#!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "#!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "\n",
        "#!pip3 install ta-lib --force-reinstall wheel datascience urllib3 kaggle\n",
        "!pip install ts ts-flint\n",
        "\n",
        "import findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "#spark = SparkSession.builder.config('spark.driver.extraClassPath', '/path/to/postgresql.jar').getOrCreate()\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhVFgcBMFYTG",
        "colab_type": "text"
      },
      "source": [
        "# Daten auf Interpolation vorbereiten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll32ulM7Fd4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################\n",
        "#\n",
        "#  Alle csv Dateien einlesen\n",
        "#\n",
        "###############################################################\n",
        "\n",
        "allData = []       \n",
        "allFilenames = []\n",
        "\n",
        "directory = \"ABDA2019/testdaten/cryptominuteresolution\"   #Hier Pfad auf das ändern was man auslesen möchte\n",
        "\n",
        "import os\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "  if filename.endswith(\".csv\"):\n",
        "    print(filename)\n",
        "    actualFile = directory + \"/\" \n",
        "    actualFile = actualFile + filename\n",
        "    allData.append(spark.read.csv(actualFile, inferSchema=True, header=True))\n",
        "    allFilenames.append(filename)\n",
        "\n",
        "import numpy as np\n",
        "data = np.asarray(allData)\n",
        "filenames = np.asarray(allFilenames)\n",
        "\n",
        "#data[1].show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTpMtsmzF_WP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "1d89d7dc-0b5a-4fd8-834e-2513195d73cb"
      },
      "source": [
        "###############################################################\n",
        "#\n",
        "#  Die CSV Dateien für Interpolation aufbereiten\n",
        "#\n",
        "###############################################################\n",
        "\n",
        "modifiedData = []\n",
        "\n",
        "for i in range (0, len(data)):\n",
        "  # Erstmal in richtigen Timestamp umwandeln\n",
        "  tableOne = data[i]\n",
        "  tableOne.createOrReplaceTempView(\"viewOne\")\n",
        "  basis_daten = spark.sql(\"SELECT time, open, close, high, low, volume, CAST(time/1000 AS Timestamp) AS the_date FROM viewOne\")\n",
        " \n",
        "  # Timestamp in richtiges Datum umwandeln\n",
        "  basis_daten.createOrReplaceTempView(\"oneday\")\n",
        "  dftemp = spark.sql(\"SELECT time, open, close, high, low, volume, CAST(the_date AS DATE) FROM oneday ORDER BY time\")\n",
        "\n",
        "  #End of the Day bestimmen\n",
        "  dftemp.createOrReplaceTempView(\"dates\")\n",
        "  relevantVals = spark.sql(\"SELECT m1.* FROM dates m1 LEFT JOIN dates m2 ON(m1.the_date = m2.the_date AND m1.time < m2.time) WHERE m2.time IS NULL\")\n",
        "  relevantVals = relevantVals.orderBy('the_date')\n",
        "  modifiedData.append(relevantVals)\n",
        "\n",
        "#Daten wieder in einen Array umwandeln\n",
        "modifiedData = np.asarray(modifiedData)\n",
        "modifiedData[1].show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+--------------------+--------------------+--------------------+--------------------+------------------+----------+\n",
            "|         time|                open|               close|                high|                 low|            volume|  the_date|\n",
            "+-------------+--------------------+--------------------+--------------------+--------------------+------------------+----------+\n",
            "|1558386180000|             1.36E-4|            1.354E-4|             1.36E-4|            1.354E-4|            6100.0|2019-05-20|\n",
            "|1558468800000|           1.3349E-4|           1.3519E-4|           1.3519E-4|           1.3349E-4|     1832.18285925|2019-05-21|\n",
            "|1558557600000|           1.4275E-4|           1.4275E-4|           1.4275E-4|           1.4275E-4|1548.9725829000001|2019-05-22|\n",
            "|1558646820000|           1.4061E-4|           1.4061E-4|           1.4061E-4|           1.4061E-4|             550.9|2019-05-23|\n",
            "|1558739640000|           1.5249E-4|           1.5249E-4|           1.5249E-4|           1.5249E-4|              91.8|2019-05-24|\n",
            "|1558827720000|           1.6663E-4|           1.6634E-4|           1.6663E-4|           1.6634E-4|            3474.0|2019-05-25|\n",
            "|1558909440000|           1.7017E-4|           1.7017E-4|           1.7017E-4|           1.7017E-4|      303.19882515|2019-05-26|\n",
            "|1559000880000|           1.5956E-4|           1.5778E-4|           1.5956E-4|           1.5778E-4|      109.16388608|2019-05-27|\n",
            "|1559082780000|           1.6711E-4|           1.6711E-4|           1.6711E-4|           1.6711E-4|            2500.0|2019-05-28|\n",
            "|1559172840000|           1.7384E-4|           1.7384E-4|           1.7384E-4|           1.7384E-4|            1000.0|2019-05-29|\n",
            "|1559260680000|           1.6979E-4|           1.6973E-4|           1.6979E-4|           1.6973E-4|     2882.34505671|2019-05-30|\n",
            "|1559346420000|           1.7352E-4|           1.7351E-4|           1.7352E-4|           1.7351E-4|            804.76|2019-05-31|\n",
            "|1559427900000|           1.6429E-4|           1.6429E-4|           1.6429E-4|           1.6429E-4|          66.07608|2019-06-01|\n",
            "|1559518260000|           1.6471E-4|           1.6471E-4|           1.6471E-4|           1.6471E-4|           2375.09|2019-06-02|\n",
            "|1559604180000|           1.7216E-4|           1.7216E-4|           1.7216E-4|           1.7216E-4|      907.82754099|2019-06-03|\n",
            "|1559691480000|           1.8975E-4|           1.8975E-4|           1.8975E-4|           1.8975E-4|             300.0|2019-06-04|\n",
            "|1559779020000|           1.9065E-4|           1.9065E-4|           1.9065E-4|           1.9065E-4|             100.0|2019-06-05|\n",
            "|1559865480000|2.119000000000000...|           2.1192E-4|           2.1192E-4|2.119000000000000...|            363.27|2019-06-06|\n",
            "|1559948640000|           2.0939E-4|           2.0939E-4|           2.0939E-4|           2.0939E-4| 821.2991568799998|2019-06-07|\n",
            "|1560036660000|2.169000000000000...|2.169000000000000...|2.169000000000000...|2.169000000000000...|       52.80984074|2019-06-08|\n",
            "+-------------+--------------------+--------------------+--------------------+--------------------+------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trY5oEuCFzmO",
        "colab_type": "text"
      },
      "source": [
        "# Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klsKj6Sr8UFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ausführen, falls es den Interpolationsordner noch nicht gibt\n",
        "\n",
        "!mkdir ABDA2019/Interpolation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoaGlej4GXCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "#\n",
        "# Methoden für die Interpolation\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "def getPolVal (xbefore, ybefore, xafter, yafter, x):\n",
        "  y = ybefore + ((float(x-xbefore)/float(xafter-xbefore)) * (yafter - ybefore))\n",
        "  return y\n",
        "\n",
        "from datetime import timedelta, date, datetime\n",
        "\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int((end_date - start_date).days)):\n",
        "      yield start_date + timedelta(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NPsDChJGZx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "#\n",
        "# Die Interpolation selbst\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "for ii in range (0, len(modifiedData)):\n",
        "  # Tabelle kopieren, weil bei neue Datensätze hinzukommen\n",
        "  duplicateData = modifiedData[ii]\n",
        "  #duplicateData.show()\n",
        "\n",
        "  duplicateDataCollect = duplicateData.collect()\n",
        "  if (len(duplicateDataCollect)==0): continue\n",
        "\n",
        "  firstdate = duplicateDataCollect[0].the_date\n",
        "  lastdate = duplicateDataCollect[-1].the_date\n",
        "\n",
        "  i = 0\n",
        "  for single_date in daterange(firstdate, lastdate):\n",
        "    print(single_date.strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "    # zweiten Listeneintrag bestimmen, weil sobald ein Tag gegeben ist wird i hochgestzt\n",
        "    dayTwo = duplicateDataCollect[i]\n",
        "\n",
        "    # Gibt es diesen Tag nicht beginnt Interpolation\n",
        "    if(dayTwo.the_date != single_date):   \n",
        "\n",
        "      # vorheringen Tag bestimmen\n",
        "      dayOne = duplicateDataCollect[i-1]\n",
        "\n",
        "      # Zeitstempel bestimmen\n",
        "      timest = datetime.combine(single_date, datetime.min.time())\n",
        "      import time \n",
        "      timest = time.mktime(timest.timetuple()) * 1000\n",
        "\n",
        "      # Open Course bestimmen\n",
        "      from decimal import Decimal\n",
        "      openCourse = getPolVal(dayOne.time,dayOne.open,dayTwo.time,dayTwo.open,timest)\n",
        "      #openCourse = '%.3E' % Decimal(openCourse)                                  \n",
        "      openCourse = format(openCourse, \"10.3E\")\n",
        "\n",
        "      # Closing Course bestimmen\n",
        "      closeCourse = getPolVal(dayOne.time,dayOne.close,dayTwo.time,dayTwo.close,timest) \n",
        "      closeCourse = format(closeCourse, \"10.3E\")\n",
        "\n",
        "      # High bestimmen\n",
        "      highCourse = getPolVal(dayOne.time,dayOne.high,dayTwo.time,dayTwo.high,timest) \n",
        "      highCourse = format(highCourse, \"10.3E\")\n",
        "\n",
        "      # Low bestimmen\n",
        "      lowCourse = getPolVal(dayOne.time,dayOne.low,dayTwo.time,dayTwo.low,timest) \n",
        "      lowCourse = format(lowCourse, \"10.3E\")\n",
        "\n",
        "      # volume bestimmen\n",
        "      volumeCourse = getPolVal(dayOne.time,dayOne.volume,dayTwo.time,dayTwo.volume,timest) \n",
        "      volumeCourse = format(volumeCourse, \"10.3E\")\n",
        "\n",
        "      # Neue Zeile erstellen und hinzufügen\n",
        "      newRow = spark.createDataFrame([(timest, openCourse, closeCourse, highCourse, lowCourse, volumeCourse ,single_date)])\n",
        "      modifiedData[ii] = modifiedData[ii].union(newRow)\n",
        "\n",
        "    # Es gab den Eintrag, also nächsten Eintag ansehen  \n",
        "    else:\n",
        "      i = i+1\n",
        "\n",
        "  modifiedData[ii] = modifiedData[ii].orderBy('the_date')\n",
        "  print(filenames[ii])\n",
        "  print(ii)\n",
        "  filestring = \"ABDA2019/Interpolation/\"+filenames[ii] \n",
        "  modifiedData[ii].toPandas().to_csv(filestring)       \n",
        "  \n",
        "  #modifiedData[ii].coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(filestring)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2jMOoZXHppW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "#\n",
        "# Daten zippen, um sie runterladen zu können\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "!zip -r ABDA2019/Inter_down3.zip ABDA2019/Interpolation\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"ABDA2019/Inter_down3.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}